<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Algorithms · FrankWolfe.jl</title><meta name="title" content="Algorithms · FrankWolfe.jl"/><meta property="og:title" content="Algorithms · FrankWolfe.jl"/><meta property="twitter:title" content="Algorithms · FrankWolfe.jl"/><meta name="description" content="Documentation for FrankWolfe.jl."/><meta property="og:description" content="Documentation for FrankWolfe.jl."/><meta property="twitter:description" content="Documentation for FrankWolfe.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">FrankWolfe.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../basics/">How does it work?</a></li><li><a class="tocitem" href="../../advanced/">Advanced features</a></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">Examples</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../examples/docs_00_fw_visualized/">Visualization of Frank-Wolfe running on a 2-dimensional polytope</a></li><li><a class="tocitem" href="../../examples/docs_01_mathopt_lmo/">Comparison with MathOptInterface on a Probability Simplex</a></li><li><a class="tocitem" href="../../examples/docs_02_polynomial_regression/">Polynomial Regression</a></li><li><a class="tocitem" href="../../examples/docs_03_matrix_completion/">Matrix Completion</a></li><li><a class="tocitem" href="../../examples/docs_04_rational_opt/">Exact Optimization with Rational Arithmetic</a></li><li><a class="tocitem" href="../../examples/docs_05_blended_cg/">Blended Conditional Gradients</a></li><li><a class="tocitem" href="../../examples/docs_06_spectrahedron/">Spectrahedron</a></li><li><a class="tocitem" href="../../examples/docs_07_shifted_norm_polytopes/">FrankWolfe for scaled, shifted <span>$\ell^1$</span> and <span>$\ell^{\infty}$</span> norm balls</a></li><li><a class="tocitem" href="../../examples/docs_08_callback_and_tracking/">Tracking, counters and custom callbacks for Frank Wolfe</a></li><li><a class="tocitem" href="../../examples/docs_09_extra_vertex_storage/">Extra-lazification</a></li><li><a class="tocitem" href="../../examples/docs_10_alternating_methods/">Alternating methods</a></li><li><a class="tocitem" href="../../examples/docs_11_block_coordinate_fw/">Block-Coordinate Frank-Wolfe and Block-Vectors</a></li><li><a class="tocitem" href="../../examples/docs_12_quadratic_symmetric/">Accelerations for quadratic functions and symmetric problems</a></li><li><a class="tocitem" href="../../examples/docs_13_dca_fw/">Difference-of-Convex Algorithm with Frank-Wolfe</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox" checked/><label class="tocitem" for="menuitem-5"><span class="docs-label">API reference</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../0_reference/">API Reference</a></li><li class="is-active"><a class="tocitem" href>Algorithms</a><ul class="internal"><li><a class="tocitem" href="#Standard-algorithms"><span>Standard algorithms</span></a></li><li><a class="tocitem" href="#Active-set-based-methods"><span>Active-set based methods</span></a></li><li><a class="tocitem" href="#Alternating-Methods"><span>Alternating Methods</span></a></li><li><a class="tocitem" href="#Difference-of-Convex-Algorithm-with-Frank-Wolfe"><span>Difference-of-Convex Algorithm with Frank-Wolfe</span></a></li><li><a class="tocitem" href="#Index"><span>Index</span></a></li></ul></li><li><a class="tocitem" href="../2_lmo/">Linear Minimization Oracles</a></li><li><a class="tocitem" href="../3_backend/">Utilities and data structures</a></li><li><a class="tocitem" href="../4_linesearch/">Line search and step size settings</a></li><li><a class="tocitem" href="../5_gradient_descent/">Adaptive Proximal Gradient Descent Methods</a></li></ul></li><li><a class="tocitem" href="../../contributing/">Contributing</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">API reference</a></li><li class="is-active"><a href>Algorithms</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Algorithms</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/ZIB-IOL/FrankWolfe.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/ZIB-IOL/FrankWolfe.jl/blob/master/docs/src/reference/1_algorithms.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Algorithms"><a class="docs-heading-anchor" href="#Algorithms">Algorithms</a><a id="Algorithms-1"></a><a class="docs-heading-anchor-permalink" href="#Algorithms" title="Permalink"></a></h1><p>This section contains all main algorithms of the package. These are the ones typical users will call.</p><p>The typical signature for these algorithms is:</p><pre><code class="language-julia hljs">my_algorithm(f, grad!, lmo, x0)</code></pre><h2 id="Standard-algorithms"><a class="docs-heading-anchor" href="#Standard-algorithms">Standard algorithms</a><a id="Standard-algorithms-1"></a><a class="docs-heading-anchor-permalink" href="#Standard-algorithms" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="FrankWolfe.frank_wolfe-NTuple{4, Any}" href="#FrankWolfe.frank_wolfe-NTuple{4, Any}"><code>FrankWolfe.frank_wolfe</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">frank_wolfe(f, grad!, lmo, x0; kwargs...)</code></pre><p>Simplest form of the Frank-Wolfe algorithm.</p><p><strong>Common arguments</strong></p><p>These positional arguments are common to most Frank-Wolfe variants:</p><ul><li><code>f</code>: a function <code>f(x)</code> computing the value of the objective to minimize at point <code>x</code></li><li><code>grad!</code>: a function <code>grad!(g, x)</code> overwriting <code>g</code> with the gradient of <code>f</code> at point <code>x</code></li><li><code>lmo</code>: a linear minimization oracle, subtyping <a href="../2_lmo/#FrankWolfe.LinearMinimizationOracle"><code>LinearMinimizationOracle</code></a></li><li><code>x0</code>: a starting point for the optimization (will be modified in-place for <code>frank_wolfe</code> with <code>InplaceEmphasis</code>)</li></ul><p><strong>Common keyword arguments</strong></p><p>These keyword arguments are common to most Frank-Wolfe variants.</p><div class="admonition is-warning" id="Warning-8d349c160860f6b8"><header class="admonition-header">Warning<a class="admonition-anchor" href="#Warning-8d349c160860f6b8" title="Permalink"></a></header><div class="admonition-body"><p>The current variant may have additional keyword arguments, documented elsewhere, or it may only use a subset of the ones listed below. The default values of these arguments may also vary between variants, and thus are not part of the public API.</p></div></div><ul><li><code>line_search::LineSearchMethod</code>: an object specifying the line search and its parameters (see <a href="../4_linesearch/#FrankWolfe.LineSearchMethod"><code>LineSearchMethod</code></a>)</li><li><code>momentum::Union{Real,Nothing}=nothing</code>: constant momentum to apply to the gradient</li><li><code>epsilon::Real</code>: absolute dual gap threshold at which the algorithm is interrupted</li><li><code>max_iteration::Integer</code>: maximum number of iterations after which the algorithm is interrupted</li><li><code>print_iter::Integer</code>: interval between two consecutive log prints, expressed in number of iterations</li><li><code>trajectory::Bool=false</code>: whether to record the trajectory of algorithm states (through callbacks)</li><li><code>verbose::Bool</code>: whether to print periodic logs (through callbacks)</li><li><code>memory_mode::MemoryEmphasis</code>: an object dictating whether the algorithm operates in-place or out-of-place (see <a href="@ref"><code>MemoryEmphasis</code></a>)</li><li><code>gradient=nothing</code>: pre-allocated container for the gradient</li><li><code>callback=nothing</code>: function called on a <a href="../3_backend/#FrankWolfe.CallbackState"><code>CallbackState</code></a> at each iteration</li><li><code>traj_data=[]</code>: pre-allocated storage for the trajectory of algorithm states</li><li><code>timeout::Real=Inf</code>: maximum time after which the algorithm is interrupted (in nanoseconds)</li><li><code>linesearch_workspace=nothing</code>: pre-allocated workspace for the line search </li><li><code>dual_gap_compute_frequency::Integer=1</code>: frequency of dual gap computation</li><li><code>x_container=nothing</code>: pre-allocated container for the iterates of the algorithm</li><li><code>d_container=nothing</code>: pre-allocated container for the update directions computed by the algorithm</li></ul><p><strong>Return</strong></p><p>Returns a named tuple <code>(; x, v, primal, dual_gap, status, traj_data)</code> with:</p><ul><li><code>x</code>: the final iterate</li><li><code>v</code>: the last vertex from the linear minimization oracle</li><li><code>primal</code>: the final primal value <code>f(x)</code></li><li><code>dual_gap</code>: the final Frank-Wolfe gap</li><li><code>status</code>: the <code>ExecutionStatus</code> with which the algorithm terminated</li><li><code>traj_data</code>: a vector of trajectory information, each element being the output of <a href="@ref"><code>callback_state</code></a>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/ZIB-IOL/FrankWolfe.jl/blob/fd1cf67dd875fe4273b7bb036765ea168c0ce4fd/src/fw_algorithms.jl#L2-L12">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="FrankWolfe.lazified_conditional_gradient-NTuple{4, Any}" href="#FrankWolfe.lazified_conditional_gradient-NTuple{4, Any}"><code>FrankWolfe.lazified_conditional_gradient</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">lazified_conditional_gradient(f, grad!, lmo_base, x0; kwargs...)</code></pre><p>Similar to <a href="#FrankWolfe.frank_wolfe-NTuple{4, Any}"><code>FrankWolfe.frank_wolfe</code></a> but lazyfying the LMO: each call is stored in a cache, which is looked up first for a good-enough direction. The cache used is a <a href="../2_lmo/#FrankWolfe.MultiCacheLMO"><code>FrankWolfe.MultiCacheLMO</code></a> or a <a href="../2_lmo/#FrankWolfe.VectorCacheLMO"><code>FrankWolfe.VectorCacheLMO</code></a> depending on whether the provided <code>cache_size</code> option is finite.</p><p><strong>Common arguments</strong></p><p>These positional arguments are common to most Frank-Wolfe variants:</p><ul><li><code>f</code>: a function <code>f(x)</code> computing the value of the objective to minimize at point <code>x</code></li><li><code>grad!</code>: a function <code>grad!(g, x)</code> overwriting <code>g</code> with the gradient of <code>f</code> at point <code>x</code></li><li><code>lmo</code>: a linear minimization oracle, subtyping <a href="../2_lmo/#FrankWolfe.LinearMinimizationOracle"><code>LinearMinimizationOracle</code></a></li><li><code>x0</code>: a starting point for the optimization (will be modified in-place for <code>frank_wolfe</code> with <code>InplaceEmphasis</code>)</li></ul><p><strong>Common keyword arguments</strong></p><p>These keyword arguments are common to most Frank-Wolfe variants.</p><div class="admonition is-warning" id="Warning-8d349c160860f6b8"><header class="admonition-header">Warning<a class="admonition-anchor" href="#Warning-8d349c160860f6b8" title="Permalink"></a></header><div class="admonition-body"><p>The current variant may have additional keyword arguments, documented elsewhere, or it may only use a subset of the ones listed below. The default values of these arguments may also vary between variants, and thus are not part of the public API.</p></div></div><ul><li><code>line_search::LineSearchMethod</code>: an object specifying the line search and its parameters (see <a href="../4_linesearch/#FrankWolfe.LineSearchMethod"><code>LineSearchMethod</code></a>)</li><li><code>momentum::Union{Real,Nothing}=nothing</code>: constant momentum to apply to the gradient</li><li><code>epsilon::Real</code>: absolute dual gap threshold at which the algorithm is interrupted</li><li><code>max_iteration::Integer</code>: maximum number of iterations after which the algorithm is interrupted</li><li><code>print_iter::Integer</code>: interval between two consecutive log prints, expressed in number of iterations</li><li><code>trajectory::Bool=false</code>: whether to record the trajectory of algorithm states (through callbacks)</li><li><code>verbose::Bool</code>: whether to print periodic logs (through callbacks)</li><li><code>memory_mode::MemoryEmphasis</code>: an object dictating whether the algorithm operates in-place or out-of-place (see <a href="@ref"><code>MemoryEmphasis</code></a>)</li><li><code>gradient=nothing</code>: pre-allocated container for the gradient</li><li><code>callback=nothing</code>: function called on a <a href="../3_backend/#FrankWolfe.CallbackState"><code>CallbackState</code></a> at each iteration</li><li><code>traj_data=[]</code>: pre-allocated storage for the trajectory of algorithm states</li><li><code>timeout::Real=Inf</code>: maximum time after which the algorithm is interrupted (in nanoseconds)</li><li><code>linesearch_workspace=nothing</code>: pre-allocated workspace for the line search </li><li><code>dual_gap_compute_frequency::Integer=1</code>: frequency of dual gap computation</li><li><code>x_container=nothing</code>: pre-allocated container for the iterates of the algorithm</li><li><code>d_container=nothing</code>: pre-allocated container for the update directions computed by the algorithm</li></ul><p><strong>Return</strong></p><p>Returns a named tuple <code>(; x, v, primal, dual_gap, status, traj_data)</code> with:</p><ul><li><code>x</code>: the final iterate</li><li><code>v</code>: the last vertex from the linear minimization oracle</li><li><code>primal</code>: the final primal value <code>f(x)</code></li><li><code>dual_gap</code>: the final Frank-Wolfe gap</li><li><code>status</code>: the <code>ExecutionStatus</code> with which the algorithm terminated</li><li><code>traj_data</code>: a vector of trajectory information, each element being the output of <a href="@ref"><code>callback_state</code></a>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/ZIB-IOL/FrankWolfe.jl/blob/fd1cf67dd875fe4273b7bb036765ea168c0ce4fd/src/fw_algorithms.jl#L272-L285">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="FrankWolfe.block_coordinate_frank_wolfe" href="#FrankWolfe.block_coordinate_frank_wolfe"><code>FrankWolfe.block_coordinate_frank_wolfe</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">block_coordinate_frank_wolfe(f, grad!, lmo::ProductLMO{N}, x0; kwargs...) where {N}</code></pre><p>Block-coordinate version of the Frank-Wolfe algorithm. Minimizes objective <code>f</code> over the product of feasible domains specified by the <code>lmo</code>. The optional argument the <code>update_order</code> is of type <a href="../3_backend/#FrankWolfe.BlockCoordinateUpdateOrder"><code>FrankWolfe.BlockCoordinateUpdateOrder</code></a> and controls the order in which the blocks are updated. The argument <code>update_step</code> is a single instance or tuple of <a href="../3_backend/#FrankWolfe.UpdateStep"><code>FrankWolfe.UpdateStep</code></a> and defines which FW-algorithms to use to update the iterates in the different blocks.</p><p>See <a href="https://arxiv.org/abs/1207.4747">S. Lacoste-Julien, M. Jaggi, M. Schmidt, and P. Pletscher 2013</a> and <a href="https://arxiv.org/abs/1502.03716">A. Beck, E. Pauwels and S. Sabach 2015</a> for more details about Block-Coordinate Frank-Wolfe.</p><p><strong>Common arguments</strong></p><p>These positional arguments are common to most Frank-Wolfe variants:</p><ul><li><code>f</code>: a function <code>f(x)</code> computing the value of the objective to minimize at point <code>x</code></li><li><code>grad!</code>: a function <code>grad!(g, x)</code> overwriting <code>g</code> with the gradient of <code>f</code> at point <code>x</code></li><li><code>lmo</code>: a linear minimization oracle, subtyping <a href="../2_lmo/#FrankWolfe.LinearMinimizationOracle"><code>LinearMinimizationOracle</code></a></li><li><code>x0</code>: a starting point for the optimization (will be modified in-place for <code>frank_wolfe</code> with <code>InplaceEmphasis</code>)</li></ul><p><strong>Common keyword arguments</strong></p><p>These keyword arguments are common to most Frank-Wolfe variants.</p><div class="admonition is-warning" id="Warning-8d349c160860f6b8"><header class="admonition-header">Warning<a class="admonition-anchor" href="#Warning-8d349c160860f6b8" title="Permalink"></a></header><div class="admonition-body"><p>The current variant may have additional keyword arguments, documented elsewhere, or it may only use a subset of the ones listed below. The default values of these arguments may also vary between variants, and thus are not part of the public API.</p></div></div><ul><li><code>line_search::LineSearchMethod</code>: an object specifying the line search and its parameters (see <a href="../4_linesearch/#FrankWolfe.LineSearchMethod"><code>LineSearchMethod</code></a>)</li><li><code>momentum::Union{Real,Nothing}=nothing</code>: constant momentum to apply to the gradient</li><li><code>epsilon::Real</code>: absolute dual gap threshold at which the algorithm is interrupted</li><li><code>max_iteration::Integer</code>: maximum number of iterations after which the algorithm is interrupted</li><li><code>print_iter::Integer</code>: interval between two consecutive log prints, expressed in number of iterations</li><li><code>trajectory::Bool=false</code>: whether to record the trajectory of algorithm states (through callbacks)</li><li><code>verbose::Bool</code>: whether to print periodic logs (through callbacks)</li><li><code>memory_mode::MemoryEmphasis</code>: an object dictating whether the algorithm operates in-place or out-of-place (see <a href="@ref"><code>MemoryEmphasis</code></a>)</li><li><code>gradient=nothing</code>: pre-allocated container for the gradient</li><li><code>callback=nothing</code>: function called on a <a href="../3_backend/#FrankWolfe.CallbackState"><code>CallbackState</code></a> at each iteration</li><li><code>traj_data=[]</code>: pre-allocated storage for the trajectory of algorithm states</li><li><code>timeout::Real=Inf</code>: maximum time after which the algorithm is interrupted (in nanoseconds)</li><li><code>linesearch_workspace=nothing</code>: pre-allocated workspace for the line search </li><li><code>dual_gap_compute_frequency::Integer=1</code>: frequency of dual gap computation</li><li><code>x_container=nothing</code>: pre-allocated container for the iterates of the algorithm</li><li><code>d_container=nothing</code>: pre-allocated container for the update directions computed by the algorithm</li></ul><p><strong>Return</strong></p><p>The method returns a tuple <code>(x, v, primal, dual_gap, traj_data)</code> with:</p><ul><li><code>x</code> cartesian product of final iterates</li><li><code>v</code> cartesian product of last vertices of the LMOs</li><li><code>primal</code> primal value <code>f(x)</code></li><li><code>dual_gap</code> final Frank-Wolfe gap</li><li><code>status</code> the termination status</li><li><code>traj_data</code> vector of trajectory information.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/ZIB-IOL/FrankWolfe.jl/blob/fd1cf67dd875fe4273b7bb036765ea168c0ce4fd/src/block_coordinate_algorithms.jl#L450-L474">source</a></section></article><h2 id="Active-set-based-methods"><a class="docs-heading-anchor" href="#Active-set-based-methods">Active-set based methods</a><a id="Active-set-based-methods-1"></a><a class="docs-heading-anchor-permalink" href="#Active-set-based-methods" title="Permalink"></a></h2><p>The following algorithms maintain the representation of the iterates as a convex combination of vertices.</p><h3 id="Away-step"><a class="docs-heading-anchor" href="#Away-step">Away-step</a><a id="Away-step-1"></a><a class="docs-heading-anchor-permalink" href="#Away-step" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="FrankWolfe.away_frank_wolfe-NTuple{4, Any}" href="#FrankWolfe.away_frank_wolfe-NTuple{4, Any}"><code>FrankWolfe.away_frank_wolfe</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">away_frank_wolfe(f, grad!, lmo, x0; kwargs...)</code></pre><p>Frank-Wolfe with away steps. The algorithm maintains the current iterate as a convex combination of vertices in the <a href="../3_backend/#FrankWolfe.ActiveSet"><code>FrankWolfe.ActiveSet</code></a> data structure. See <a href="https://arxiv.org/abs/2104.06675">M. Besançon, A. Carderera and S. Pokutta 2021</a> for illustrations of away steps.</p><p><strong>Common arguments</strong></p><p>These positional arguments are common to most Frank-Wolfe variants:</p><ul><li><code>f</code>: a function <code>f(x)</code> computing the value of the objective to minimize at point <code>x</code></li><li><code>grad!</code>: a function <code>grad!(g, x)</code> overwriting <code>g</code> with the gradient of <code>f</code> at point <code>x</code></li><li><code>lmo</code>: a linear minimization oracle, subtyping <a href="../2_lmo/#FrankWolfe.LinearMinimizationOracle"><code>LinearMinimizationOracle</code></a></li><li><code>x0</code>: a starting point for the optimization (will be modified in-place for <code>frank_wolfe</code> with <code>InplaceEmphasis</code>)</li></ul><p><strong>Common keyword arguments</strong></p><p>These keyword arguments are common to most Frank-Wolfe variants.</p><div class="admonition is-warning" id="Warning-8d349c160860f6b8"><header class="admonition-header">Warning<a class="admonition-anchor" href="#Warning-8d349c160860f6b8" title="Permalink"></a></header><div class="admonition-body"><p>The current variant may have additional keyword arguments, documented elsewhere, or it may only use a subset of the ones listed below. The default values of these arguments may also vary between variants, and thus are not part of the public API.</p></div></div><ul><li><code>line_search::LineSearchMethod</code>: an object specifying the line search and its parameters (see <a href="../4_linesearch/#FrankWolfe.LineSearchMethod"><code>LineSearchMethod</code></a>)</li><li><code>momentum::Union{Real,Nothing}=nothing</code>: constant momentum to apply to the gradient</li><li><code>epsilon::Real</code>: absolute dual gap threshold at which the algorithm is interrupted</li><li><code>max_iteration::Integer</code>: maximum number of iterations after which the algorithm is interrupted</li><li><code>print_iter::Integer</code>: interval between two consecutive log prints, expressed in number of iterations</li><li><code>trajectory::Bool=false</code>: whether to record the trajectory of algorithm states (through callbacks)</li><li><code>verbose::Bool</code>: whether to print periodic logs (through callbacks)</li><li><code>memory_mode::MemoryEmphasis</code>: an object dictating whether the algorithm operates in-place or out-of-place (see <a href="@ref"><code>MemoryEmphasis</code></a>)</li><li><code>gradient=nothing</code>: pre-allocated container for the gradient</li><li><code>callback=nothing</code>: function called on a <a href="../3_backend/#FrankWolfe.CallbackState"><code>CallbackState</code></a> at each iteration</li><li><code>traj_data=[]</code>: pre-allocated storage for the trajectory of algorithm states</li><li><code>timeout::Real=Inf</code>: maximum time after which the algorithm is interrupted (in nanoseconds)</li><li><code>linesearch_workspace=nothing</code>: pre-allocated workspace for the line search </li><li><code>dual_gap_compute_frequency::Integer=1</code>: frequency of dual gap computation</li><li><code>x_container=nothing</code>: pre-allocated container for the iterates of the algorithm</li><li><code>d_container=nothing</code>: pre-allocated container for the update directions computed by the algorithm</li></ul><p><strong>Return</strong></p><p>Returns a named tuple <code>(; x, v, primal, dual_gap, status, traj_data, active_set)</code> with:</p><ul><li><code>x</code>: the final iterate</li><li><code>v</code>: the last vertex from the linear minimization oracle</li><li><code>primal</code>: the final primal value <code>f(x)</code></li><li><code>dual_gap</code>: the final Frank-Wolfe gap</li><li><code>status</code>: the <code>ExecutionStatus</code> with which the algorithm terminated</li><li><code>traj_data</code>: a vector of trajectory information, each element being the output of <a href="@ref"><code>callback_state</code></a>.</li><li><code>active_set</code>: the computed active set of vertices, of which the solution is a convex combination</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/ZIB-IOL/FrankWolfe.jl/blob/fd1cf67dd875fe4273b7bb036765ea168c0ce4fd/src/afw.jl#L2-L15">source</a></section></article><h3 id="Pairwise-Frank-Wolfe"><a class="docs-heading-anchor" href="#Pairwise-Frank-Wolfe">Pairwise Frank-Wolfe</a><a id="Pairwise-Frank-Wolfe-1"></a><a class="docs-heading-anchor-permalink" href="#Pairwise-Frank-Wolfe" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="FrankWolfe.blended_pairwise_conditional_gradient-NTuple{4, Any}" href="#FrankWolfe.blended_pairwise_conditional_gradient-NTuple{4, Any}"><code>FrankWolfe.blended_pairwise_conditional_gradient</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">blended_pairwise_conditional_gradient(f, grad!, lmo, x0; kwargs...)</code></pre><p>Implements the BPCG algorithm from <a href="https://arxiv.org/abs/2110.12650">Tsuji, Tanaka, Pokutta (2021)</a>. The method uses an active set of current vertices. Unlike away-step, it transfers weight from an away vertex to another vertex of the active set.</p><p><strong>Common arguments</strong></p><p>These positional arguments are common to most Frank-Wolfe variants:</p><ul><li><code>f</code>: a function <code>f(x)</code> computing the value of the objective to minimize at point <code>x</code></li><li><code>grad!</code>: a function <code>grad!(g, x)</code> overwriting <code>g</code> with the gradient of <code>f</code> at point <code>x</code></li><li><code>lmo</code>: a linear minimization oracle, subtyping <a href="../2_lmo/#FrankWolfe.LinearMinimizationOracle"><code>LinearMinimizationOracle</code></a></li><li><code>x0</code>: a starting point for the optimization (will be modified in-place for <code>frank_wolfe</code> with <code>InplaceEmphasis</code>)</li></ul><p><strong>Common keyword arguments</strong></p><p>These keyword arguments are common to most Frank-Wolfe variants.</p><div class="admonition is-warning" id="Warning-8d349c160860f6b8"><header class="admonition-header">Warning<a class="admonition-anchor" href="#Warning-8d349c160860f6b8" title="Permalink"></a></header><div class="admonition-body"><p>The current variant may have additional keyword arguments, documented elsewhere, or it may only use a subset of the ones listed below. The default values of these arguments may also vary between variants, and thus are not part of the public API.</p></div></div><ul><li><code>line_search::LineSearchMethod</code>: an object specifying the line search and its parameters (see <a href="../4_linesearch/#FrankWolfe.LineSearchMethod"><code>LineSearchMethod</code></a>)</li><li><code>momentum::Union{Real,Nothing}=nothing</code>: constant momentum to apply to the gradient</li><li><code>epsilon::Real</code>: absolute dual gap threshold at which the algorithm is interrupted</li><li><code>max_iteration::Integer</code>: maximum number of iterations after which the algorithm is interrupted</li><li><code>print_iter::Integer</code>: interval between two consecutive log prints, expressed in number of iterations</li><li><code>trajectory::Bool=false</code>: whether to record the trajectory of algorithm states (through callbacks)</li><li><code>verbose::Bool</code>: whether to print periodic logs (through callbacks)</li><li><code>memory_mode::MemoryEmphasis</code>: an object dictating whether the algorithm operates in-place or out-of-place (see <a href="@ref"><code>MemoryEmphasis</code></a>)</li><li><code>gradient=nothing</code>: pre-allocated container for the gradient</li><li><code>callback=nothing</code>: function called on a <a href="../3_backend/#FrankWolfe.CallbackState"><code>CallbackState</code></a> at each iteration</li><li><code>traj_data=[]</code>: pre-allocated storage for the trajectory of algorithm states</li><li><code>timeout::Real=Inf</code>: maximum time after which the algorithm is interrupted (in nanoseconds)</li><li><code>linesearch_workspace=nothing</code>: pre-allocated workspace for the line search </li><li><code>dual_gap_compute_frequency::Integer=1</code>: frequency of dual gap computation</li><li><code>x_container=nothing</code>: pre-allocated container for the iterates of the algorithm</li><li><code>d_container=nothing</code>: pre-allocated container for the update directions computed by the algorithm</li></ul><p><strong>Return</strong></p><p>Returns a named tuple <code>(; x, v, primal, dual_gap, status, traj_data, active_set)</code> with:</p><ul><li><code>x</code>: the final iterate</li><li><code>v</code>: the last vertex from the linear minimization oracle</li><li><code>primal</code>: the final primal value <code>f(x)</code></li><li><code>dual_gap</code>: the final Frank-Wolfe gap</li><li><code>status</code>: the <code>ExecutionStatus</code> with which the algorithm terminated</li><li><code>traj_data</code>: a vector of trajectory information, each element being the output of <a href="@ref"><code>callback_state</code></a>.</li><li><code>active_set</code>: the computed active set of vertices, of which the solution is a convex combination</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/ZIB-IOL/FrankWolfe.jl/blob/fd1cf67dd875fe4273b7bb036765ea168c0ce4fd/src/blended_pairwise.jl#L1-L13">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="FrankWolfe.blended_pairwise_conditional_gradient-Union{Tuple{R}, Tuple{AT}, Tuple{Any, Any, Any, FrankWolfe.AbstractActiveSet{AT, R, IT} where IT}} where {AT, R}" href="#FrankWolfe.blended_pairwise_conditional_gradient-Union{Tuple{R}, Tuple{AT}, Tuple{Any, Any, Any, FrankWolfe.AbstractActiveSet{AT, R, IT} where IT}} where {AT, R}"><code>FrankWolfe.blended_pairwise_conditional_gradient</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">blended_pairwise_conditional_gradient(f, grad!, lmo, active_set::AbstractActiveSet; kwargs...)</code></pre><p>Warm-starts BPCG with a pre-defined <code>active_set</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/ZIB-IOL/FrankWolfe.jl/blob/fd1cf67dd875fe4273b7bb036765ea168c0ce4fd/src/blended_pairwise.jl#L71-L75">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="FrankWolfe.pairwise_frank_wolfe-NTuple{4, Any}" href="#FrankWolfe.pairwise_frank_wolfe-NTuple{4, Any}"><code>FrankWolfe.pairwise_frank_wolfe</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">pairwise_frank_wolfe(f, grad!, lmo, x0; kwargs...)</code></pre><p>Frank-Wolfe with pairwise steps. The algorithm maintains the current iterate as a convex combination of vertices in the <a href="../3_backend/#FrankWolfe.ActiveSet"><code>FrankWolfe.ActiveSet</code></a> data structure. See <a href="https://arxiv.org/abs/2104.06675">M. Besançon, A. Carderera and S. Pokutta 2021</a> for illustrations of away steps.  Unlike away-step, it transfers weight from an away vertex to another vertex.</p><p><strong>Common arguments</strong></p><p>These positional arguments are common to most Frank-Wolfe variants:</p><ul><li><code>f</code>: a function <code>f(x)</code> computing the value of the objective to minimize at point <code>x</code></li><li><code>grad!</code>: a function <code>grad!(g, x)</code> overwriting <code>g</code> with the gradient of <code>f</code> at point <code>x</code></li><li><code>lmo</code>: a linear minimization oracle, subtyping <a href="../2_lmo/#FrankWolfe.LinearMinimizationOracle"><code>LinearMinimizationOracle</code></a></li><li><code>x0</code>: a starting point for the optimization (will be modified in-place for <code>frank_wolfe</code> with <code>InplaceEmphasis</code>)</li></ul><p><strong>Common keyword arguments</strong></p><p>These keyword arguments are common to most Frank-Wolfe variants.</p><div class="admonition is-warning" id="Warning-8d349c160860f6b8"><header class="admonition-header">Warning<a class="admonition-anchor" href="#Warning-8d349c160860f6b8" title="Permalink"></a></header><div class="admonition-body"><p>The current variant may have additional keyword arguments, documented elsewhere, or it may only use a subset of the ones listed below. The default values of these arguments may also vary between variants, and thus are not part of the public API.</p></div></div><ul><li><code>line_search::LineSearchMethod</code>: an object specifying the line search and its parameters (see <a href="../4_linesearch/#FrankWolfe.LineSearchMethod"><code>LineSearchMethod</code></a>)</li><li><code>momentum::Union{Real,Nothing}=nothing</code>: constant momentum to apply to the gradient</li><li><code>epsilon::Real</code>: absolute dual gap threshold at which the algorithm is interrupted</li><li><code>max_iteration::Integer</code>: maximum number of iterations after which the algorithm is interrupted</li><li><code>print_iter::Integer</code>: interval between two consecutive log prints, expressed in number of iterations</li><li><code>trajectory::Bool=false</code>: whether to record the trajectory of algorithm states (through callbacks)</li><li><code>verbose::Bool</code>: whether to print periodic logs (through callbacks)</li><li><code>memory_mode::MemoryEmphasis</code>: an object dictating whether the algorithm operates in-place or out-of-place (see <a href="@ref"><code>MemoryEmphasis</code></a>)</li><li><code>gradient=nothing</code>: pre-allocated container for the gradient</li><li><code>callback=nothing</code>: function called on a <a href="../3_backend/#FrankWolfe.CallbackState"><code>CallbackState</code></a> at each iteration</li><li><code>traj_data=[]</code>: pre-allocated storage for the trajectory of algorithm states</li><li><code>timeout::Real=Inf</code>: maximum time after which the algorithm is interrupted (in nanoseconds)</li><li><code>linesearch_workspace=nothing</code>: pre-allocated workspace for the line search </li><li><code>dual_gap_compute_frequency::Integer=1</code>: frequency of dual gap computation</li><li><code>x_container=nothing</code>: pre-allocated container for the iterates of the algorithm</li><li><code>d_container=nothing</code>: pre-allocated container for the update directions computed by the algorithm</li></ul><p><strong>Return</strong></p><p>Returns a named tuple <code>(; x, v, primal, dual_gap, status, traj_data, active_set)</code> with:</p><ul><li><code>x</code>: the final iterate</li><li><code>v</code>: the last vertex from the linear minimization oracle</li><li><code>primal</code>: the final primal value <code>f(x)</code></li><li><code>dual_gap</code>: the final Frank-Wolfe gap</li><li><code>status</code>: the <code>ExecutionStatus</code> with which the algorithm terminated</li><li><code>traj_data</code>: a vector of trajectory information, each element being the output of <a href="@ref"><code>callback_state</code></a>.</li><li><code>active_set</code>: the computed active set of vertices, of which the solution is a convex combination</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/ZIB-IOL/FrankWolfe.jl/blob/fd1cf67dd875fe4273b7bb036765ea168c0ce4fd/src/pairwise.jl#L2-L16">source</a></section></article><h3 id="Blended-Conditional-Gradient"><a class="docs-heading-anchor" href="#Blended-Conditional-Gradient">Blended Conditional Gradient</a><a id="Blended-Conditional-Gradient-1"></a><a class="docs-heading-anchor-permalink" href="#Blended-Conditional-Gradient" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="FrankWolfe.accelerated_simplex_gradient_descent_over_probability_simplex-Tuple{Any, Any, Any, Any, Any, Any, FrankWolfe.AbstractActiveSet}" href="#FrankWolfe.accelerated_simplex_gradient_descent_over_probability_simplex-Tuple{Any, Any, Any, Any, Any, Any, FrankWolfe.AbstractActiveSet}"><code>FrankWolfe.accelerated_simplex_gradient_descent_over_probability_simplex</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">accelerated_simplex_gradient_descent_over_probability_simplex</code></pre><p>Minimizes an objective function over the unit probability simplex until the Strong-Wolfe gap is below tolerance using Nesterov&#39;s accelerated gradient descent.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/ZIB-IOL/FrankWolfe.jl/blob/fd1cf67dd875fe4273b7bb036765ea168c0ce4fd/src/blended_cg.jl#L721-L727">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="FrankWolfe.blended_conditional_gradient-NTuple{4, Any}" href="#FrankWolfe.blended_conditional_gradient-NTuple{4, Any}"><code>FrankWolfe.blended_conditional_gradient</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">blended_conditional_gradient(f, grad!, lmo, x0; kwargs...)</code></pre><p>Entry point for the Blended Conditional Gradient algorithm. See Braun, Gábor, et al. &quot;Blended conditonal gradients&quot; ICML 2019. The method works on an active set like <a href="#FrankWolfe.away_frank_wolfe-NTuple{4, Any}"><code>FrankWolfe.away_frank_wolfe</code></a>, performing gradient descent over the convex hull of active vertices, removing vertices when their weight drops to 0 and adding new vertices by calling the linear oracle in a lazy fashion.</p><p><strong>Common arguments</strong></p><p>These positional arguments are common to most Frank-Wolfe variants:</p><ul><li><code>f</code>: a function <code>f(x)</code> computing the value of the objective to minimize at point <code>x</code></li><li><code>grad!</code>: a function <code>grad!(g, x)</code> overwriting <code>g</code> with the gradient of <code>f</code> at point <code>x</code></li><li><code>lmo</code>: a linear minimization oracle, subtyping <a href="../2_lmo/#FrankWolfe.LinearMinimizationOracle"><code>LinearMinimizationOracle</code></a></li><li><code>x0</code>: a starting point for the optimization (will be modified in-place for <code>frank_wolfe</code> with <code>InplaceEmphasis</code>)</li></ul><p><strong>Common keyword arguments</strong></p><p>These keyword arguments are common to most Frank-Wolfe variants.</p><div class="admonition is-warning" id="Warning-8d349c160860f6b8"><header class="admonition-header">Warning<a class="admonition-anchor" href="#Warning-8d349c160860f6b8" title="Permalink"></a></header><div class="admonition-body"><p>The current variant may have additional keyword arguments, documented elsewhere, or it may only use a subset of the ones listed below. The default values of these arguments may also vary between variants, and thus are not part of the public API.</p></div></div><ul><li><code>line_search::LineSearchMethod</code>: an object specifying the line search and its parameters (see <a href="../4_linesearch/#FrankWolfe.LineSearchMethod"><code>LineSearchMethod</code></a>)</li><li><code>momentum::Union{Real,Nothing}=nothing</code>: constant momentum to apply to the gradient</li><li><code>epsilon::Real</code>: absolute dual gap threshold at which the algorithm is interrupted</li><li><code>max_iteration::Integer</code>: maximum number of iterations after which the algorithm is interrupted</li><li><code>print_iter::Integer</code>: interval between two consecutive log prints, expressed in number of iterations</li><li><code>trajectory::Bool=false</code>: whether to record the trajectory of algorithm states (through callbacks)</li><li><code>verbose::Bool</code>: whether to print periodic logs (through callbacks)</li><li><code>memory_mode::MemoryEmphasis</code>: an object dictating whether the algorithm operates in-place or out-of-place (see <a href="@ref"><code>MemoryEmphasis</code></a>)</li><li><code>gradient=nothing</code>: pre-allocated container for the gradient</li><li><code>callback=nothing</code>: function called on a <a href="../3_backend/#FrankWolfe.CallbackState"><code>CallbackState</code></a> at each iteration</li><li><code>traj_data=[]</code>: pre-allocated storage for the trajectory of algorithm states</li><li><code>timeout::Real=Inf</code>: maximum time after which the algorithm is interrupted (in nanoseconds)</li><li><code>linesearch_workspace=nothing</code>: pre-allocated workspace for the line search </li><li><code>dual_gap_compute_frequency::Integer=1</code>: frequency of dual gap computation</li><li><code>x_container=nothing</code>: pre-allocated container for the iterates of the algorithm</li><li><code>d_container=nothing</code>: pre-allocated container for the update directions computed by the algorithm</li></ul><p><strong>Return</strong></p><p>Returns a named tuple <code>(; x, v, primal, dual_gap, status, traj_data, active_set)</code> with:</p><ul><li><code>x</code>: the final iterate</li><li><code>v</code>: the last vertex from the linear minimization oracle</li><li><code>primal</code>: the final primal value <code>f(x)</code></li><li><code>dual_gap</code>: the final Frank-Wolfe gap</li><li><code>status</code>: the <code>ExecutionStatus</code> with which the algorithm terminated</li><li><code>traj_data</code>: a vector of trajectory information, each element being the output of <a href="@ref"><code>callback_state</code></a>.</li><li><code>active_set</code>: the computed active set of vertices, of which the solution is a convex combination</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/ZIB-IOL/FrankWolfe.jl/blob/fd1cf67dd875fe4273b7bb036765ea168c0ce4fd/src/blended_cg.jl#L2-L17">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="FrankWolfe.build_reduced_problem-Tuple{AbstractVector{var&quot;#s344&quot;} where var&quot;#s344&quot;&lt;:FrankWolfe.ScaledHotVector, Any, Any, Any, Any}" href="#FrankWolfe.build_reduced_problem-Tuple{AbstractVector{var&quot;#s344&quot;} where var&quot;#s344&quot;&lt;:FrankWolfe.ScaledHotVector, Any, Any, Any, Any}"><code>FrankWolfe.build_reduced_problem</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">build_reduced_problem(atoms::AbstractVector{&lt;:AbstractVector}, hessian, weights, gradient, tolerance)</code></pre><p>Given an active set formed by vectors , a (constant) Hessian and a gradient constructs a quadratic problem over the unit probability simplex that is equivalent to minimizing the original function over the convex hull of the active set. If λ are the barycentric coordinates of dimension equal to the cardinality of the active set, the objective function is:</p><pre><code class="nohighlight hljs">f(λ) = reduced_linear^T λ + 0.5 * λ^T reduced_hessian λ</code></pre><p>In the case where we find that the current iterate has a strong-Wolfe gap over the convex hull of the active set that is below the tolerance we return nothing (as there is nothing to do).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/ZIB-IOL/FrankWolfe.jl/blob/fd1cf67dd875fe4273b7bb036765ea168c0ce4fd/src/blended_cg.jl#L606-L623">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="FrankWolfe.lp_separation_oracle-Tuple{FrankWolfe.LinearMinimizationOracle, FrankWolfe.AbstractActiveSet, Any, Any, Any}" href="#FrankWolfe.lp_separation_oracle-Tuple{FrankWolfe.LinearMinimizationOracle, FrankWolfe.AbstractActiveSet, Any, Any, Any}"><code>FrankWolfe.lp_separation_oracle</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>Returns either a tuple <code>(y, val)</code> with <code>y</code> an atom from the active set satisfying the progress criterion and <code>val</code> the corresponding gap <code>dot(y, direction)</code> or the same tuple with <code>y</code> from the LMO.</p><p><code>inplace_loop</code> controls whether the iterate type allows in-place writes. <code>kwargs</code> are passed on to the LMO oracle.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/ZIB-IOL/FrankWolfe.jl/blob/fd1cf67dd875fe4273b7bb036765ea168c0ce4fd/src/blended_cg.jl#L1132-L1139">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="FrankWolfe.minimize_over_convex_hull!-Union{Tuple{R}, Tuple{AT}, Tuple{Any, Any, Any, FrankWolfe.AbstractActiveSet{AT, R, IT} where IT, Any, Any, Any, Any}} where {AT, R}" href="#FrankWolfe.minimize_over_convex_hull!-Union{Tuple{R}, Tuple{AT}, Tuple{Any, Any, Any, FrankWolfe.AbstractActiveSet{AT, R, IT} where IT, Any, Any, Any, Any}} where {AT, R}"><code>FrankWolfe.minimize_over_convex_hull!</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">minimize_over_convex_hull!</code></pre><p>Given a function f with gradient grad! and an active set active_set this function will minimize the function over the convex hull of the active set until the strong-wolfe gap over the active set is below tolerance.</p><p>It will either directly minimize over the convex hull using simplex gradient descent, or it will transform the problem to barycentric coordinates and minimize over the unit probability simplex using gradient descent or Nesterov&#39;s accelerated gradient descent.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/ZIB-IOL/FrankWolfe.jl/blob/fd1cf67dd875fe4273b7bb036765ea168c0ce4fd/src/blended_cg.jl#L460-L473">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="FrankWolfe.projection_simplex_sort-Tuple{Any}" href="#FrankWolfe.projection_simplex_sort-Tuple{Any}"><code>FrankWolfe.projection_simplex_sort</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">projection_simplex_sort(x; s=1.0)</code></pre><p>Perform a projection onto the probability simplex of radius <code>s</code> using a sorting algorithm.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/ZIB-IOL/FrankWolfe.jl/blob/fd1cf67dd875fe4273b7bb036765ea168c0ce4fd/src/blended_cg.jl#L886-L891">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="FrankWolfe.simplex_gradient_descent_over_convex_hull-Union{Tuple{R}, Tuple{AT}, Tuple{Any, Any, Any, FrankWolfe.AbstractActiveSet{AT, R, IT} where IT, Any, Any, Any, Any}, Tuple{Any, Any, Any, FrankWolfe.AbstractActiveSet{AT, R, IT} where IT, Any, Any, Any, Any, FrankWolfe.MemoryEmphasis}} where {AT, R}" href="#FrankWolfe.simplex_gradient_descent_over_convex_hull-Union{Tuple{R}, Tuple{AT}, Tuple{Any, Any, Any, FrankWolfe.AbstractActiveSet{AT, R, IT} where IT, Any, Any, Any, Any}, Tuple{Any, Any, Any, FrankWolfe.AbstractActiveSet{AT, R, IT} where IT, Any, Any, Any, Any, FrankWolfe.MemoryEmphasis}} where {AT, R}"><code>FrankWolfe.simplex_gradient_descent_over_convex_hull</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">simplex_gradient_descent_over_convex_hull(f, grad!, gradient, active_set, tolerance, t, time_start, non_simplex_iter)</code></pre><p>Minimizes an objective function over the convex hull of the active set until the Strong-Wolfe gap is below tolerance using simplex gradient descent.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/ZIB-IOL/FrankWolfe.jl/blob/fd1cf67dd875fe4273b7bb036765ea168c0ce4fd/src/blended_cg.jl#L930-L936">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="FrankWolfe.simplex_gradient_descent_over_probability_simplex-Tuple{Any, Any, Any, Any, Any, Any, Any, FrankWolfe.AbstractActiveSet}" href="#FrankWolfe.simplex_gradient_descent_over_probability_simplex-Tuple{Any, Any, Any, Any, Any, Any, Any, FrankWolfe.AbstractActiveSet}"><code>FrankWolfe.simplex_gradient_descent_over_probability_simplex</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">simplex_gradient_descent_over_probability_simplex</code></pre><p>Minimizes an objective function over the unit probability simplex until the Strong-Wolfe gap is below tolerance using gradient descent.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/ZIB-IOL/FrankWolfe.jl/blob/fd1cf67dd875fe4273b7bb036765ea168c0ce4fd/src/blended_cg.jl#L814-L819">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="FrankWolfe.strong_frankwolfe_gap-Tuple{Any}" href="#FrankWolfe.strong_frankwolfe_gap-Tuple{Any}"><code>FrankWolfe.strong_frankwolfe_gap</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>Checks the strong Frank-Wolfe gap for the reduced problem.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/ZIB-IOL/FrankWolfe.jl/blob/fd1cf67dd875fe4273b7bb036765ea168c0ce4fd/src/blended_cg.jl#L703-L705">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="FrankWolfe.strong_frankwolfe_gap_probability_simplex-Tuple{Any, Any}" href="#FrankWolfe.strong_frankwolfe_gap_probability_simplex-Tuple{Any, Any}"><code>FrankWolfe.strong_frankwolfe_gap_probability_simplex</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">strong_frankwolfe_gap_probability_simplex</code></pre><p>Compute the Strong-Wolfe gap over the unit probability simplex given a gradient.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/ZIB-IOL/FrankWolfe.jl/blob/fd1cf67dd875fe4273b7bb036765ea168c0ce4fd/src/blended_cg.jl#L906-L911">source</a></section></article><h3 id="Blended-Pairwise-Conditional-Gradient"><a class="docs-heading-anchor" href="#Blended-Pairwise-Conditional-Gradient">Blended Pairwise Conditional Gradient</a><a id="Blended-Pairwise-Conditional-Gradient-1"></a><a class="docs-heading-anchor-permalink" href="#Blended-Pairwise-Conditional-Gradient" title="Permalink"></a></h3><h3 id="Corrective-Frank-Wolfe"><a class="docs-heading-anchor" href="#Corrective-Frank-Wolfe">Corrective Frank-Wolfe</a><a id="Corrective-Frank-Wolfe-1"></a><a class="docs-heading-anchor-permalink" href="#Corrective-Frank-Wolfe" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="FrankWolfe.corrective_frank_wolfe-Union{Tuple{R}, Tuple{AT}, Tuple{Any, Any, Any, FrankWolfe.CorrectiveStep, FrankWolfe.AbstractActiveSet{AT, R, IT} where IT}} where {AT, R}" href="#FrankWolfe.corrective_frank_wolfe-Union{Tuple{R}, Tuple{AT}, Tuple{Any, Any, Any, FrankWolfe.CorrectiveStep, FrankWolfe.AbstractActiveSet{AT, R, IT} where IT}} where {AT, R}"><code>FrankWolfe.corrective_frank_wolfe</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">corrective_frank_wolfe(f, grad!, lmo, corrective_step, active_set::AS; kwargs...)</code></pre><p>A corrective Frank-Wolfe variant with corrective step defined by <code>corrective_step</code>.</p><p>A corrective FW algorithm alternates between a standard FW step at which a vertex is added to the active set and a corrective step at which an update is performed in the convex hull of current vertices. Examples of corrective FW algorithms include blended (pairwise) conditional gradients, away-step Frank-Wolfe, and fully-corrective Frank-Wolfe.</p><p><strong>Common keyword arguments</strong></p><p>These keyword arguments are common to most Frank-Wolfe variants.</p><div class="admonition is-warning" id="Warning-8d349c160860f6b8"><header class="admonition-header">Warning<a class="admonition-anchor" href="#Warning-8d349c160860f6b8" title="Permalink"></a></header><div class="admonition-body"><p>The current variant may have additional keyword arguments, documented elsewhere, or it may only use a subset of the ones listed below. The default values of these arguments may also vary between variants, and thus are not part of the public API.</p></div></div><ul><li><code>line_search::LineSearchMethod</code>: an object specifying the line search and its parameters (see <a href="../4_linesearch/#FrankWolfe.LineSearchMethod"><code>LineSearchMethod</code></a>)</li><li><code>momentum::Union{Real,Nothing}=nothing</code>: constant momentum to apply to the gradient</li><li><code>epsilon::Real</code>: absolute dual gap threshold at which the algorithm is interrupted</li><li><code>max_iteration::Integer</code>: maximum number of iterations after which the algorithm is interrupted</li><li><code>print_iter::Integer</code>: interval between two consecutive log prints, expressed in number of iterations</li><li><code>trajectory::Bool=false</code>: whether to record the trajectory of algorithm states (through callbacks)</li><li><code>verbose::Bool</code>: whether to print periodic logs (through callbacks)</li><li><code>memory_mode::MemoryEmphasis</code>: an object dictating whether the algorithm operates in-place or out-of-place (see <a href="@ref"><code>MemoryEmphasis</code></a>)</li><li><code>gradient=nothing</code>: pre-allocated container for the gradient</li><li><code>callback=nothing</code>: function called on a <a href="../3_backend/#FrankWolfe.CallbackState"><code>CallbackState</code></a> at each iteration</li><li><code>traj_data=[]</code>: pre-allocated storage for the trajectory of algorithm states</li><li><code>timeout::Real=Inf</code>: maximum time after which the algorithm is interrupted (in nanoseconds)</li><li><code>linesearch_workspace=nothing</code>: pre-allocated workspace for the line search </li><li><code>dual_gap_compute_frequency::Integer=1</code>: frequency of dual gap computation</li><li><code>x_container=nothing</code>: pre-allocated container for the iterates of the algorithm</li><li><code>d_container=nothing</code>: pre-allocated container for the update directions computed by the algorithm</li></ul><p><strong>Return</strong></p><p>Returns a named tuple <code>(; x, v, primal, dual_gap, status, traj_data, active_set)</code> with:</p><ul><li><code>x</code>: the final iterate</li><li><code>v</code>: the last vertex from the linear minimization oracle</li><li><code>primal</code>: the final primal value <code>f(x)</code></li><li><code>dual_gap</code>: the final Frank-Wolfe gap</li><li><code>status</code>: the <code>ExecutionStatus</code> with which the algorithm terminated</li><li><code>traj_data</code>: a vector of trajectory information, each element being the output of <a href="@ref"><code>callback_state</code></a>.</li><li><code>active_set</code>: the computed active set of vertices, of which the solution is a convex combination</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/ZIB-IOL/FrankWolfe.jl/blob/fd1cf67dd875fe4273b7bb036765ea168c0ce4fd/src/corrective_frankwolfe.jl#L2-L13">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="FrankWolfe.AwayStep" href="#FrankWolfe.AwayStep"><code>FrankWolfe.AwayStep</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">(Lazified) away-step for corrective Frank-Wolfe</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/ZIB-IOL/FrankWolfe.jl/blob/fd1cf67dd875fe4273b7bb036765ea168c0ce4fd/src/corrective_step_interface.jl#L20-L22">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="FrankWolfe.HybridPairAwayStep" href="#FrankWolfe.HybridPairAwayStep"><code>FrankWolfe.HybridPairAwayStep</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>Compares a pairwise and away step and chooses the one with most progress. The line search is computed for both steps. If one step incurs a drop, it is favored, otherwise the one decreasing the primal value the most is favored.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/ZIB-IOL/FrankWolfe.jl/blob/fd1cf67dd875fe4273b7bb036765ea168c0ce4fd/src/corrective_step_interface.jl#L362-L366">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="FrankWolfe.PairwiseStep" href="#FrankWolfe.PairwiseStep"><code>FrankWolfe.PairwiseStep</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>Computes a classic pairwise step, i.e., <code>d = v^FW - v^away</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/ZIB-IOL/FrankWolfe.jl/blob/fd1cf67dd875fe4273b7bb036765ea168c0ce4fd/src/corrective_step_interface.jl#L627-L629">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="FrankWolfe.prepare_corrective_step" href="#FrankWolfe.prepare_corrective_step"><code>FrankWolfe.prepare_corrective_step</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">prepare_corrective_step(corrective_step::CS, f, grad!, gradient, active_set, t, lmo, primal, phi_value, tot_time) -&gt; (should_compute_vertex, use_corrective)</code></pre><p><code>should_compute_vertex</code> is a boolean flag deciding whether a new vertex should be computed. <code>use_corrective</code> is a function that takes the vertex (the vertex is a valid new vertex only if should<em>compute</em>vertex was true)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/ZIB-IOL/FrankWolfe.jl/blob/fd1cf67dd875fe4273b7bb036765ea168c0ce4fd/src/corrective_step_interface.jl#L12-L17">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="FrankWolfe.run_corrective_step" href="#FrankWolfe.run_corrective_step"><code>FrankWolfe.run_corrective_step</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">run_corrective_step(corrective_step, f, grad!, gradient, x, active_set, t, lmo, line_search, linesearch_workspace, primal, phi_value, tot_time, callback, renorm_interval) -&gt; (x, phi_value, primal)</code></pre><p>Corrective step method specific to the <code>CS</code> corrective<em>step type. The corrective step can perform whatever update over the current active set, the function should return the new iterate  a FW step should be run next with the boolean `should</em>fw<em>step<code>and compute a new dual gap estimate</code>phi</em>value`.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/ZIB-IOL/FrankWolfe.jl/blob/fd1cf67dd875fe4273b7bb036765ea168c0ce4fd/src/corrective_step_interface.jl#L4-L9">source</a></section></article><h2 id="Alternating-Methods"><a class="docs-heading-anchor" href="#Alternating-Methods">Alternating Methods</a><a id="Alternating-Methods-1"></a><a class="docs-heading-anchor-permalink" href="#Alternating-Methods" title="Permalink"></a></h2><p>Problems over intersections of convex sets, i.e.</p><p class="math-container">\[\min_{x \in \bigcap_{i=1}^n P_i} f(x),\]</p><p>pose a challenge as one has to combine the information of two or more LMOs.</p><p><a href="#FrankWolfe.alternating_linear_minimization-Union{Tuple{LS}, Tuple{N}, Tuple{Any, Any, Any, Tuple{Vararg{FrankWolfe.LinearMinimizationOracle, N}}, Tuple{Vararg{Any, N}}}} where {N, LS&lt;:Union{Tuple{Vararg{FrankWolfe.LineSearchMethod, N}}, FrankWolfe.LineSearchMethod}}"><code>FrankWolfe.alternating_linear_minimization</code></a> converts the problem over the intersection of sets into a series of subproblems over single sets. To find a point within the intersection, one minimizes both the distance to the iterates of the other subproblems and the original objective function.</p><p><a href="#FrankWolfe.alternating_projections-Union{Tuple{N}, Tuple{Tuple{Vararg{FrankWolfe.LinearMinimizationOracle, N}}, Any}} where N"><code>FrankWolfe.alternating_projections</code></a> solves feasibility problems over intersections of feasible regions.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="FrankWolfe.alternating_linear_minimization-Union{Tuple{LS}, Tuple{N}, Tuple{Any, Any, Any, Tuple{Vararg{FrankWolfe.LinearMinimizationOracle, N}}, Tuple{Vararg{Any, N}}}} where {N, LS&lt;:Union{Tuple{Vararg{FrankWolfe.LineSearchMethod, N}}, FrankWolfe.LineSearchMethod}}" href="#FrankWolfe.alternating_linear_minimization-Union{Tuple{LS}, Tuple{N}, Tuple{Any, Any, Any, Tuple{Vararg{FrankWolfe.LinearMinimizationOracle, N}}, Tuple{Vararg{Any, N}}}} where {N, LS&lt;:Union{Tuple{Vararg{FrankWolfe.LineSearchMethod, N}}, FrankWolfe.LineSearchMethod}}"><code>FrankWolfe.alternating_linear_minimization</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">alternating_linear_minimization(bc_algo::BlockCoordinateMethod, f, grad!, lmos::NTuple{N,LinearMinimizationOracle}, x0; ...) where {N}</code></pre><p>Alternating Linear Minimization minimizes the objective <code>f</code> over the intersections of the feasible domains specified by <code>lmos</code>. The tuple <code>x0</code> defines the initial points for each domain. Returns a tuple <code>(x, v, primal, dual_gap, dist2, traj_data)</code> with:</p><ul><li><code>x</code> cartesian product of final iterates</li><li><code>v</code> cartesian product of last vertices of the LMOs</li><li><code>primal</code> primal value <code>f(x)</code></li><li><code>dual_gap</code> final Frank-Wolfe gap</li><li><code>status</code> is the termination status</li><li><code>dist2</code> is 1/2 of the sum of squared, pairwise distances between iterates</li><li><code>traj_data</code> vector of trajectory information.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/ZIB-IOL/FrankWolfe.jl/blob/fd1cf67dd875fe4273b7bb036765ea168c0ce4fd/src/alternating_methods.jl#L33-L46">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="FrankWolfe.alternating_projections-Union{Tuple{N}, Tuple{Tuple{Vararg{FrankWolfe.LinearMinimizationOracle, N}}, Any}} where N" href="#FrankWolfe.alternating_projections-Union{Tuple{N}, Tuple{Tuple{Vararg{FrankWolfe.LinearMinimizationOracle, N}}, Any}} where N"><code>FrankWolfe.alternating_projections</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">alternating_projections(lmos::NTuple{N,LinearMinimizationOracle}, x0; ...) where {N}</code></pre><p>Computes a point in the intersection of feasible domains specified by <code>lmos</code>. Returns a named tuple <code>(; x, v, dual_gap, dist2, status, traj_data)</code> with:</p><ul><li><code>x</code> cartesian product of final iterates</li><li><code>v</code> cartesian product of last vertices of the LMOs</li><li><code>dual_gap</code> final Frank-Wolfe gap</li><li><code>dist2</code> is 1/2 * sum of squared, pairwise distances between iterates</li><li><code>status</code> the ExecutionStatus of the algorithm</li><li><code>traj_data</code> vector of trajectory information.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/ZIB-IOL/FrankWolfe.jl/blob/fd1cf67dd875fe4273b7bb036765ea168c0ce4fd/src/alternating_methods.jl#L227-L238">source</a></section></article><h2 id="Difference-of-Convex-Algorithm-with-Frank-Wolfe"><a class="docs-heading-anchor" href="#Difference-of-Convex-Algorithm-with-Frank-Wolfe">Difference-of-Convex Algorithm with Frank-Wolfe</a><a id="Difference-of-Convex-Algorithm-with-Frank-Wolfe-1"></a><a class="docs-heading-anchor-permalink" href="#Difference-of-Convex-Algorithm-with-Frank-Wolfe" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="FrankWolfe._boost_line_search_basic-Tuple{Any, Any, Any}" href="#FrankWolfe._boost_line_search_basic-Tuple{Any, Any, Any}"><code>FrankWolfe._boost_line_search_basic</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">_boost_line_search_basic(objective_function, x_old, x_new; kwargs...)</code></pre><p>Simple line search for the boosted DCA variant to find optimal convex combination.</p><p>Finds α* ∈ [0,1] that minimizes φ(α·x<em>new + (1-α)·x</em>old) where φ is the original objective function. Uses coarse grid search followed by local refinement.</p><p><strong>Arguments</strong></p><ul><li><code>objective_function</code>: Original objective φ(x) = f(x) - g(x)</li><li><code>x_old</code>: Previous iterate x_t</li><li><code>x_new</code>: New iterate from DCA subproblem  </li><li><code>max_iter</code>: Maximum refinement iterations (default: 20)</li><li><code>shrink_factor</code>: Step size reduction factor (default: 0.5)</li><li><code>min_step</code>: Minimum step size for refinement (default: 1e-6)</li></ul><p><strong>Returns</strong></p><ul><li><code>α*</code>: Optimal mixing parameter in [0,1]</li></ul><p><strong>Note</strong></p><p>This is a simple implementation suitable for non-convex objectives. More sophisticated line search methods could be substituted.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/ZIB-IOL/FrankWolfe.jl/blob/fd1cf67dd875fe4273b7bb036765ea168c0ce4fd/src/dca.jl#L80-L102">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="FrankWolfe._dca_linearized_gradient!-NTuple{4, Any}" href="#FrankWolfe._dca_linearized_gradient!-NTuple{4, Any}"><code>FrankWolfe._dca_linearized_gradient!</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">_dca_linearized_gradient!(storage, x, grad_f!, grad_g_at_xt)</code></pre><p>Compute the gradient of the DCA linearized objective: ∇m(x) = ∇f(x) - ∇g(x_t).</p><p><strong>Arguments</strong></p><ul><li><code>storage</code>: Pre-allocated array to store the gradient result</li><li><code>x</code>: Point where to evaluate the gradient  </li><li><code>grad_f!</code>: Gradient function for f (modifies storage in-place)</li><li><code>grad_g_at_xt</code>: Precomputed gradient ∇g(x_t) (constant for the subproblem)</li></ul><p><strong>Side Effects</strong></p><ul><li>Modifies <code>storage</code> to contain ∇m(x) = ∇f(x) - ∇g(x_t)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/ZIB-IOL/FrankWolfe.jl/blob/fd1cf67dd875fe4273b7bb036765ea168c0ce4fd/src/dca.jl#L60-L73">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="FrankWolfe._dca_linearized_objective-NTuple{5, Any}" href="#FrankWolfe._dca_linearized_objective-NTuple{5, Any}"><code>FrankWolfe._dca_linearized_objective</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">_dca_linearized_objective(x, f, g_value_at_xt, grad_g_at_xt, grad_g_dot_xt)</code></pre><p>Compute the DCA linearized objective function m(x) = f(x) - g(x<em>t) - ⟨∇g(x</em>t), x - x_t⟩.</p><p>This is mathematically equivalent to: m(x) = f(x) - g(x<em>t) - ⟨∇g(x</em>t), x⟩ + ⟨∇g(x<em>t), x</em>t⟩</p><p>where the terms g(x<em>t) and ⟨∇g(x</em>t), x_t⟩ are constants for the optimization.</p><p><strong>Arguments</strong></p><ul><li><code>x</code>: Current point where to evaluate the function</li><li><code>f</code>: Original convex function f</li><li><code>g_value_at_xt</code>: Precomputed value g(x_t)  </li><li><code>grad_g_at_xt</code>: Precomputed gradient ∇g(x_t)</li><li><code>grad_g_dot_xt</code>: Precomputed dot product ⟨∇g(x<em>t), x</em>t⟩</li></ul><p><strong>Returns</strong></p><ul><li>Value of the linearized objective m(x)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/ZIB-IOL/FrankWolfe.jl/blob/fd1cf67dd875fe4273b7bb036765ea168c0ce4fd/src/dca.jl#L36-L55">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="FrankWolfe.dca_fw-NTuple{6, Any}" href="#FrankWolfe.dca_fw-NTuple{6, Any}"><code>FrankWolfe.dca_fw</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">dca_fw(f, grad_f!, g, grad_g!, lmo, x0; kwargs...)</code></pre><p>Difference of Convex Algorithm with Frank-Wolfe (DCA-FW) for minimizing φ(x) = f(x) - g(x).</p><p>This algorithm solves non-convex optimization problems where the objective can be written as the difference of two convex functions. At each iteration, it:</p><ol><li>Linearizes the concave part g around the current point x_t</li><li>Solves the convex subproblem min<em>x f(x) - ⟨∇g(x</em>t), x⟩ using Frank-Wolfe  </li><li>Updates x_{t+1} to the subproblem solution</li></ol><p><strong>Algorithm Parameters</strong></p><ul><li><code>f</code>: Convex function (differentiable)</li><li><code>grad_f!</code>: In-place gradient of f  </li><li><code>g</code>: Convex function (differentiable) - will be subtracted from f</li><li><code>grad_g!</code>: In-place gradient of g</li><li><code>lmo</code>: Linear Minimization Oracle for the constraint set</li><li><code>x0</code>: Initial point (must be feasible)</li></ul><p><strong>Optimization Parameters</strong></p><ul><li><code>epsilon</code>: Convergence tolerance for DCA gap (default: 1e-7)</li><li><code>max_iteration</code>: Maximum outer DCA iterations (default: 10000)</li><li><code>max_inner_iteration</code>: Maximum inner Frank-Wolfe iterations (default: 1000)</li><li><code>line_search</code>: Line search method for inner Frank-Wolfe (default: Agnostic())</li></ul><p><strong>Algorithm Variants</strong></p><ul><li><code>use_corrective_fw</code>: Use corrective FW based on an active set instead of standard Frank-Wolfe (default: true)</li><li><code>use_dca_early_stopping</code>: Enable early stopping based on DCA optimality (default: true)</li><li><code>boosted</code>: Use boosted variant with convex combinations (default: false)</li></ul><p><strong>Output Control</strong></p><ul><li><code>verbose</code>: Print algorithm progress (default: false)</li><li><code>verbose_inner</code>: Print inner Frank-Wolfe progress (default: false)</li><li><code>print_iter</code>: Print frequency for outer iterations (default: 10)</li><li><code>trajectory</code>: Store trajectory data (default: false)</li><li><code>timeout</code>: Maximum wall-clock time in seconds (default: Inf)</li></ul><p><strong>Memory Management</strong></p><ul><li><code>memory_mode</code>: InplaceEmphasis() or OutplaceEmphasis() (default: InplaceEmphasis())</li><li><code>grad_f_workspace</code>: Pre-allocated gradient workspace (optional)</li><li><code>grad_g_workspace</code>: Pre-allocated gradient workspace (optional) </li><li><code>effective_grad_workspace</code>: Pre-allocated effective gradient workspace (optional)</li></ul><p><strong>Advanced Options</strong></p><ul><li><code>callback</code>: Custom callback function (optional)</li><li><code>traj_data</code>: External trajectory storage (default: [])</li><li><code>linesearch_workspace</code>: Workspace for line search (optional)</li><li><code>boost_line_search</code>: Custom line search for boosted variant (optional)</li></ul><p><strong>Returns</strong></p><p>Named tuple with:</p><ul><li><code>x</code>: Final iterate x_T</li><li><code>primal</code>: Final objective value φ(x<em>T) = f(x</em>T) - g(x_T)  </li><li><code>dca_gap</code>: Final DCA gap estimate</li><li><code>iterations</code>: Number of outer iterations performed</li><li><code>status</code>: <code>ExecutionStatus</code> with which the algorithm terminated</li><li><code>traj_data</code>: Trajectory data (if trajectory=true)</li></ul><p><strong>DCA Gap Definition</strong></p><p>The DCA gap is defined as:     DCA<em>gap = φ(x</em>t) - m(x<em>{t+1}) + FW</em>gap(x_{t+1})</p><p>where:</p><ul><li>φ(x<em>t) = f(x</em>t) - g(x_t) is the original objective at current point</li><li>m(x_{t+1}) is the linearized objective at the new point  </li><li>FW<em>gap(x</em>{t+1}) is the Frank-Wolfe gap at the new point</li></ul><p>This gap measures progress toward DCA stationarity and converges to 0.</p><p><strong>References</strong></p><ul><li>Pokutta, S. (2025). Scalable DC Optimization via Adaptive Frank-Wolfe Algorithms. https://arxiv.org/abs/2507.17545</li><li>Maskan, H., Hou, Y., Sra, S., and Yurtsever, A. (2025). Revisiting Frank-Wolfe for Structured Nonconvex Optimization. https://arxiv.org/abs/2503.08921</li><li>Pham Dinh, T., &amp; Le Thi, H. A. (1997). Convex analysis approach to DC programming</li><li>Beck, A., &amp; Guttmann-Beck, N. (2019). FW-DCA for non-convex regularized problems</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/ZIB-IOL/FrankWolfe.jl/blob/fd1cf67dd875fe4273b7bb036765ea168c0ce4fd/src/dca.jl#L212-L287">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="FrankWolfe.make_dca_early_stopping_callback-NTuple{4, Any}" href="#FrankWolfe.make_dca_early_stopping_callback-NTuple{4, Any}"><code>FrankWolfe.make_dca_early_stopping_callback</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">make_dca_early_stopping_callback(wrapped_callback, linearized_obj, grad_linearized_obj!, phi_value_at_xt)</code></pre><p>Create a callback function that implements early stopping for DCA subproblems.</p><p>The early stopping criterion is based on the DCA optimality condition:     m(x) ≤ φ(x_t) - ⟨∇m(x), x - v⟩</p><p>where:</p><ul><li>m(x) is the linearized objective function</li><li>φ(x<em>t) = f(x</em>t) - g(x<em>t) is the original objective at x</em>t  </li><li>x is the current iterate in the Frank-Wolfe subproblem</li><li>v is the Frank-Wolfe vertex (extreme point)</li></ul><p><strong>Arguments</strong></p><ul><li><code>wrapped_callback</code>: Optional callback to wrap (can be <code>nothing</code>)</li><li><code>linearized_obj</code>: Linearized objective function m(x)</li><li><code>grad_linearized_obj!</code>: Gradient of linearized objective ∇m(x)</li><li><code>phi_value_at_xt</code>: Value φ(x<em>t) = f(x</em>t) - g(x_t), constant for the subproblem</li></ul><p><strong>Returns</strong></p><ul><li>Callback function that returns <code>false</code> (stop) if criterion is satisfied, <code>true</code> otherwise</li></ul><p><strong>Mathematical Note</strong></p><p>The criterion can be rearranged as: m(x) + ⟨∇m(x), x - v⟩ ≤ φ(x_t) This checks if the current point satisfies the DCA stationarity condition.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/ZIB-IOL/FrankWolfe.jl/blob/fd1cf67dd875fe4273b7bb036765ea168c0ce4fd/src/dca.jl#L156-L182">source</a></section></article><h2 id="Index"><a class="docs-heading-anchor" href="#Index">Index</a><a id="Index-1"></a><a class="docs-heading-anchor-permalink" href="#Index" title="Permalink"></a></h2><ul></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../0_reference/">« API Reference</a><a class="docs-footer-nextpage" href="../2_lmo/">Linear Minimization Oracles »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.15.0 on <span class="colophon-date" title="Thursday 13 November 2025 14:39">Thursday 13 November 2025</span>. Using Julia version 1.6.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>

var documenterSearchIndex = {"docs":
[{"location":"contributing/","page":"Contributing","title":"Contributing","text":"EditURL = \"https://github.com/ZIB-IOL/FrankWolfe.jl/blob/master/CONTRIBUTING.md\"","category":"page"},{"location":"contributing/#Contributing-to-FrankWolfe","page":"Contributing","title":"Contributing to FrankWolfe","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"First, thanks for taking the time to contribute. Contributions in any form, such as documentation, bug fix, examples or algorithms, are appreciated and welcome.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"We list below some guidelines to help you contribute to the package.","category":"page"},{"location":"contributing/#Table-of-Contents","page":"Contributing","title":"Table of Contents","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Contributing to FrankWolfe\nCommunity Standards\nWhere can I get an overview\nI just have a question\nHow can I file an issue\nHow can I contribute\nImprove the documentation\nProvide a new example or test\nProvide a new feature\nCode style","category":"page"},{"location":"contributing/#Community-Standards","page":"Contributing","title":"Community Standards","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Interactions on this repository must follow the Julia Community Standards including Pull Requests and issues.","category":"page"},{"location":"contributing/#Where-can-I-get-an-overview","page":"Contributing","title":"Where can I get an overview","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Check out the paper presenting the package for a high-level overview of the feature and algorithms and the documentation for more details.","category":"page"},{"location":"contributing/#I-just-have-a-question","page":"Contributing","title":"I just have a question","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"If your question is related to Julia, its syntax or tooling, the best places to get help will be tied to the Julia community, see the Julia community page for a number of communication channels.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"For now, the best way to ask a question is to reach out to Mathieu Besançon or Sebastian Pokutta. You can also ask your question on discourse.julialang.org in the optimization topic or on the Julia Slack on #mathematical-optimization, see the Julia community page to gain access.","category":"page"},{"location":"contributing/#How-can-I-file-an-issue","page":"Contributing","title":"How can I file an issue","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"If you found a bug or want to propose a feature, we track our issues within the GitHub repository. Once opened, you can edit the issue or add new comments to continue the conversation.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"If you encounter a bug, send the stack trace (the lines appearing after the error occurred containing some source files) and ideally a Minimal Working Example (MWE), a small program that reproduces the bug.","category":"page"},{"location":"contributing/#How-can-I-contribute","page":"Contributing","title":"How can I contribute","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Contributing to the repository will likely be made in a Pull Request (PR). You will need to:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Fork the repository\nClone it on your machine to perform the changes\nCreate a branch for your modifications, based on the branch you want to merge on (typically master)\nPush to this branch on your fork\nThe GitHub web interface will then automatically suggest opening a PR onto the original repository.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"See the GitHub guide to creating PRs for more help on workflows using Git and GitHub.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"A PR should do a single thing to reduce the amount of code that must be reviewed. Do not run the formatter on the whole repository except if your PR is specifically about formatting.","category":"page"},{"location":"contributing/#Improve-the-documentation","page":"Contributing","title":"Improve the documentation","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"The documentation can be improved by changing the files in docs/src, for example to add a section in the documentation, expand a paragraph or add a plot. The documentation attached to a given type of function can be modified in the source files directly, it appears above the thing you try to document with three double quotations mark like this:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"\"\"\"\nThis explains what the function `f` does, it supports markdown.\n\"\"\"\nfunction f(x)\n    # ...\nend","category":"page"},{"location":"contributing/#Provide-a-new-example-or-test","page":"Contributing","title":"Provide a new example or test","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"If you fix a bug, one would typically expect to add a test that validates that the bug is gone. A test would be added in a file in the test/ folder, for which the entry point is runtests.jl.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"The examples/ folder features several examples covering different problem settings and algorithms. The examples are expected to run with the same environment and dependencies as the tests using TestEnv. If the example is lightweight enough, it can be added to the docs/src/examples/ folder which generates pages for the documentation based on Literate.jl.","category":"page"},{"location":"contributing/#Provide-a-new-feature","page":"Contributing","title":"Provide a new feature","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Contributions bringing new features are also welcome. If the feature is likely to impact performance, some benchmarks should be run with BenchmarkTools on several of the examples to assert the effect at different problem sizes. If the feature should only be active in some cases, a keyword should be added to the main algorithms to support it.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Some typical features to implement are:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"A new Linear Minimization Oracle (LMO)\nA new step size\nA new algorithm (less frequent) following the same API.","category":"page"},{"location":"contributing/#Code-style","page":"Contributing","title":"Code style","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"We try to follow the Julia documentation guidelines. We run JuliaFormatter.jl on the repo in the way set in the .JuliaFormatter.toml file, which enforces a number of conventions.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"This contribution guide was inspired by ColPrac and the one in Manopt.jl.","category":"page"},{"location":"examples/4_rational_opt/","page":"Exact Optimization with Rational Arithmetic","title":"Exact Optimization with Rational Arithmetic","text":"EditURL = \"https://github.com/ZIB-IOL/FrankWolfe.jl/blob/master/docs/src/examples/4_rational_opt.jl\"","category":"page"},{"location":"examples/4_rational_opt/#Exact-Optimization-with-Rational-Arithmetic","page":"Exact Optimization with Rational Arithmetic","title":"Exact Optimization with Rational Arithmetic","text":"","category":"section"},{"location":"examples/4_rational_opt/","page":"Exact Optimization with Rational Arithmetic","title":"Exact Optimization with Rational Arithmetic","text":"This example can be found in section 4.3 in the paper. The package allows for exact optimization with rational arithmetic. For this, it suffices to set up the LMO to be rational and choose an appropriate step-size rule as detailed below. For the LMOs included in the package, this simply means initializing the radius with a rational-compatible element type, e.g., 1, rather than a floating-point number, e.g., 1.0. Given that numerators and denominators can become quite large in rational arithmetic, it is strongly advised to base the used rationals on extended-precision integer types such as BigInt, i.e., we use Rational{BigInt}.","category":"page"},{"location":"examples/4_rational_opt/","page":"Exact Optimization with Rational Arithmetic","title":"Exact Optimization with Rational Arithmetic","text":"The second requirement ensuring that the computation runs in rational arithmetic is a rational-compatible step-size rule. The most basic step-size rule compatible with rational optimization is the agnostic step-size rule with gamma_t = 2(2 + t). With this step-size rule, the gradient does not even need to be rational as long as the atom computed by the LMO is of a rational type. Assuming these requirements are met, all iterates and the computed solution will then be rational.","category":"page"},{"location":"examples/4_rational_opt/","page":"Exact Optimization with Rational Arithmetic","title":"Exact Optimization with Rational Arithmetic","text":"using FrankWolfe\nusing LinearAlgebra\n\nn = 100\nk = n\n\nx = fill(big(1)//100, n)\n\nf(x) = dot(x, x)\nfunction grad!(storage, x)\n    @. storage = 2 * x\nend","category":"page"},{"location":"examples/4_rational_opt/","page":"Exact Optimization with Rational Arithmetic","title":"Exact Optimization with Rational Arithmetic","text":"pick feasible region radius needs to be integer or rational","category":"page"},{"location":"examples/4_rational_opt/","page":"Exact Optimization with Rational Arithmetic","title":"Exact Optimization with Rational Arithmetic","text":"lmo = FrankWolfe.ProbabilitySimplexOracle{Rational{BigInt}}(1)","category":"page"},{"location":"examples/4_rational_opt/","page":"Exact Optimization with Rational Arithmetic","title":"Exact Optimization with Rational Arithmetic","text":"compute some initial vertex","category":"page"},{"location":"examples/4_rational_opt/","page":"Exact Optimization with Rational Arithmetic","title":"Exact Optimization with Rational Arithmetic","text":"x0 = FrankWolfe.compute_extreme_point(lmo, zeros(n));\n\nx, v, primal, dual_gap, trajectory = FrankWolfe.frank_wolfe(\n    f,\n    grad!,\n    lmo,\n    x0,\n    max_iteration=k,\n    line_search=FrankWolfe.Agnostic(),\n    print_iter=k / 10,\n    verbose=true,\n    emphasis=FrankWolfe.blas,\n);\n\nprintln(\"\\nOutput type of solution: \", eltype(x))","category":"page"},{"location":"examples/4_rational_opt/","page":"Exact Optimization with Rational Arithmetic","title":"Exact Optimization with Rational Arithmetic","text":"Another possible step-size rule is rationalshortstep which computes the step size by minimizing the smoothness inequality as gamma_t=fraclangle nabla f(x_t)x_t-v_trangle2Lx_t-v_t^2. However, as this step size depends on an upper bound on the Lipschitz constant L as well as the inner product with the gradient nabla f(x_t), both have to be of a rational type.","category":"page"},{"location":"examples/4_rational_opt/","page":"Exact Optimization with Rational Arithmetic","title":"Exact Optimization with Rational Arithmetic","text":"@time x, v, primal, dual_gap, trajectory = FrankWolfe.frank_wolfe(\n    f,\n    grad!,\n    lmo,\n    x0,\n    max_iteration=k,\n    line_search=FrankWolfe.RationalShortstep(),\n    L=2,\n    print_iter=k / 10,\n    verbose=true,\n    emphasis=FrankWolfe.blas,\n);\nnothing #hide","category":"page"},{"location":"examples/4_rational_opt/","page":"Exact Optimization with Rational Arithmetic","title":"Exact Optimization with Rational Arithmetic","text":"Note: at the last step, we exactly close the gap, finding the solution 1//n * ones(n)","category":"page"},{"location":"examples/4_rational_opt/","page":"Exact Optimization with Rational Arithmetic","title":"Exact Optimization with Rational Arithmetic","text":"","category":"page"},{"location":"examples/4_rational_opt/","page":"Exact Optimization with Rational Arithmetic","title":"Exact Optimization with Rational Arithmetic","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/5_blended_cg/","page":"Blended Conditional Gradients","title":"Blended Conditional Gradients","text":"EditURL = \"https://github.com/ZIB-IOL/FrankWolfe.jl/blob/master/docs/src/examples/5_blended_cg.jl\"","category":"page"},{"location":"examples/5_blended_cg/#Blended-Conditional-Gradients","page":"Blended Conditional Gradients","title":"Blended Conditional Gradients","text":"","category":"section"},{"location":"examples/5_blended_cg/","page":"Blended Conditional Gradients","title":"Blended Conditional Gradients","text":"The FW and AFW algorithms, and their lazy variants share one feature: they attempt to make primal progress over a reduced set of vertices. The AFW algorithm does this through away steps (which do not increase the cardinality of the active set), and the lazy variants do this through the use of previously exploited vertices. A third strategy that one can follow is to explicitly blend Frank-Wolfe steps with gradient descent steps over the convex hull of the active set (note that this can be done without requiring a projection oracle over C, thus making the algorithm projection-free). This results in the Blended Conditional Gradient (BCG) algorithm, which attempts to make as much progress as possible through the convex hull of the current active set S_t until it automatically detects that in order to make further progress it requires additional calls to the LMO.","category":"page"},{"location":"examples/5_blended_cg/","page":"Blended Conditional Gradients","title":"Blended Conditional Gradients","text":"See also Blended Conditional Gradients: the unconditioning of conditional gradients, Braun et al, 2019, https://arxiv.org/abs/1805.07311","category":"page"},{"location":"examples/5_blended_cg/","page":"Blended Conditional Gradients","title":"Blended Conditional Gradients","text":"using FrankWolfe\nusing LinearAlgebra\nusing Random\nusing SparseArrays\n\nn = 1000\nk = 10000\n\nRandom.seed!(41)\n\nmatrix = rand(n, n)\nhessian = transpose(matrix) * matrix\nlinear = rand(n)\nf(x) = dot(linear, x) + 0.5 * transpose(x) * hessian * x\nfunction grad!(storage, x)\n    return storage .= linear + hessian * x\nend\nL = eigmax(hessian)","category":"page"},{"location":"examples/5_blended_cg/","page":"Blended Conditional Gradients","title":"Blended Conditional Gradients","text":"We run over the probability simplex and call the LMO to get an initial feasible point:","category":"page"},{"location":"examples/5_blended_cg/","page":"Blended Conditional Gradients","title":"Blended Conditional Gradients","text":"lmo = FrankWolfe.ProbabilitySimplexOracle(1.0);\nx00 = FrankWolfe.compute_extreme_point(lmo, zeros(n))\n\ntarget_tolerance = 1e-5\n\nx0 = deepcopy(x00)\nx, v, primal, dual_gap, trajectoryBCG_accel_simplex = FrankWolfe.blended_conditional_gradient(\n    f,\n    grad!,\n    lmo,\n    x0,\n    epsilon=target_tolerance,\n    max_iteration=k,\n    line_search=FrankWolfe.Adaptive(),\n    print_iter=k / 10,\n    hessian=hessian,\n    emphasis=FrankWolfe.memory,\n    L=L,\n    accelerated=true,\n    verbose=true,\n    trajectory=true,\n    K=1.00,\n    weight_purge_threshold=1e-10,\n)\n\nx0 = deepcopy(x00)\nx, v, primal, dual_gap, trajectoryBCG_simplex = FrankWolfe.blended_conditional_gradient(\n    f,\n    grad!,\n    lmo,\n    x0,\n    epsilon=target_tolerance,\n    max_iteration=k,\n    line_search=FrankWolfe.Adaptive(),\n    print_iter=k / 10,\n    hessian=hessian,\n    emphasis=FrankWolfe.memory,\n    L=L,\n    accelerated=false,\n    verbose=true,\n    trajectory=true,\n    K=1.00,\n    weight_purge_threshold=1e-10,\n)\n\nx0 = deepcopy(x00)\nx, v, primal, dual_gap, trajectoryBCG_convex = FrankWolfe.blended_conditional_gradient(\n    f,\n    grad!,\n    lmo,\n    x0,\n    epsilon=target_tolerance,\n    max_iteration=k,\n    line_search=FrankWolfe.Adaptive(),\n    print_iter=k / 10,\n    emphasis=FrankWolfe.memory,\n    L=L,\n    verbose=true,\n    trajectory=true,\n    K=1.00,\n    weight_purge_threshold=1e-10,\n)\n\ndata = [trajectoryBCG_accel_simplex, trajectoryBCG_simplex, trajectoryBCG_convex]\nlabel = [\"BCG (accel simplex)\", \"BCG (simplex)\", \"BCG (convex)\"]\nFrankWolfe.plot_trajectories(data, label, xscalelog=true)\n\n\n\nmatrix = rand(n, n)\nhessian = transpose(matrix) * matrix\nlinear = rand(n)\nf(x) = dot(linear, x) + 0.5 * transpose(x) * hessian * x + 10\nfunction grad!(storage, x)\n    return storage .= linear + hessian * x\nend\nL = eigmax(hessian)\n\nlmo = FrankWolfe.KSparseLMO(100, 100.0)\nx00 = FrankWolfe.compute_extreme_point(lmo, zeros(n))\n\nx0 = deepcopy(x00)\nx, v, primal, dual_gap, trajectoryBCG_accel_simplex = FrankWolfe.blended_conditional_gradient(\n    f,\n    grad!,\n    lmo,\n    x0,\n    epsilon=target_tolerance,\n    max_iteration=k,\n    line_search=FrankWolfe.Adaptive(),\n    print_iter=k / 10,\n    hessian=hessian,\n    emphasis=FrankWolfe.memory,\n    L=L,\n    accelerated=true,\n    verbose=true,\n    trajectory=true,\n    K=1.00,\n    weight_purge_threshold=1e-10,\n)\n\nx0 = deepcopy(x00)\nx, v, primal, dual_gap, trajectoryBCG_simplex = FrankWolfe.blended_conditional_gradient(\n    f,\n    grad!,\n    lmo,\n    x0,\n    epsilon=target_tolerance,\n    max_iteration=k,\n    line_search=FrankWolfe.Adaptive(),\n    print_iter=k / 10,\n    hessian=hessian,\n    emphasis=FrankWolfe.memory,\n    L=L,\n    accelerated=false,\n    verbose=true,\n    trajectory=true,\n    K=1.00,\n    weight_purge_threshold=1e-10,\n)\n\nx0 = deepcopy(x00)\nx, v, primal, dual_gap, trajectoryBCG_convex = FrankWolfe.blended_conditional_gradient(\n    f,\n    grad!,\n    lmo,\n    x0,\n    epsilon=target_tolerance,\n    max_iteration=k,\n    line_search=FrankWolfe.Adaptive(),\n    print_iter=k / 10,\n    emphasis=FrankWolfe.memory,\n    L=L,\n    verbose=true,\n    trajectory=true,\n    K=1.00,\n    weight_purge_threshold=1e-10,\n)\n\ndata = [trajectoryBCG_accel_simplex, trajectoryBCG_simplex, trajectoryBCG_convex]\nlabel = [\"BCG (accel simplex)\", \"BCG (simplex)\", \"BCG (convex)\"]\nFrankWolfe.plot_trajectories(data, label, xscalelog=true)","category":"page"},{"location":"examples/5_blended_cg/","page":"Blended Conditional Gradients","title":"Blended Conditional Gradients","text":"","category":"page"},{"location":"examples/5_blended_cg/","page":"Blended Conditional Gradients","title":"Blended Conditional Gradients","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/6_spectrahedron/","page":"Spectrahedron","title":"Spectrahedron","text":"EditURL = \"https://github.com/ZIB-IOL/FrankWolfe.jl/blob/master/docs/src/examples/6_spectrahedron.jl\"","category":"page"},{"location":"examples/6_spectrahedron/#Spectrahedron","page":"Spectrahedron","title":"Spectrahedron","text":"","category":"section"},{"location":"examples/6_spectrahedron/","page":"Spectrahedron","title":"Spectrahedron","text":"This example shows an optimization problem over the spectraplex:","category":"page"},{"location":"examples/6_spectrahedron/","page":"Spectrahedron","title":"Spectrahedron","text":"S = X in mathbbS_+^n Tr(X) = 1","category":"page"},{"location":"examples/6_spectrahedron/","page":"Spectrahedron","title":"Spectrahedron","text":"with mathbbS_+^n the set of positive semidefinite matrices. Linear optimization with symmetric objective D over the spetraplex consists in computing the leading eigenvector of D.","category":"page"},{"location":"examples/6_spectrahedron/","page":"Spectrahedron","title":"Spectrahedron","text":"The package also exposes UnitSpectrahedronLMO which corresponds to the feasible set:","category":"page"},{"location":"examples/6_spectrahedron/","page":"Spectrahedron","title":"Spectrahedron","text":"S_u = X in mathbbS_+^n Tr(X) leq 1","category":"page"},{"location":"examples/6_spectrahedron/","page":"Spectrahedron","title":"Spectrahedron","text":"using FrankWolfe\nusing LinearAlgebra\nusing Random\nusing SparseArrays","category":"page"},{"location":"examples/6_spectrahedron/","page":"Spectrahedron","title":"Spectrahedron","text":"The objective function will be the symmetric squared distance to a set of known or observed entries Y_ij of the matrix.","category":"page"},{"location":"examples/6_spectrahedron/","page":"Spectrahedron","title":"Spectrahedron","text":"f(X) = sum_(ij) in L 12 (X_ij - Y_ij)^2","category":"page"},{"location":"examples/6_spectrahedron/#Setting-up-the-input-data,-objective,-and-gradient","page":"Spectrahedron","title":"Setting up the input data, objective, and gradient","text":"","category":"section"},{"location":"examples/6_spectrahedron/","page":"Spectrahedron","title":"Spectrahedron","text":"Dimension, number of iterations and number of known entries:","category":"page"},{"location":"examples/6_spectrahedron/","page":"Spectrahedron","title":"Spectrahedron","text":"n = 500\nk = 10000\nn_entries = 50\n\nRandom.seed!(41)\n\nconst entry_indices = unique!([minmax(rand(1:n, 2)...) for _ in 1:n_entries])\nconst entry_values = randn(length(entry_indices))\n\nfunction f(X)\n    r = zero(eltype(X))\n    for (idx, (i, j)) in enumerate(entry_indices)\n        r += 1/2 * (X[i,j] - entry_values[idx])^2\n        r += 1/2 * (X[j,i] - entry_values[idx])^2\n    end\n    return r\nend\n\nfunction grad!(storage, X)\n    storage .= 0\n    for (idx, (i, j)) in enumerate(entry_indices)\n        storage[i,j] += (X[i,j] - entry_values[idx])\n        storage[j,i] += (X[j,i] - entry_values[idx])\n    end\nend","category":"page"},{"location":"examples/6_spectrahedron/","page":"Spectrahedron","title":"Spectrahedron","text":"Note that the ensure_symmetry = false argument to SpectraplexLMO. It skips an additional step making the used direction symmetric. It is not necessary when the gradient is a LinearAlgebra.Symmetric (or more rarely a LinearAlgebra.Diagonal or LinearAlgebra.UniformScaling).","category":"page"},{"location":"examples/6_spectrahedron/","page":"Spectrahedron","title":"Spectrahedron","text":"const lmo = FrankWolfe.SpectraplexLMO(1.0, n, false)\nconst x0 = FrankWolfe.compute_extreme_point(lmo, spzeros(n, n))\n\ntarget_tolerance = 1e-6;\nnothing #hide","category":"page"},{"location":"examples/6_spectrahedron/#Running-standard-and-lazified-Frank-Wolfe","page":"Spectrahedron","title":"Running standard and lazified Frank-Wolfe","text":"","category":"section"},{"location":"examples/6_spectrahedron/","page":"Spectrahedron","title":"Spectrahedron","text":"Xfinal, Vfinal, primal, dual_gap, trajectory = FrankWolfe.frank_wolfe(\n    f,\n    grad!,\n    lmo,\n    x0,\n    max_iteration=k,\n    line_search=FrankWolfe.MonotonousStepSize(),\n    print_iter=k / 10,\n    emphasis=FrankWolfe.memory,\n    verbose=true,\n    trajectory=true,\n    epsilon=target_tolerance,\n)\n\nXfinal, Vfinal, primal, dual_gap, trajectory_lazy = FrankWolfe.lazified_conditional_gradient(\n    f,\n    grad!,\n    lmo,\n    x0,\n    max_iteration=k,\n    line_search=FrankWolfe.MonotonousStepSize(),\n    print_iter=k / 10,\n    emphasis=FrankWolfe.memory,\n    verbose=true,\n    trajectory=true,\n    epsilon=target_tolerance,\n)","category":"page"},{"location":"examples/6_spectrahedron/#Plotting-the-resulting-trajectories","page":"Spectrahedron","title":"Plotting the resulting trajectories","text":"","category":"section"},{"location":"examples/6_spectrahedron/","page":"Spectrahedron","title":"Spectrahedron","text":"data = [trajectory, trajectory_lazy]\nlabel = [\"FW\", \"LCG\"]\nFrankWolfe.plot_trajectories(data, label, xscalelog=true)","category":"page"},{"location":"examples/6_spectrahedron/","page":"Spectrahedron","title":"Spectrahedron","text":"","category":"page"},{"location":"examples/6_spectrahedron/","page":"Spectrahedron","title":"Spectrahedron","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/3_matrix_completion/","page":"Matrix Completion","title":"Matrix Completion","text":"EditURL = \"https://github.com/ZIB-IOL/FrankWolfe.jl/blob/master/docs/src/examples/3_matrix_completion.jl\"","category":"page"},{"location":"examples/3_matrix_completion/#Matrix-Completion","page":"Matrix Completion","title":"Matrix Completion","text":"","category":"section"},{"location":"examples/3_matrix_completion/","page":"Matrix Completion","title":"Matrix Completion","text":"We present another example that is about matrix completion. The idea is, given a partially observed matrix YinmathbbR^mtimes n, to find XinmathbbR^mtimes n to minimize the sum of squared errors from the observed entries while 'completing' the matrix Y, i.e. filling the unobserved entries to match Y as good as possible. A detailed explanation can be found in section 4.2 of the paper. We will try to solve","category":"page"},{"location":"examples/3_matrix_completion/","page":"Matrix Completion","title":"Matrix Completion","text":"min_X_*le tau sum_(ij)inmathcalI (X_ijiY_ij)^2","category":"page"},{"location":"examples/3_matrix_completion/","page":"Matrix Completion","title":"Matrix Completion","text":"where tau0 and mathcalI denote the indices of the observed entries. We will use FrankWolfe.NuclearNormLMO and compare our Frank-Wolfe implementation with a Projected Gradient Descent (PGD) algorithm which, after each gradient descent step, projects the iterates back onto the nuclear norm ball. We use a movielens dataset for comparison.","category":"page"},{"location":"examples/3_matrix_completion/","page":"Matrix Completion","title":"Matrix Completion","text":"using FrankWolfe\nusing ZipFile, DataFrames, CSV\n\nusing Random\nusing Plots\n\nusing Profile\n\nimport Arpack\nusing SparseArrays, LinearAlgebra\n\nusing LaTeXStrings\n\ntemp_zipfile = download(\"http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\")\n\nzarchive = ZipFile.Reader(temp_zipfile)\n\nmovies_file = zarchive.files[findfirst(f -> occursin(\"movies\", f.name), zarchive.files)]\nmovies_frame = CSV.read(movies_file, DataFrame)\n\nratings_file = zarchive.files[findfirst(f -> occursin(\"ratings\", f.name), zarchive.files)]\nratings_frame = CSV.read(ratings_file, DataFrame)\n\nusers = unique(ratings_frame[:, :userId])\nmovies = unique(ratings_frame[:, :movieId])\n\n@assert users == eachindex(users)\nmovies_revert = zeros(Int, maximum(movies))\nfor (idx, m) in enumerate(movies)\n    movies_revert[m] = idx\nend\nmovies_indices = [movies_revert[idx] for idx in ratings_frame[:, :movieId]]\n\nconst rating_matrix = sparse(\n    ratings_frame[:, :userId],\n    movies_indices,\n    ratings_frame[:, :rating],\n    length(users),\n    length(movies),\n)\n\nmissing_rate = 0.05\n\nRandom.seed!(42)\n\nconst missing_ratings = Tuple{Int,Int}[]\nconst present_ratings = Tuple{Int,Int}[]\nlet\n    (I, J, V) = SparseArrays.findnz(rating_matrix)\n    for idx in eachindex(I)\n        if V[idx] > 0\n            if rand() <= missing_rate\n                push!(missing_ratings, (I[idx], J[idx]))\n            else\n                push!(present_ratings, (I[idx], J[idx]))\n            end\n        end\n    end\nend\n\nfunction f(X)\n    r = 0.0\n    for (i, j) in present_ratings\n        r += 0.5 * (X[i, j] - rating_matrix[i, j])^2\n    end\n    return r\nend\n\nfunction grad!(storage, X)\n    storage .= 0\n    for (i, j) in present_ratings\n        storage[i, j] = X[i, j] - rating_matrix[i, j]\n    end\n    return nothing\nend\n\nfunction test_loss(X)\n    r = 0.0\n    for (i, j) in missing_ratings\n        r += 0.5 * (X[i, j] - rating_matrix[i, j])^2\n    end\n    return r\nend\n\nfunction project_nuclear_norm_ball(X; radius=1.0)\n    U, sing_val, Vt = svd(X)\n    if (sum(sing_val) <= radius)\n        return X, -norm_estimation * U[:, 1] * Vt[:, 1]'\n    end\n    sing_val = FrankWolfe.projection_simplex_sort(sing_val, s=radius)\n    return U * Diagonal(sing_val) * Vt', -norm_estimation * U[:, 1] * Vt[:, 1]'\nend\n\nnorm_estimation = 10 * Arpack.svds(rating_matrix, nsv=1, ritzvec=false)[1].S[1]\n\nconst lmo = FrankWolfe.NuclearNormLMO(norm_estimation)\nconst x0 = FrankWolfe.compute_extreme_point(lmo, ones(size(rating_matrix)))\nconst k = 10\n\nFrankWolfe.benchmark_oracles(\n    f,\n    (str, x) -> grad!(str, x),\n    () -> randn(size(rating_matrix)),\n    lmo;\n    k=10,\n)\n\ngradient = spzeros(size(x0)...)\ngradient_aux = spzeros(size(x0)...)\n\nfunction build_callback(trajectory_arr)\n    return function callback(state)\n        return push!(trajectory_arr, (Tuple(state)[1:5]..., test_loss(state.x)))\n    end\nend","category":"page"},{"location":"examples/3_matrix_completion/","page":"Matrix Completion","title":"Matrix Completion","text":"The smoothness constant is estimated:","category":"page"},{"location":"examples/3_matrix_completion/","page":"Matrix Completion","title":"Matrix Completion","text":"num_pairs = 100\nL_estimate = -Inf\nfor i in 1:num_pairs\n    global L_estimate\n    u1 = rand(size(x0, 1))\n    u1 ./= sum(u1)\n    u1 .*= norm_estimation\n    v1 = rand(size(x0, 2))\n    v1 ./= sum(v1)\n    x = FrankWolfe.RankOneMatrix(u1, v1)\n    u2 = rand(size(x0, 1))\n    u2 ./= sum(u2)\n    u2 .*= norm_estimation\n    v2 = rand(size(x0, 2))\n    v2 ./= sum(v2)\n    y = FrankWolfe.RankOneMatrix(u2, v2)\n    grad!(gradient, x)\n    grad!(gradient_aux, y)\n    new_L = norm(gradient - gradient_aux) / norm(x - y)\n    if new_L > L_estimate\n        L_estimate = new_L\n    end\nend","category":"page"},{"location":"examples/3_matrix_completion/","page":"Matrix Completion","title":"Matrix Completion","text":"We can now perform projected gradient descent:","category":"page"},{"location":"examples/3_matrix_completion/","page":"Matrix Completion","title":"Matrix Completion","text":"xgd = Matrix(x0)\nfunction_values = Float64[]\ntiming_values = Float64[]\nfunction_test_values = Float64[]\n\ntime_start = time_ns()\nfor _ in 1:k\n    f_val = f(xgd)\n    push!(function_values, f_val)\n    push!(function_test_values, test_loss(xgd))\n    push!(timing_values, (time_ns() - time_start) / 1e9)\n    @info f_val\n    grad!(gradient, xgd)\n    xgd_new, vertex = project_nuclear_norm_ball(xgd - gradient / L_estimate, radius=norm_estimation)\n    gamma, _ = FrankWolfe.backtrackingLS(f, gradient, xgd, xgd - xgd_new, 1.0)\n    @. xgd -= gamma * (xgd - xgd_new)\nend\n\ntrajectory_arr_fw = Vector{Tuple{Int64,Float64,Float64,Float64,Float64,Float64}}()\ncallback = build_callback(trajectory_arr_fw)\nxfin, _, _, _, traj_data = FrankWolfe.frank_wolfe(\n    f,\n    grad!,\n    lmo,\n    x0;\n    epsilon=1e-9,\n    max_iteration=10 * k,\n    print_iter=k / 10,\n    verbose=false,\n    linesearch_tol=1e-8,\n    line_search=FrankWolfe.Adaptive(),\n    emphasis=FrankWolfe.memory,\n    gradient=gradient,\n    callback=callback,\n)\n\ntrajectory_arr_lazy = Vector{Tuple{Int64,Float64,Float64,Float64,Float64,Float64}}()\ncallback = build_callback(trajectory_arr_lazy)\nxlazy, _, _, _, _ = FrankWolfe.lazified_conditional_gradient(\n    f,\n    grad!,\n    lmo,\n    x0;\n    epsilon=1e-9,\n    max_iteration=10 * k,\n    print_iter=k / 10,\n    verbose=false,\n    linesearch_tol=1e-8,\n    line_search=FrankWolfe.Adaptive(),\n    emphasis=FrankWolfe.memory,\n    gradient=gradient,\n    callback=callback,\n)\n\n\ntrajectory_arr_lazy_ref = Vector{Tuple{Int64,Float64,Float64,Float64,Float64,Float64}}()\ncallback = build_callback(trajectory_arr_lazy_ref)\nxlazy, _, _, _, _ = FrankWolfe.lazified_conditional_gradient(\n    f,\n    grad!,\n    lmo,\n    x0;\n    epsilon=1e-9,\n    max_iteration=50 * k,\n    print_iter=k / 10,\n    verbose=false,\n    linesearch_tol=1e-8,\n    line_search=FrankWolfe.Adaptive(),\n    emphasis=FrankWolfe.memory,\n    gradient=gradient,\n    callback=callback,\n)\n\nfw_test_values = getindex.(trajectory_arr_fw, 6)\nlazy_test_values = getindex.(trajectory_arr_lazy, 6)\n\nresults = Dict(\"svals_gd\"=>svdvals(xgd),\n\"svals_fw\"=>svdvals(xfin),\n\"svals_lcg\"=>svdvals(xlazy),\n\"fw_test_values\"=>fw_test_values,\n\"lazy_test_values\"=>lazy_test_values,\n\"trajectory_arr_fw\"=>trajectory_arr_fw,\n\"trajectory_arr_lazy\"=>trajectory_arr_lazy,\n\"function_values_gd\"=>function_values,\n\"function_values_test_gd\"=>function_test_values,\n\"timing_values_gd\"=>timing_values,\n\"trajectory_arr_lazy_ref\"=>trajectory_arr_lazy_ref)\n\nref_optimum = results[\"trajectory_arr_lazy_ref\"][end][2]\n\niteration_list = [\n    [x[1] + 1 for x in results[\"trajectory_arr_fw\"]],\n    [x[1] + 1 for x in results[\"trajectory_arr_lazy\"]],\n    collect(1:1:length(results[\"function_values_gd\"])),\n]\ntime_list = [\n    [x[5] for x in results[\"trajectory_arr_fw\"]],\n    [x[5] for x in results[\"trajectory_arr_lazy\"]],\n    results[\"timing_values_gd\"],\n]\nprimal_gap_list = [\n    [x[2] - ref_optimum for x in results[\"trajectory_arr_fw\"]],\n    [x[2] - ref_optimum for x in results[\"trajectory_arr_lazy\"]],\n    [x - ref_optimum for x in results[\"function_values_gd\"]],\n]\ntest_list =\n    [results[\"fw_test_values\"], results[\"lazy_test_values\"], results[\"function_values_test_gd\"]]\n\nlabel = [L\"\\textrm{FW}\", L\"\\textrm{L-CG}\", L\"\\textrm{GD}\"]\n\nFrankWolfe.plot_results(\n    [primal_gap_list, primal_gap_list, test_list, test_list],\n    [iteration_list, time_list, iteration_list, time_list],\n    label,\n    [L\"\\textrm{Iteration}\", L\"\\textrm{Time}\", L\"\\textrm{Iteration}\", L\"\\textrm{Time}\"],\n    [\n        L\"\\textrm{Primal Gap}\",\n        L\"\\textrm{Primal Gap}\",\n        L\"\\textrm{Test Error}\",\n        L\"\\textrm{Test Error}\",\n    ],\n    xscalelog=[:log, :identity, :log, :identity],\n    legend_position=[:bottomleft, nothing, nothing, nothing]\n)","category":"page"},{"location":"examples/3_matrix_completion/","page":"Matrix Completion","title":"Matrix Completion","text":"","category":"page"},{"location":"examples/3_matrix_completion/","page":"Matrix Completion","title":"Matrix Completion","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/2_polynomial_regression/","page":"Polynomial Regression","title":"Polynomial Regression","text":"EditURL = \"https://github.com/ZIB-IOL/FrankWolfe.jl/blob/master/docs/src/examples/2_polynomial_regression.jl\"","category":"page"},{"location":"examples/2_polynomial_regression/#Polynomial-Regression","page":"Polynomial Regression","title":"Polynomial Regression","text":"","category":"section"},{"location":"examples/2_polynomial_regression/","page":"Polynomial Regression","title":"Polynomial Regression","text":"The following example features the LMO for polynomial regression on the ell_1 norm ball. Given input/output pairs x_iy_i_i=1^N and sparse coefficients c_j, where","category":"page"},{"location":"examples/2_polynomial_regression/","page":"Polynomial Regression","title":"Polynomial Regression","text":"y_i=sum_j=1^m c_j f_j(x_i)","category":"page"},{"location":"examples/2_polynomial_regression/","page":"Polynomial Regression","title":"Polynomial Regression","text":"and f_j mathbbR^ntomathbbR, the task is to recover those c_j that are non-zero alongside their corresponding values. Under certain assumptions, this problem can be convexified into","category":"page"},{"location":"examples/2_polynomial_regression/","page":"Polynomial Regression","title":"Polynomial Regression","text":"min_cinmathcalCy-Ac^2","category":"page"},{"location":"examples/2_polynomial_regression/","page":"Polynomial Regression","title":"Polynomial Regression","text":"for a convex set mathcalC. It can also be found as example 4.1 in the paper. In order to evaluate the polynomial, we generate a total of 1000 data points x_i_i=1^N from the standard multivariate Gaussian, with which we will compute the output variables y_i_i=1^N. Before evaluating the polynomial, these points will be contaminated with noise drawn from a standard multivariate Gaussian. We run the away_frank_wolfe and blended_conditional_gradient algorithms, and compare them to Projected Gradient Descent using a smoothness estimate. We will evaluate the output solution on test points drawn in a similar manner as the training points.","category":"page"},{"location":"examples/2_polynomial_regression/","page":"Polynomial Regression","title":"Polynomial Regression","text":"using FrankWolfe\n\nusing LinearAlgebra\nimport Random\n\nusing MultivariatePolynomials\nusing DynamicPolynomials\n\nusing Plots\n\nusing LaTeXStrings\n\nconst N = 10\n\nDynamicPolynomials.@polyvar X[1:15]\n\nconst max_degree = 4\ncoefficient_magnitude = 10\nnoise_magnitude = 1\n\nconst var_monomials = MultivariatePolynomials.monomials(X, 0:max_degree)\n\nRandom.seed!(42)\nconst all_coeffs = map(var_monomials) do m\n    d = MultivariatePolynomials.degree(m)\n    return coefficient_magnitude * rand() .* (rand() .> 0.95 * d / max_degree)\nend\n\nconst true_poly = dot(all_coeffs, var_monomials)\n\nconst training_data = map(1:500) do _\n    x = 0.1 * randn(N)\n    y = MultivariatePolynomials.subs(true_poly, Pair(X, x)) + noise_magnitude * randn()\n    return (x, y.a[1])\nend\n\nconst extended_training_data = map(training_data) do (x, y)\n    x_ext = getproperty.(MultivariatePolynomials.subs.(var_monomials, X => x), :α)\n    return (x_ext, y)\nend\n\nconst test_data = map(1:1000) do _\n    x = 0.4 * randn(N)\n    y = MultivariatePolynomials.subs(true_poly, Pair(X, x)) + noise_magnitude * randn()\n    return (x, y.a[1])\nend\n\nconst extended_test_data = map(test_data) do (x, y)\n    x_ext = getproperty.(MultivariatePolynomials.subs.(var_monomials, X => x), :α)\n    return (x_ext, y)\nend\n\nfunction f(coefficients)\n    return 0.5 / length(extended_training_data) * sum(extended_training_data) do (x, y)\n        return (dot(coefficients, x) - y)^2\n    end\nend\n\nfunction f_test(coefficients)\n    return 0.5 / length(extended_test_data) * sum(extended_test_data) do (x, y)\n        return (dot(coefficients, x) - y)^2\n    end\nend\n\nfunction coefficient_errors(coeffs)\n    return 0.5 * sum(eachindex(all_coeffs)) do idx\n        return (all_coeffs[idx] - coeffs[idx])^2\n    end\nend\n\nfunction grad!(storage, coefficients)\n    storage .= 0\n    for (x, y) in extended_training_data\n        p_i = dot(coefficients, x) - y\n        @. storage += x * p_i\n    end\n    storage ./= length(training_data)\n    return nothing\nend\n\nfunction build_callback(trajectory_arr)\n    return function callback(state)\n        return push!(\n            trajectory_arr,\n            (Tuple(state)[1:5]..., f_test(state.x), coefficient_errors(state.x)),\n        )\n    end\nend\n\ngradient = similar(all_coeffs)\n\nmax_iter = 10000\nrandom_initialization_vector = rand(length(all_coeffs))\n\nlmo = FrankWolfe.LpNormLMO{1}(0.95 * norm(all_coeffs, 1))\n\n# Estimating smoothness parameter\nnum_pairs = 1000\nL_estimate = -Inf\ngradient_aux = similar(gradient)\n\nfor i in 1:num_pairs # hide\n    global L_estimate # hide\n    x = compute_extreme_point(lmo, randn(size(all_coeffs))) # hide\n    y = compute_extreme_point(lmo, randn(size(all_coeffs))) # hide\n    grad!(gradient, x) # hide\n    grad!(gradient_aux, y) # hide\n    new_L = norm(gradient - gradient_aux) / norm(x - y) # hide\n    if new_L > L_estimate # hide\n        L_estimate = new_L # hide\n    end # hide\nend # hide\n\nfunction projnorm1(x, τ)\n    n = length(x)\n    if norm(x, 1) ≤ τ\n        return x\n    end\n    u = abs.(x)\n    # simplex projection\n    bget = false\n    s_indices = sortperm(u, rev=true)\n    tsum = zero(τ)\n\n    @inbounds for i in 1:n-1\n        tsum += u[s_indices[i]]\n        tmax = (tsum - τ) / i\n        if tmax ≥ u[s_indices[i+1]]\n            bget = true\n            break\n        end\n    end\n    if !bget\n        tmax = (tsum + u[s_indices[n]] - τ) / n\n    end\n\n    @inbounds for i in 1:n\n        u[i] = max(u[i] - tmax, 0)\n        u[i] *= sign(x[i])\n    end\n    return u\nend\nxgd = FrankWolfe.compute_extreme_point(lmo, random_initialization_vector) # hide\ntraining_gd = Float64[] # hide\ntest_gd = Float64[] # hide\ncoeff_error = Float64[] # hide\ntime_start = time_ns() # hide\ngd_times = Float64[] # hide\nfor iter in 1:max_iter # hide\n    global xgd # hide\n    grad!(gradient, xgd) # hide\n    xgd = projnorm1(xgd - gradient / L_estimate, lmo.right_hand_side) # hide\n    push!(training_gd, f(xgd)) # hide\n    push!(test_gd, f_test(xgd)) # hide\n    push!(coeff_error, coefficient_errors(xgd)) # hide\n    push!(gd_times, (time_ns() - time_start) * 1e-9) # hide\nend # hide\n\nx00 = FrankWolfe.compute_extreme_point(lmo, random_initialization_vector) # hide\nx0 = deepcopy(x00) # hide\n\ntrajectory_lafw = [] # hide\ncallback = build_callback(trajectory_lafw) # hide\nx_lafw, v, primal, dual_gap, _ = FrankWolfe.away_frank_wolfe( # hide\n    f, # hide\n    grad!, # hide\n    lmo, # hide\n    x0, # hide\n    max_iteration=max_iter, # hide\n    line_search=FrankWolfe.Adaptive(), # hide\n    print_iter=max_iter ÷ 10, # hide\n    emphasis=FrankWolfe.memory, # hide\n    verbose=false, # hide\n    lazy=true, # hide\n    gradient=gradient, # hide\n    callback=callback, # hide\n    L=L_estimate, # hide\n) # hide\n\ntrajectory_bcg = [] # hide\ncallback = build_callback(trajectory_bcg) # hide\nx0 = deepcopy(x00) # hide\nx_bcg, v, primal, dual_gap, _ = FrankWolfe.blended_conditional_gradient( # hide\n    f, # hide\n    grad!, # hide\n    lmo, # hide\n    x0, # hide\n    max_iteration=max_iter, # hide\n    line_search=FrankWolfe.Adaptive(), # hide\n    print_iter=max_iter ÷ 10, # hide\n    emphasis=FrankWolfe.memory, # hide\n    verbose=false, # hide\n    weight_purge_threshold=1e-10, # hide\n    callback=callback, # hide\n    L=L_estimate, # hide\n) # hide\nx0 = deepcopy(x00) # hide\ntrajectory_lafw_ref = [] # hide\ncallback = build_callback(trajectory_lafw_ref) # hide\n_, _, primal_ref, _, _ = FrankWolfe.away_frank_wolfe( # hide\n    f, # hide\n    grad!, # hide\n    lmo, # hide\n    x0, # hide\n    max_iteration=2 * max_iter, # hide\n    line_search=FrankWolfe.Adaptive(), # hide\n    print_iter=max_iter ÷ 10, # hide\n    emphasis=FrankWolfe.memory, # hide\n    verbose=false, # hide\n    lazy=true, # hide\n    gradient=gradient, # hide\n    callback=callback, # hide\n    L=L_estimate, # hide\n) # hide\n\n\nfor i in 1:num_pairs\n    global L_estimate\n    x = compute_extreme_point(lmo, randn(size(all_coeffs)))\n    y = compute_extreme_point(lmo, randn(size(all_coeffs)))\n    grad!(gradient, x)\n    grad!(gradient_aux, y)\n    new_L = norm(gradient - gradient_aux) / norm(x - y)\n    if new_L > L_estimate\n        L_estimate = new_L\n    end\nend","category":"page"},{"location":"examples/2_polynomial_regression/","page":"Polynomial Regression","title":"Polynomial Regression","text":"We can now perform projected gradient descent:","category":"page"},{"location":"examples/2_polynomial_regression/","page":"Polynomial Regression","title":"Polynomial Regression","text":"xgd = FrankWolfe.compute_extreme_point(lmo, random_initialization_vector)\ntraining_gd = Float64[]\ntest_gd = Float64[]\ncoeff_error = Float64[]\ntime_start = time_ns()\ngd_times = Float64[]\nfor iter in 1:max_iter\n    global xgd\n    grad!(gradient, xgd)\n    xgd = projnorm1(xgd - gradient / L_estimate, lmo.right_hand_side)\n    push!(training_gd, f(xgd))\n    push!(test_gd, f_test(xgd))\n    push!(coeff_error, coefficient_errors(xgd))\n    push!(gd_times, (time_ns() - time_start) * 1e-9)\nend\n\nx00 = FrankWolfe.compute_extreme_point(lmo, random_initialization_vector)\nx0 = deepcopy(x00)\n\ntrajectory_lafw = []\ncallback = build_callback(trajectory_lafw)\nx_lafw, v, primal, dual_gap, _ = FrankWolfe.away_frank_wolfe(\n    f,\n    grad!,\n    lmo,\n    x0,\n    max_iteration=max_iter,\n    line_search=FrankWolfe.Adaptive(),\n    print_iter=max_iter ÷ 10,\n    emphasis=FrankWolfe.memory,\n    verbose=false,\n    lazy=true,\n    gradient=gradient,\n    callback=callback,\n    L=L_estimate,\n)\n\ntrajectory_bcg = []\ncallback = build_callback(trajectory_bcg)\n\nx0 = deepcopy(x00)\nx_bcg, v, primal, dual_gap, _ = FrankWolfe.blended_conditional_gradient(\n    f,\n    grad!,\n    lmo,\n    x0,\n    max_iteration=max_iter,\n    line_search=FrankWolfe.Adaptive(),\n    print_iter=max_iter ÷ 10,\n    emphasis=FrankWolfe.memory,\n    verbose=false,\n    weight_purge_threshold=1e-10,\n    callback=callback,\n    L=L_estimate,\n)\n\nx0 = deepcopy(x00)\n\ntrajectory_lafw_ref = []\ncallback = build_callback(trajectory_lafw_ref)\n_, _, primal_ref, _, _ = FrankWolfe.away_frank_wolfe(\n    f,\n    grad!,\n    lmo,\n    x0,\n    max_iteration=2 * max_iter,\n    line_search=FrankWolfe.Adaptive(),\n    print_iter=max_iter ÷ 10,\n    emphasis=FrankWolfe.memory,\n    verbose=false,\n    lazy=true,\n    gradient=gradient,\n    callback=callback,\n    L=L_estimate,\n)\n\niteration_list = [\n    [x[1] + 1 for x in trajectory_lafw],\n    [x[1] + 1 for x in trajectory_bcg],\n    collect(eachindex(training_gd)),\n]\ntime_list = [\n    [x[5] for x in trajectory_lafw],\n    [x[5] for x in trajectory_bcg],\n    gd_times,\n]\nprimal_list = [\n    [x[2] - primal_ref for x in trajectory_lafw],\n    [x[2] - primal_ref for x in trajectory_bcg],\n    [x - primal_ref for x in training_gd],\n]\ntest_list = [\n    [x[6] for x in trajectory_lafw],\n    [x[6] for x in trajectory_bcg],\n    test_gd,\n]\nlabel = [L\"\\textrm{L-AFW}\", L\"\\textrm{BCG}\", L\"\\textrm{GD}\"]\ncoefficient_error_values = [\n    [x[7] for x in trajectory_lafw],\n    [x[7] for x in trajectory_bcg],\n    coeff_error,\n]\n\n\nFrankWolfe.plot_results(\n    [primal_list, primal_list, test_list, test_list],\n    [iteration_list, time_list, iteration_list, time_list],\n    label,\n    [L\"\\textrm{Iteration}\", L\"\\textrm{Time}\", L\"\\textrm{Iteration}\", L\"\\textrm{Time}\"],\n    [L\"\\textrm{Primal Gap}\", L\"\\textrm{Primal Gap}\", L\"\\textrm{Test loss}\", L\"\\textrm{Test loss}\"],\n    xscalelog=[:log, :identity, :log, :identity],\n    legend_position=[:bottomleft, nothing, nothing, nothing],\n)","category":"page"},{"location":"examples/2_polynomial_regression/","page":"Polynomial Regression","title":"Polynomial Regression","text":"","category":"page"},{"location":"examples/2_polynomial_regression/","page":"Polynomial Regression","title":"Polynomial Regression","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/0_fw_visualized/","page":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","title":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","text":"EditURL = \"https://github.com/ZIB-IOL/FrankWolfe.jl/blob/master/docs/src/examples/0_fw_visualized.jl\"","category":"page"},{"location":"examples/0_fw_visualized/#Visualization-of-Frank-Wolfe-running-on-a-2-dimensional-polytope","page":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","title":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","text":"","category":"section"},{"location":"examples/0_fw_visualized/","page":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","title":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","text":"This example provides an intuitive view of the Frank-Wolfe algorithm by running it on a polyhedral set with a quadratic function. The Linear Minimization Oracle (LMO) corresponds to a call to a generic simplex solver from MathOptInterface.jl (MOI).","category":"page"},{"location":"examples/0_fw_visualized/#Import-and-setup","page":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","title":"Import and setup","text":"","category":"section"},{"location":"examples/0_fw_visualized/","page":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","title":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","text":"We first import the necessary packages, including Polyhedra to visualize the feasible set.","category":"page"},{"location":"examples/0_fw_visualized/","page":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","title":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","text":"using LinearAlgebra\nusing FrankWolfe\n\nimport MathOptInterface\nconst MOI = MathOptInterface\nusing GLPK\n\nusing Polyhedra\nusing Plots","category":"page"},{"location":"examples/0_fw_visualized/","page":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","title":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","text":"We can then define the objective function, here the squared distance to a point in the place, and its in-place gradient.","category":"page"},{"location":"examples/0_fw_visualized/","page":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","title":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","text":"n = 2\ny = [3.2, 0.5]\n\nfunction f(x)\n    return 1 / 2 * norm(x - y)^2\nend\nfunction grad!(storage, x)\n    @. storage = x - y\nend","category":"page"},{"location":"examples/0_fw_visualized/#Custom-callback","page":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","title":"Custom callback","text":"","category":"section"},{"location":"examples/0_fw_visualized/","page":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","title":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","text":"FrankWolfe.jl lets users define custom callbacks to record information about each iteration. In that case, the callback will copy the current iterate x, the current vertex v, and the current step size gamma to an array thanks to a closure. We then declare the array and the callback over this array. Each iteration will then push to this array.","category":"page"},{"location":"examples/0_fw_visualized/","page":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","title":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","text":"function build_callback(trajectory_arr)\n    return function callback(state)\n        return push!(trajectory_arr, (copy(state.x), copy(state.v), state.gamma))\n    end\nend\n\niterates_information_vector = []\ncallback = build_callback(iterates_information_vector)","category":"page"},{"location":"examples/0_fw_visualized/#Creating-the-Linear-Minimization-Oracle","page":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","title":"Creating the Linear Minimization Oracle","text":"","category":"section"},{"location":"examples/0_fw_visualized/","page":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","title":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","text":"The LMO is defined as a call to a linear optimization solver, each iteration resets the objective and calls the solver. The linear constraints must be defined only once at the beginning and remain identical along iterations. We use here MathOptInterface directly but the constraints could also be defined with JuMP or Convex.jl.","category":"page"},{"location":"examples/0_fw_visualized/","page":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","title":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","text":"o = GLPK.Optimizer()\nx = MOI.add_variables(o, n)\n\n# −x + y ≤ 2\nc1 = MOI.add_constraint(\n    o,\n    -1.0x[1] + x[2],\n    MOI.LessThan(2.0),\n)\n\n# x + 2 y ≤ 4\nc2 = MOI.add_constraint(\n    o,\n    x[1] + 2.0x[2],\n    MOI.LessThan(4.0),\n)\n\n# −2 x − y ≤ 1\nc3 = MOI.add_constraint(\n    o,\n    -2.0x[1] - x[2],\n    MOI.LessThan(1.0),\n)\n\n# x − 2 y ≤ 2\nc4 = MOI.add_constraint(\n    o,\n    x[1] - 2.0x[2],\n    MOI.LessThan(2.0),\n)\n\n# x ≤ 2\nc5 = MOI.add_constraint(\n    o,\n    x[1] + 0.0x[2],\n    MOI.LessThan(2.0),\n)","category":"page"},{"location":"examples/0_fw_visualized/","page":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","title":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","text":"The LMO is then built by wrapping the current MOI optimizer","category":"page"},{"location":"examples/0_fw_visualized/","page":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","title":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","text":"lmo_moi = FrankWolfe.MathOptLMO(o)","category":"page"},{"location":"examples/0_fw_visualized/#Calling-Frank-Wolfe","page":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","title":"Calling Frank-Wolfe","text":"","category":"section"},{"location":"examples/0_fw_visualized/","page":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","title":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","text":"We can now compute an initial starting point from any direction and call the Frank-Wolfe algorithm. Note that we copy x0 before passing it to the algorithm because it is modified in-place by frank_wolfe.","category":"page"},{"location":"examples/0_fw_visualized/","page":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","title":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","text":"x0 = FrankWolfe.compute_extreme_point(lmo_moi, zeros(n))\n\nxfinal, vfinal, primal_value, dual_gap, traj_data = FrankWolfe.frank_wolfe(\n    f,\n    grad!,\n    lmo_moi,\n    copy(x0),\n    line_search = FrankWolfe.Adaptive(),\n    max_iteration = 10,\n    epsilon=1e-8,\n    callback=callback,\n    verbose=true,\n    print_iter=1,\n)","category":"page"},{"location":"examples/0_fw_visualized/","page":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","title":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","text":"We now collect the iterates and vertices across iterations.","category":"page"},{"location":"examples/0_fw_visualized/","page":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","title":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","text":"iterates = Vector{Vector{Float64}}()\npush!(iterates, x0)\nvertices = Vector{Vector{Float64}}()\nfor s in iterates_information_vector\n    push!(iterates, s[1])\n    push!(vertices, s[2])\nend","category":"page"},{"location":"examples/0_fw_visualized/#Plotting-the-algorithm-run","page":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","title":"Plotting the algorithm run","text":"","category":"section"},{"location":"examples/0_fw_visualized/","page":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","title":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","text":"We define another method for f adapted to plot its contours.","category":"page"},{"location":"examples/0_fw_visualized/","page":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","title":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","text":"function f(x1, x2)\n    x = [x1, x2]\n    return f(x)\nend\n\nxlist = collect(range(-1, 3, step = 0.2))\nylist = collect(range(-1, 3, step = 0.2))\n\nX = repeat(reshape(xlist, 1, :), length(ylist), 1)\nY = repeat(ylist, 1, length(xlist))","category":"page"},{"location":"examples/0_fw_visualized/","page":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","title":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","text":"The feasible space is represented using Polyhedra.","category":"page"},{"location":"examples/0_fw_visualized/","page":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","title":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","text":"h = HalfSpace([-1, 1], 2) ∩\n    HalfSpace([1, 2], 4) ∩\n    HalfSpace([-2, -1], 1) ∩\n    HalfSpace([1, -2], 2) ∩\n    HalfSpace([1, 0], 2)\n\np = polyhedron(h)\n\np1 = contour(xlist, ylist, f, fill = true, line_smoothing = 0.85)\nplot(p1, opacity = 0.5)\nplot!(p, ratio = :equal, opacity = 0.5, label = \"feasible region\", framestyle = :zerolines, legend = true, color=:blue);\nnothing #hide","category":"page"},{"location":"examples/0_fw_visualized/","page":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","title":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","text":"Finally, we add all iterates and vertices to the plot.","category":"page"},{"location":"examples/0_fw_visualized/","page":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","title":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","text":"colors = [\"gold\", \"purple\", \"darkorange2\", \"firebrick3\"]\niterates = unique!(iterates)\nfor i in 1:3\n    scatter!([iterates[i][1]], [iterates[i][2]], label = string(\"x_\", i - 1), markersize = 6, color = colors[i])\nend\nscatter!([last(iterates)[1]], [last(iterates)[2]], label = string(\"x_\", length(iterates) - 1), markersize = 6, color = last(colors))","category":"page"},{"location":"examples/0_fw_visualized/","page":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","title":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","text":"plot chosen vertices","category":"page"},{"location":"examples/0_fw_visualized/","page":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","title":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","text":"scatter!([vertices[1][1]], [vertices[1][2]], m = :diamond, markersize = 6, color = colors[1], label = \"v_1\")\nscatter!([vertices[2][1]], [vertices[2][2]], m = :diamond, markersize = 6, color = colors[2], label = \"v_2\", legend = :outerleft, colorbar = true)","category":"page"},{"location":"examples/0_fw_visualized/","page":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","title":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","text":"","category":"page"},{"location":"examples/0_fw_visualized/","page":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","title":"Visualization of Frank-Wolfe running on a 2-dimensional polytope","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/1_mathopt_lmo/","page":"Comparison with MathOptInterface on a Probability Simplex","title":"Comparison with MathOptInterface on a Probability Simplex","text":"EditURL = \"https://github.com/ZIB-IOL/FrankWolfe.jl/blob/master/docs/src/examples/1_mathopt_lmo.jl\"","category":"page"},{"location":"examples/1_mathopt_lmo/#Comparison-with-MathOptInterface-on-a-Probability-Simplex","page":"Comparison with MathOptInterface on a Probability Simplex","title":"Comparison with MathOptInterface on a Probability Simplex","text":"","category":"section"},{"location":"examples/1_mathopt_lmo/","page":"Comparison with MathOptInterface on a Probability Simplex","title":"Comparison with MathOptInterface on a Probability Simplex","text":"In this example, we project a random point onto a probability simplex with the Frank-Wolfe algorithm using either the specialized LMO defined in the package or a generic LP formulation using MathOptInterface.jl (MOI) and GLPK as underlying LP solver. It can be found as Example 4.4 in the paper.","category":"page"},{"location":"examples/1_mathopt_lmo/","page":"Comparison with MathOptInterface on a Probability Simplex","title":"Comparison with MathOptInterface on a Probability Simplex","text":"using FrankWolfe\n\nusing LinearAlgebra\nusing LaTeXStrings\n\nusing Plots\n\nusing JuMP\nconst MOI = JuMP.MOI\n\nimport GLPK\n\nn = Int(1e3)\nk = 10000\n\nxpi = rand(n);\ntotal = sum(xpi);\nconst xp = xpi ./ total;\n\nf(x) = norm(x - xp)^2\nfunction grad!(storage, x)\n    @. storage = 2 * (x - xp)\n    return nothing\nend\n\nlmo_radius = 2.5\nlmo = FrankWolfe.FrankWolfe.ProbabilitySimplexOracle(lmo_radius)\n\nx00 = FrankWolfe.compute_extreme_point(lmo, zeros(n))\ngradient = collect(x00)\n\nx_lmo, v, primal, dual_gap, trajectory_lmo = FrankWolfe.frank_wolfe(\n    f,\n    grad!,\n    lmo,\n    collect(copy(x00)),\n    max_iteration=k,\n    line_search=FrankWolfe.Shortstep(),\n    L=2,\n    print_iter=k / 10,\n    emphasis=FrankWolfe.memory,\n    verbose=false,\n    trajectory=true,\n);\nnothing #hide","category":"page"},{"location":"examples/1_mathopt_lmo/","page":"Comparison with MathOptInterface on a Probability Simplex","title":"Comparison with MathOptInterface on a Probability Simplex","text":"Create a MathOptInterface Optimizer and build the same linear constraints:","category":"page"},{"location":"examples/1_mathopt_lmo/","page":"Comparison with MathOptInterface on a Probability Simplex","title":"Comparison with MathOptInterface on a Probability Simplex","text":"o = GLPK.Optimizer()\nx = MOI.add_variables(o, n)\n\nfor xi in x\n    MOI.add_constraint(o, xi, MOI.GreaterThan(0.0))\nend\n\nMOI.add_constraint(\n    o,\n    MOI.ScalarAffineFunction(MOI.ScalarAffineTerm.(1.0, x), 0.0),\n    MOI.EqualTo(lmo_radius),\n)\n\nlmo_moi = FrankWolfe.MathOptLMO(o)\n\nx, v, primal, dual_gap, trajectory_moi = FrankWolfe.frank_wolfe(\n    f,\n    grad!,\n    lmo_moi,\n    collect(copy(x00)),\n    max_iteration=k,\n    line_search=FrankWolfe.Shortstep(),\n    L=2,\n    print_iter=k / 10,\n    emphasis=FrankWolfe.memory,\n    verbose=false,\n    trajectory=true,\n);\nnothing #hide","category":"page"},{"location":"examples/1_mathopt_lmo/","page":"Comparison with MathOptInterface on a Probability Simplex","title":"Comparison with MathOptInterface on a Probability Simplex","text":"Alternatively, we can use one of the modelling interfaces based on MOI to formulate the LP. The following example builds the same set of constraints using JuMP:","category":"page"},{"location":"examples/1_mathopt_lmo/","page":"Comparison with MathOptInterface on a Probability Simplex","title":"Comparison with MathOptInterface on a Probability Simplex","text":"m = JuMP.Model(GLPK.Optimizer)\n@variable(m, y[1:n] ≥ 0)\n\n@constraint(m, sum(y) == lmo_radius)\n\nlmo_jump = FrankWolfe.MathOptLMO(m.moi_backend)\n\nx, v, primal, dual_gap, trajectory_jump = FrankWolfe.frank_wolfe(\n    f,\n    grad!,\n    lmo_jump,\n    collect(copy(x00)),\n    max_iteration=k,\n    line_search=FrankWolfe.Shortstep(),\n    L=2,\n    print_iter=k / 10,\n    emphasis=FrankWolfe.memory,\n    verbose=false,\n    trajectory=true,\n);\n\nx_lmo, v, primal, dual_gap, trajectory_lmo_blas = FrankWolfe.frank_wolfe(\n    f,\n    grad!,\n    lmo,\n    x00,\n    max_iteration=k,\n    line_search=FrankWolfe.Shortstep(),\n    L=2,\n    print_iter=k / 10,\n    emphasis=FrankWolfe.blas,\n    verbose=false,\n    trajectory=true,\n);\n\nx, v, primal, dual_gap, trajectory_jump_blas = FrankWolfe.frank_wolfe(\n    f,\n    grad!,\n    lmo_jump,\n    x00,\n    max_iteration=k,\n    line_search=FrankWolfe.Shortstep(),\n    L=2,\n    print_iter=k / 10,\n    emphasis=FrankWolfe.blas,\n    verbose=false,\n    trajectory=true,\n);\n\n\niteration_list = [[x[1] + 1 for x in trajectory_lmo], [x[1] + 1 for x in trajectory_moi]]\ntime_list = [[x[5] for x in trajectory_lmo], [x[5] for x in trajectory_moi]]\nprimal_gap_list = [[x[2] for x in trajectory_lmo], [x[2] for x in trajectory_moi]]\ndual_gap_list = [[x[4] for x in trajectory_lmo], [x[4] for x in trajectory_moi]]\n\nlabel = [L\"\\textrm{Closed-form LMO}\", L\"\\textrm{MOI LMO}\"]\n\nFrankWolfe.plot_results(\n    [primal_gap_list, primal_gap_list, dual_gap_list, dual_gap_list],\n    [iteration_list, time_list, iteration_list, time_list],\n    label,\n    [\"\", \"\", L\"\\textrm{Iteration}\", L\"\\textrm{Time}\"],\n    [L\"\\textrm{Primal Gap}\", \"\", L\"\\textrm{Dual Gap}\", \"\"],\n    xscalelog=[:log, :identity, :log, :identity],\n    yscalelog=[:log, :log, :log, :log],\n    legend_position=[:bottomleft, nothing, nothing, nothing]\n)","category":"page"},{"location":"examples/1_mathopt_lmo/","page":"Comparison with MathOptInterface on a Probability Simplex","title":"Comparison with MathOptInterface on a Probability Simplex","text":"","category":"page"},{"location":"examples/1_mathopt_lmo/","page":"Comparison with MathOptInterface on a Probability Simplex","title":"Comparison with MathOptInterface on a Probability Simplex","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/7_shifted_norm_polytopes/","page":"FrankWolfe for scaled, shifted ell^1 and ell^infty norm balls","title":"FrankWolfe for scaled, shifted ell^1 and ell^infty norm balls","text":"EditURL = \"https://github.com/ZIB-IOL/FrankWolfe.jl/blob/master/docs/src/examples/7_shifted_norm_polytopes.jl\"","category":"page"},{"location":"examples/7_shifted_norm_polytopes/","page":"FrankWolfe for scaled, shifted ell^1 and ell^infty norm balls","title":"FrankWolfe for scaled, shifted ell^1 and ell^infty norm balls","text":"using FrankWolfe\nusing LinearAlgebra\nusing LaTeXStrings\nusing Plots","category":"page"},{"location":"examples/7_shifted_norm_polytopes/#FrankWolfe-for-scaled,-shifted-\\ell1-and-\\ell{\\infty}-norm-balls","page":"FrankWolfe for scaled, shifted ell^1 and ell^infty norm balls","title":"FrankWolfe for scaled, shifted ell^1 and ell^infty norm balls","text":"","category":"section"},{"location":"examples/7_shifted_norm_polytopes/","page":"FrankWolfe for scaled, shifted ell^1 and ell^infty norm balls","title":"FrankWolfe for scaled, shifted ell^1 and ell^infty norm balls","text":"In this example, we run the vanilla FrankWolfe algorithm on a scaled and shifted ell^1 and ell^infty norm ball, using the ScaledBoundL1NormBall and ScaledBoundLInfNormBall LMOs. We shift both onto the point (10) and then scale them by a factor of 2 along the x-axis. We project the point (21) onto the polytopes.","category":"page"},{"location":"examples/7_shifted_norm_polytopes/","page":"FrankWolfe for scaled, shifted ell^1 and ell^infty norm balls","title":"FrankWolfe for scaled, shifted ell^1 and ell^infty norm balls","text":"n = 2\n\nk = 1000\n\nxp = [2.0,1.0]\n\nf(x) = norm(x-xp)^2\n\nfunction grad!(storage,x)\n    @. storage = 2 * (x - xp)\n    return nothing\nend\n\nlower = [-1.0,-1.0]\nupper = [3.0,1.0]\n\nl1 = FrankWolfe.ScaledBoundL1NormBall(lower, upper)\n\nlinf = FrankWolfe.ScaledBoundLInfNormBall(lower, upper)\n\nx1 = FrankWolfe.compute_extreme_point(l1, zeros(n))\ngradient = collect(x1)\n\nx_l1, v_1, primal_1, dual_gap_1, trajectory_1 = FrankWolfe.frank_wolfe(\n    f,\n    grad!,\n    l1,\n    collect(copy(x1)),\n    max_iteration=k,\n    line_search=FrankWolfe.Shortstep(),\n    L=2,\n    print_iter=50,\n    emphasis=FrankWolfe.memory,\n    verbose=true,\n    trajectory=true,\n);\n\nprintln(\"\\nFinal solution: \", x_l1)\n\nx2 = FrankWolfe.compute_extreme_point(linf, zeros(n))\ngradient = collect(x2)\n\nx_linf, v_2, primal_2, dual_gap_2, trajectory_2 = FrankWolfe.frank_wolfe(\n    f,\n    grad!,\n    linf,\n    collect(copy(x2)),\n    max_iteration=k,\n    line_search=FrankWolfe.Shortstep(),\n    L=2,\n    print_iter=50,\n    emphasis=FrankWolfe.memory,\n    verbose=true,\n    trajectory=true,\n);\n\nprintln(\"\\nFinal solution: \", x_linf)","category":"page"},{"location":"examples/7_shifted_norm_polytopes/","page":"FrankWolfe for scaled, shifted ell^1 and ell^infty norm balls","title":"FrankWolfe for scaled, shifted ell^1 and ell^infty norm balls","text":"We plot the polytopes alongside the solutions from above:","category":"page"},{"location":"examples/7_shifted_norm_polytopes/","page":"FrankWolfe for scaled, shifted ell^1 and ell^infty norm balls","title":"FrankWolfe for scaled, shifted ell^1 and ell^infty norm balls","text":"xcoord1 = [1,3,1,-1,1]\nycoord1 = [-1,0,1,0,-1]\n\nxcoord2 = [3,3,-1,-1,3]\nycoord2 = [-1,1,1,-1,-1]\n\nplot(xcoord1, ycoord1, title = \"Visualization of scaled shifted norm balls\", lw = 2, label = L\"\\ell^1 \\textrm{ norm}\")\nplot!(xcoord2, ycoord2, lw = 2, label = L\"\\ell^{\\infty} \\textrm{ norm}\")\nplot!([x_l1[1]], [x_l1[2]], seriestype = :scatter, lw = 5, color = \"blue\", label = L\"\\ell^1 \\textrm{ solution}\")\nplot!([x_linf[1]], [x_linf[2]], seriestype = :scatter, lw = 5, color = \"orange\", label = L\"\\ell^{\\infty} \\textrm{ solution}\", legend = :bottomleft)","category":"page"},{"location":"examples/7_shifted_norm_polytopes/","page":"FrankWolfe for scaled, shifted ell^1 and ell^infty norm balls","title":"FrankWolfe for scaled, shifted ell^1 and ell^infty norm balls","text":"","category":"page"},{"location":"examples/7_shifted_norm_polytopes/","page":"FrankWolfe for scaled, shifted ell^1 and ell^infty norm balls","title":"FrankWolfe for scaled, shifted ell^1 and ell^infty norm balls","text":"This page was generated using Literate.jl.","category":"page"},{"location":"reference/#Algorithms","page":"References","title":"Algorithms","text":"","category":"section"},{"location":"reference/","page":"References","title":"References","text":"This section contains all main algorithms of the FrankWolfe.jl package. These are the ones typical users will call.","category":"page"},{"location":"reference/","page":"References","title":"References","text":"frank_wolfe\nlazified_conditional_gradient\naway_frank_wolfe\nblended_conditional_gradient\nFrankWolfe.stochastic_frank_wolfe","category":"page"},{"location":"reference/#FrankWolfe.frank_wolfe","page":"References","title":"FrankWolfe.frank_wolfe","text":"frank_wolfe(f, grad!, lmo, x0; ...)\n\nSimplest form of the Frank-Wolfe algorithm. Returns a tuple (x, v, primal, dual_gap, traj_data) with:\n\nx final iterate\nv last vertex from the LMO\nprimal primal value f(x)\ndual_gap final Frank-Wolfe gap\ntraj_data vector of trajectory information.\n\n\n\n\n\n","category":"function"},{"location":"reference/#FrankWolfe.lazified_conditional_gradient","page":"References","title":"FrankWolfe.lazified_conditional_gradient","text":"lazified_conditional_gradient\n\nSimilar to frank_wolfe but lazyfying the LMO: each call is stored in a cache, which is looked up first for a good-enough direction. The cache used is a FrankWolfe.MultiCacheLMO or a FrankWolfe.VectorCacheLMO depending on whether the provided cache_size option is finite.\n\n\n\n\n\n","category":"function"},{"location":"reference/#FrankWolfe.away_frank_wolfe","page":"References","title":"FrankWolfe.away_frank_wolfe","text":"away_frank_wolfe\n\nFrank-Wolfe with away steps. The algorithm maintains the current iterate as a convex combination of vertices in the FrankWolfe.ActiveSet data structure. See the paper for illustrations of away steps.\n\n\n\n\n\n","category":"function"},{"location":"reference/#FrankWolfe.blended_conditional_gradient","page":"References","title":"FrankWolfe.blended_conditional_gradient","text":"blended_conditional_gradient(f, grad!, lmo, x0)\n\nEntry point for the Blended Conditional Gradient algorithm. See Braun, Gábor, et al. \"Blended conditonal gradients\" ICML 2019. The method works on an active set like FrankWolfe.away_frank_wolfe, performing gradient descent over the convex hull of active vertices, removing vertices when their weight drops to 0 and adding new vertices by calling the linear oracle in a lazy fashion.\n\n\n\n\n\n","category":"function"},{"location":"reference/#FrankWolfe.stochastic_frank_wolfe","page":"References","title":"FrankWolfe.stochastic_frank_wolfe","text":"stochastic_frank_wolfe(f::StochasticObjective, lmo, x0; ...)\n\nStochastic version of Frank-Wolfe, evaluates the objective and gradient stochastically, implemented through the FrankWolfe.StochasticObjective interface.\n\nKeyword arguments include batch_size to pass a fixed batch_size or a batch_iterator implementing batch_size = FrankWolfe.batchsize_iterate(batch_iterator) for algorithms like Variance-reduced and projection-free stochastic optimization, E Hazan, H Luo, 2016.\n\nSimilarly, a constant momentum can be passed or replaced by a momentum_iterator implementing momentum = FrankWolfe.momentum_iterate(momentum_iterator).\n\n\n\n\n\n","category":"function"},{"location":"reference/#Linear-Minimization-Oracle","page":"References","title":"Linear Minimization Oracle","text":"","category":"section"},{"location":"reference/","page":"References","title":"References","text":"The Linear Minimization Oracle (LMO) is a key component called at each iteration of the FW algorithm. Given din mathcalX, it returns a vertex of the feasible set:","category":"page"},{"location":"reference/","page":"References","title":"References","text":"vin argmin_xin mathcalC langle dx rangle","category":"page"},{"location":"reference/","page":"References","title":"References","text":"FrankWolfe.LinearMinimizationOracle","category":"page"},{"location":"reference/#FrankWolfe.LinearMinimizationOracle","page":"References","title":"FrankWolfe.LinearMinimizationOracle","text":"Supertype for linear minimization oracles.\n\nAll LMOs must implement compute_extreme_point(lmo::LMO, direction) and return a vector v of the appropriate type.\n\n\n\n\n\n","category":"type"},{"location":"reference/","page":"References","title":"References","text":"All of them are subtypes of FrankWolfe.LinearMinimizationOracle and implement the following method:","category":"page"},{"location":"reference/","page":"References","title":"References","text":"compute_extreme_point","category":"page"},{"location":"reference/#FrankWolfe.compute_extreme_point","page":"References","title":"FrankWolfe.compute_extreme_point","text":"compute_extreme_point(lmo::LinearMinimizationOracle, direction; kwargs...)\n\nComputes the point argmin_{v ∈ C} v ⋅ direction with C the set represented by the LMO. Most LMOs feature v as a keyword argument that allows for an in-place computation whenever v is dense. All LMOs should accept keyword arguments that they can ignore.\n\n\n\n\n\n","category":"function"},{"location":"reference/","page":"References","title":"References","text":"The package features the following common LMOs out of the box:","category":"page"},{"location":"reference/","page":"References","title":"References","text":"FrankWolfe.BirkhoffPolytopeLMO\nFrankWolfe.KNormBallLMO\nFrankWolfe.KSparseLMO\nFrankWolfe.L1ballDense\nFrankWolfe.LpNormLMO\nFrankWolfe.NuclearNormLMO\nFrankWolfe.ProbabilitySimplexOracle\nFrankWolfe.ScaledBoundL1NormBall\nFrankWolfe.ScaledBoundLInfNormBall\nFrankWolfe.SpectraplexLMO\nFrankWolfe.UnitSimplexOracle\nFrankWolfe.UnitSpectrahedronLMO\nFrankWolfe.MathOptLMO","category":"page"},{"location":"reference/#FrankWolfe.BirkhoffPolytopeLMO","page":"References","title":"FrankWolfe.BirkhoffPolytopeLMO","text":"BirkhoffPolytopeLMO\n\nThe Birkhoff polytope encodes doubly stochastic matrices. Its extreme vertices are all permutation matrices of side-dimension dimension.\n\n\n\n\n\n","category":"type"},{"location":"reference/#FrankWolfe.KNormBallLMO","page":"References","title":"FrankWolfe.KNormBallLMO","text":"KNormBallLMO{T}(K::Int, right_hand_side::T)\n\nLMO with feasible set being the K-norm ball in the sense of 2010.07243, i.e., the convex hull over the union of an L1-ball with radius τ and an L∞-ball with radius τ/K:\n\nC_{K,τ} = conv { B_1(τ) ∪ B_∞(τ / K) }\n\nwith τ the right_hand_side parameter. The K-norm is defined as the sum of the largest K absolute entries in a vector.\n\n\n\n\n\n","category":"type"},{"location":"reference/#FrankWolfe.KSparseLMO","page":"References","title":"FrankWolfe.KSparseLMO","text":"KSparseLMO{T}(K::Int, right_hand_side::T)\n\nLMO for the K-sparse polytope:\n\nC = B_1(τK) ∩ B_∞(τ)\n\nwith τ the right_hand_side parameter. The LMO results in a vector with the K largest absolute values of direction, taking values -τ sign(x_i).\n\n\n\n\n\n","category":"type"},{"location":"reference/#FrankWolfe.LpNormLMO","page":"References","title":"FrankWolfe.LpNormLMO","text":"LpNormLMO{T, p}(right_hand_side)\n\nLMO with feasible set being an L-p norm ball:\n\nC = {x ∈ R^n, norm(x, p) ≤ right_hand_side}\n\n\n\n\n\n","category":"type"},{"location":"reference/#FrankWolfe.NuclearNormLMO","page":"References","title":"FrankWolfe.NuclearNormLMO","text":"NuclearNormLMO{T}(radius)\n\nLMO over matrices that have a nuclear norm less than radius. The LMO returns the rank-one matrix with singular value radius.\n\n\n\n\n\n","category":"type"},{"location":"reference/#FrankWolfe.ProbabilitySimplexOracle","page":"References","title":"FrankWolfe.ProbabilitySimplexOracle","text":"ProbabilitySimplexOracle(right_side)\n\nRepresents the scaled probability simplex:\n\nC = {x ∈ R^n_+, ∑x = right_side}\n\n\n\n\n\n","category":"type"},{"location":"reference/#FrankWolfe.ScaledBoundL1NormBall","page":"References","title":"FrankWolfe.ScaledBoundL1NormBall","text":"ScaledBoundL1NormBall(lower_bounds, upper_bounds)\n\nPolytope similar to a L1-ball with shifted bounds. It is the convex hull of two scaled and shifted unit vectors for each axis (shifted to the center of the polytope, i.e., the elementwise midpoint of the bounds). Lower and upper bounds are passed on as abstract vectors, possibly of different types. For the standard L1-ball, all lower and upper bounds would be -1 and 1.\n\n\n\n\n\n","category":"type"},{"location":"reference/#FrankWolfe.ScaledBoundLInfNormBall","page":"References","title":"FrankWolfe.ScaledBoundLInfNormBall","text":"ScaledBoundLInfNormBall(lower_bounds, upper_bounds)\n\nPolytope similar to a L-inf-ball with shifted bounds or general box constraints. Lower- and upper-bounds are passed on as abstract vectors, possibly of different types. For the standard L-inf ball, all lower- and upper-bounds would be -1 and 1.\n\n\n\n\n\n","category":"type"},{"location":"reference/#FrankWolfe.SpectraplexLMO","page":"References","title":"FrankWolfe.SpectraplexLMO","text":"SpectraplexLMO{T,M}(radius::T,gradient_container::M,ensure_symmetry::Bool=true)\n\nFeasible set\n\n{X ∈ 𝕊_n^+, trace(X) == radius}\n\ngradient_container is used to store the symmetrized negative direction. ensure_symmetry indicates whether the linear function is made symmetric before computing the eigenvector.\n\n\n\n\n\n","category":"type"},{"location":"reference/#FrankWolfe.UnitSimplexOracle","page":"References","title":"FrankWolfe.UnitSimplexOracle","text":"UnitSimplexOracle(right_side)\n\nRepresents the scaled unit simplex:\n\nC = {x ∈ R^n_+, ∑x ≤ right_side}\n\n\n\n\n\n","category":"type"},{"location":"reference/#FrankWolfe.UnitSpectrahedronLMO","page":"References","title":"FrankWolfe.UnitSpectrahedronLMO","text":"UnitSpectrahedronLMO{T,M}(radius::T, gradient_container::M)\n\nFeasible set of PSD matrices with bounded trace:\n\n{X ∈ 𝕊_n^+, trace(X) ≤ radius}\n\ngradient_container is used to store the symmetrized negative direction. ensure_symmetry indicates whether the linear function is made symmetric before computing the eigenvector.\n\n\n\n\n\n","category":"type"},{"location":"reference/#FrankWolfe.MathOptLMO","page":"References","title":"FrankWolfe.MathOptLMO","text":"MathOptLMO{OT <: MOI.Optimizer} <: LinearMinimizationOracle\n\nLinear minimization oracle with feasible space defined through a MathOptInterface.Optimizer. The oracle call sets the direction and reruns the optimizer.\n\nThe direction vector has to be set in the same order of variables as the MOI.ListOfVariableIndices() getter.\n\nThe Boolean use_modify determines if the objective incompute_extreme_point is updated with  MOI.modify(o, ::MOI.ObjectiveFunction, ::MOI.ScalarCoefficientChange) or with MOI.set(o, ::MOI.ObjectiveFunction, f).  use_modify = true decreases the runtime and memory allocation for models created as an optimizer object and defined directly  with MathOptInterface. use_modify = false should be used for CachingOptimizers.\n\n\n\n\n\n","category":"type"},{"location":"reference/","page":"References","title":"References","text":"It also contains some meta-LMOs wrapping another one with extended behavior:","category":"page"},{"location":"reference/","page":"References","title":"References","text":"FrankWolfe.CachedLinearMinimizationOracle\nFrankWolfe.ProductLMO\nFrankWolfe.SingleLastCachedLMO\nFrankWolfe.MultiCacheLMO\nFrankWolfe.VectorCacheLMO","category":"page"},{"location":"reference/#FrankWolfe.CachedLinearMinimizationOracle","page":"References","title":"FrankWolfe.CachedLinearMinimizationOracle","text":"CachedLinearMinimizationOracle{LMO}\n\nOracle wrapping another one of type lmo. Subtypes of CachedLinearMinimizationOracle contain a cache of previous solutions.\n\nBy convention, the inner oracle is named inner. Cached optimizers are expected to implement Base.empty! and Base.length.\n\n\n\n\n\n","category":"type"},{"location":"reference/#FrankWolfe.ProductLMO","page":"References","title":"FrankWolfe.ProductLMO","text":"ProductLMO(lmos...)\n\nLinear minimization oracle over the Cartesian product of multiple LMOs.\n\n\n\n\n\n","category":"type"},{"location":"reference/#FrankWolfe.SingleLastCachedLMO","page":"References","title":"FrankWolfe.SingleLastCachedLMO","text":"SingleLastCachedLMO{LMO, VT}\n\nCaches only the last result from an LMO and stores it in last_vertex. Vertices of LMO have to be of type VT if provided.\n\n\n\n\n\n","category":"type"},{"location":"reference/#FrankWolfe.MultiCacheLMO","page":"References","title":"FrankWolfe.MultiCacheLMO","text":"MultiCacheLMO{N, LMO, A}\n\nCache for a LMO storing up to N vertices in the cache, removed in FIFO style. oldest_idx keeps track of the oldest index in the tuple, i.e. to replace next. VT, if provided, must be the type of vertices returned by LMO\n\n\n\n\n\n","category":"type"},{"location":"reference/#FrankWolfe.VectorCacheLMO","page":"References","title":"FrankWolfe.VectorCacheLMO","text":"VectorCacheLMO{N, LMO, VT}\n\nCache for a LMO storing an unbounded number of vertices of type VT in the cache. VT, if provided, must be the type of vertices returned by LMO\n\n\n\n\n\n","category":"type"},{"location":"reference/","page":"References","title":"References","text":"See Combettes, Pokutta 2021 for references on most LMOs implemented in the package and their comparison with projection operators.","category":"page"},{"location":"reference/#Functions-and-Structures","page":"References","title":"Functions and Structures","text":"","category":"section"},{"location":"reference/","page":"References","title":"References","text":"compute_extreme_point(lmo::FrankWolfe.ProductLMO, direction::Tuple; kwargs...)\ncompute_extreme_point(lmo::FrankWolfe.ProductLMO{N},direction::AbstractArray;storage=similar(direction),direction_indices,kwargs...,) where {N}\ncompute_extreme_point(lmo::FrankWolfe.UnitSimplexOracle{T}, direction) where {T}\nFrankWolfe.compute_dual_solution(::FrankWolfe.UnitSimplexOracle{T}, direction, primalSolution) where {T}\ncompute_extreme_point(lmo::FrankWolfe.ProbabilitySimplexOracle{T}, direction; kwargs...) where {T}\nFrankWolfe.compute_dual_solution(::FrankWolfe.ProbabilitySimplexOracle{T},direction,primal_solution;kwargs...,) where {T}\ncompute_extreme_point(lmo::FrankWolfe.NuclearNormLMO, direction::AbstractMatrix; tol=1e-8, kwargs...)\nFrankWolfe.convert_mathopt","category":"page"},{"location":"reference/#FrankWolfe.compute_extreme_point-Tuple{FrankWolfe.ProductLMO, Tuple}","page":"References","title":"FrankWolfe.compute_extreme_point","text":"compute_extreme_point(lmo::ProductLMO, direction::Tuple; kwargs...)\n\nExtreme point computation on Cartesian product, with a direction (d1, d2, ...) given as a tuple of directions. All keyword arguments are passed to all LMOs.\n\n\n\n\n\n","category":"method"},{"location":"reference/#FrankWolfe.compute_extreme_point-Union{Tuple{N}, Tuple{FrankWolfe.ProductLMO{N, TL} where TL<:Tuple{Vararg{FrankWolfe.LinearMinimizationOracle, N}}, AbstractArray}} where N","page":"References","title":"FrankWolfe.compute_extreme_point","text":"compute_extreme_point(lmo::ProductLMO, direction::AbstractArray; direction_indices, storage=similar(direction))\n\nExtreme point computation, with a direction array and direction_indices provided such that: direction[direction_indices[i]] is passed to the i-th LMO. The result is stored in the optional storage container.\n\nAll keyword arguments are passed to all LMOs.\n\n\n\n\n\n","category":"method"},{"location":"reference/#FrankWolfe.compute_extreme_point-Union{Tuple{T}, Tuple{FrankWolfe.UnitSimplexOracle{T}, Any}} where T","page":"References","title":"FrankWolfe.compute_extreme_point","text":"LMO for scaled unit simplex: ∑ x_i = τ Returns either vector of zeros or vector with one active value equal to RHS if there exists an improving direction.\n\n\n\n\n\n","category":"method"},{"location":"reference/#FrankWolfe.compute_dual_solution-Union{Tuple{T}, Tuple{FrankWolfe.UnitSimplexOracle{T}, Any, Any}} where T","page":"References","title":"FrankWolfe.compute_dual_solution","text":"Dual costs for a given primal solution to form a primal dual pair for scaled unit simplex. Returns two vectors. The first one is the dual costs associated with the constraints and the second is the reduced costs for the variables.\n\n\n\n\n\n","category":"method"},{"location":"reference/#FrankWolfe.compute_extreme_point-Union{Tuple{T}, Tuple{FrankWolfe.ProbabilitySimplexOracle{T}, Any}} where T","page":"References","title":"FrankWolfe.compute_extreme_point","text":"LMO for scaled probability simplex. Returns a vector with one active value equal to RHS in the most improving (or least degrading) direction.\n\n\n\n\n\n","category":"method"},{"location":"reference/#FrankWolfe.compute_dual_solution-Union{Tuple{T}, Tuple{FrankWolfe.ProbabilitySimplexOracle{T}, Any, Any}} where T","page":"References","title":"FrankWolfe.compute_dual_solution","text":"Dual costs for a given primal solution to form a primal dual pair for scaled probability simplex. Returns two vectors. The first one is the dual costs associated with the constraints and the second is the reduced costs for the variables.\n\n\n\n\n\n","category":"method"},{"location":"reference/#FrankWolfe.compute_extreme_point-Tuple{FrankWolfe.NuclearNormLMO, AbstractMatrix{T} where T}","page":"References","title":"FrankWolfe.compute_extreme_point","text":"Best rank-one approximation using the greatest singular value computed with Arpack.\n\nWarning: this does not work (yet) with all number types, BigFloat and Float16 fail.\n\n\n\n\n\n","category":"method"},{"location":"reference/#FrankWolfe.convert_mathopt","page":"References","title":"FrankWolfe.convert_mathopt","text":"convert_mathopt(lmo::LMO, optimizer::OT; kwargs...) -> MathOptLMO{OT}\n\nConverts the given LMO to its equivalent MathOptInterface representation using optimizer. Must be implemented by LMOs.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Components","page":"References","title":"Components","text":"","category":"section"},{"location":"reference/","page":"References","title":"References","text":"This section gathers all additional relevant components of the package.","category":"page"},{"location":"reference/#Active-set-management","page":"References","title":"Active set management","text":"","category":"section"},{"location":"reference/","page":"References","title":"References","text":"The active set represents an iterate as a convex combination of atoms. It maintains a vector of atoms, the corresponding weights, and the current iterate.","category":"page"},{"location":"reference/","page":"References","title":"References","text":"Modules = [FrankWolfe]\nPages = [\"active_set.jl\"]","category":"page"},{"location":"reference/#FrankWolfe.ActiveSet","page":"References","title":"FrankWolfe.ActiveSet","text":"ActiveSet{AT, R, IT}\n\nRepresents an active set of extreme vertices collected in a FW algorithm, along with their coefficients (λ_i, a_i). R is the type of the λ_i, AT is the type of the atoms a_i. The iterate x = ∑λ_i a_i is stored in x with type IT.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Base.copy-Union{Tuple{FrankWolfe.ActiveSet{AT, R, IT}}, Tuple{IT}, Tuple{R}, Tuple{AT}} where {AT, R, IT}","page":"References","title":"Base.copy","text":"Copies an active set, the weight and atom vectors and the iterate. Individual atoms are not copied.\n\n\n\n\n\n","category":"method"},{"location":"reference/#FrankWolfe.active_set_argmin-Tuple{FrankWolfe.ActiveSet, Any}","page":"References","title":"FrankWolfe.active_set_argmin","text":"active_set_argmin(active_set::ActiveSet, direction)\n\nComputes the linear minimizer in the direction on the active set. Returns (λ_i, a_i, i)\n\n\n\n\n\n","category":"method"},{"location":"reference/#FrankWolfe.active_set_argminmax-Tuple{FrankWolfe.ActiveSet, Any}","page":"References","title":"FrankWolfe.active_set_argminmax","text":"active_set_argminmax(active_set::ActiveSet, direction)\n\nComputes the linear minimizer in the direction on the active set. Returns (λ_min, a_min, i_min, λ_max, a_max, i_max)\n\n\n\n\n\n","category":"method"},{"location":"reference/#FrankWolfe.active_set_initialize!-Union{Tuple{R}, Tuple{AT}, Tuple{FrankWolfe.ActiveSet{AT, R, IT} where IT, Any}} where {AT, R}","page":"References","title":"FrankWolfe.active_set_initialize!","text":"active_set_initialize!(as, v)\n\nResets the active set structure to a single vertex v with unit weight.\n\n\n\n\n\n","category":"method"},{"location":"reference/#FrankWolfe.active_set_update!","page":"References","title":"FrankWolfe.active_set_update!","text":"active_set_update!(active_set::ActiveSet, lambda, atom)\n\nAdds the atom to the active set with weight lambda or adds lambda to existing atom.\n\n\n\n\n\n","category":"function"},{"location":"reference/#FrankWolfe.compute_active_set_iterate-Tuple{Any}","page":"References","title":"FrankWolfe.compute_active_set_iterate","text":"compute_active_set_iterate(active_set)\n\n\n\n\n\n","category":"method"},{"location":"reference/#FrankWolfe.find_minmax_directions-Tuple{FrankWolfe.ActiveSet, Any, Any}","page":"References","title":"FrankWolfe.find_minmax_directions","text":"find_minmax_directions(active_set::ActiveSet, direction, Φ)\n\nComputes the point of the active set minimizing in direction on the active set (local Frank Wolfe) and the maximizing one (away step). Returns the two corresponding indices in the active set, along with a flag indicating if the direction improvement is above a threshold. goodstep_tolerance ∈ (0, 1] is a tolerance coefficient multiplying Φ for the validation of the progress. \n\n\n\n\n\n","category":"method"},{"location":"reference/#Step-size-computation","page":"References","title":"Step size computation","text":"","category":"section"},{"location":"reference/","page":"References","title":"References","text":"For all Frank-Wolfe algorithms, a step size must be determined to move from the current iterate to the next one. This step size can be determined by exact line search or any other rule represented by a subtype of LineSearchMethod which must implement line_search_wrapper.","category":"page"},{"location":"reference/","page":"References","title":"References","text":"FrankWolfe.line_search_wrapper\nFrankWolfe.LineSearchMethod\nFrankWolfe.adaptive_step_size\nFrankWolfe.MonotonousStepSize\nFrankWolfe.MonotonousNonConvexStepSize","category":"page"},{"location":"reference/#FrankWolfe.line_search_wrapper","page":"References","title":"FrankWolfe.line_search_wrapper","text":"line search wrapper NOTE: The stepsize is defined as x - gamma * d\n\nReturns the step size gamma and the Lipschitz estimate L\n\n\n\n\n\n","category":"function"},{"location":"reference/#FrankWolfe.LineSearchMethod","page":"References","title":"FrankWolfe.LineSearchMethod","text":"Line search method to apply once the direction is computed.\n\n\n\n\n\n","category":"type"},{"location":"reference/#FrankWolfe.adaptive_step_size","page":"References","title":"FrankWolfe.adaptive_step_size","text":"Slight modification of Adaptive Step Size strategy from https://arxiv.org/pdf/1806.05123.pdf\n\nNote: direction is opposite to the improving direction norm(gradient, direction) > 0 TODO: \n\nmake emphasis aware and optimize\n\n\n\n\n\n","category":"function"},{"location":"reference/#FrankWolfe.MonotonousStepSize","page":"References","title":"FrankWolfe.MonotonousStepSize","text":"MonotonousStepSize{F}\n\nRepresents a monotonous open-loop step size. Contains a halving factor N increased at each iteration until there is primal progress gamma = 2 / (t + 2) * 2^(-N)\n\n\n\n\n\n","category":"type"},{"location":"reference/#FrankWolfe.MonotonousNonConvexStepSize","page":"References","title":"FrankWolfe.MonotonousNonConvexStepSize","text":"MonotonousNonConvexStepSize{F}\n\nRepresents a monotonous open-loop non-convex step size. Contains a halving factor N increased at each iteration until there is primal progress gamma = 1 / sqrt(t + 1) * 2^(-N)\n\n\n\n\n\n","category":"type"},{"location":"reference/","page":"References","title":"References","text":"See Pedregosa, Negiar, Askari, Jaggi 2020 for the adaptive step size, Carderera, Besançon, Pokutta 2021 for the monotonous step size.","category":"page"},{"location":"reference/#Functions-and-Structures-2","page":"References","title":"Functions and Structures","text":"","category":"section"},{"location":"reference/","page":"References","title":"References","text":"FrankWolfe.minimize_over_convex_hull!\nFrankWolfe.build_reduced_problem(atoms::AbstractVector{<:FrankWolfe.ScaledHotVector},hessian,weights,gradient,tolerance)\nFrankWolfe.strong_frankwolfe_gap\nFrankWolfe.accelerated_simplex_gradient_descent_over_probability_simplex\nFrankWolfe.simplex_gradient_descent_over_probability_simplex\nFrankWolfe.projection_simplex_sort\nFrankWolfe.strong_frankwolfe_gap_probability_simplex\nFrankWolfe.simplex_gradient_descent_over_convex_hull\nFrankWolfe.lp_separation_oracle\nFrankWolfe.Emphasis\nFrankWolfe.ObjectiveFunction\nFrankWolfe.compute_value_gradient\nFrankWolfe.StochasticObjective\nFrankWolfe.plot_results\nFrankWolfe.check_gradients\nFrankWolfe.trajectory_callback","category":"page"},{"location":"reference/#FrankWolfe.minimize_over_convex_hull!","page":"References","title":"FrankWolfe.minimize_over_convex_hull!","text":"minimize_over_convex_hull!\n\nGiven a function f with gradient grad! and an active set active_set this function will minimize the function over the convex hull of the active set until the strong-wolfe gap over the active set is below tolerance.\n\nIt will either directly minimize over the convex hull using simplex gradient descent, or it will transform the problem to barycentric coordinates and minimize over the unit probability simplex using gradient descent or Nesterov's accelerated gradient descent.\n\n\n\n\n\n","category":"function"},{"location":"reference/#FrankWolfe.build_reduced_problem-Tuple{AbstractVector{var\"#s11\"} where var\"#s11\"<:FrankWolfe.ScaledHotVector, Any, Any, Any, Any}","page":"References","title":"FrankWolfe.build_reduced_problem","text":"build_reduced_problem(atoms::AbstractVector{<:AbstractVector}, hessian, weights, gradient, tolerance)\n\nGiven an active set formed by vectors , a (constant) Hessian and a gradient constructs a quadratic problem over the unit probability simplex that is equivalent to minimizing the original function over the convex hull of the active set. If λ are the barycentric coordinates of dimension equal to the cardinality of the active set, the objective function is:     f(λ) = reducedlinear^T λ + 0.5 * λ^T reducedhessian λ\n\nIn the case where we find that the current iterate has a strong-Wolfe gap over the convex hull of the active set that is below the tolerance we return nothing (as there is nothing to do).\n\n\n\n\n\n","category":"method"},{"location":"reference/#FrankWolfe.strong_frankwolfe_gap","page":"References","title":"FrankWolfe.strong_frankwolfe_gap","text":"Checks the strong Frank-Wolfe gap for the reduced problem.\n\n\n\n\n\n","category":"function"},{"location":"reference/#FrankWolfe.accelerated_simplex_gradient_descent_over_probability_simplex","page":"References","title":"FrankWolfe.accelerated_simplex_gradient_descent_over_probability_simplex","text":"accelerated_simplex_gradient_descent_over_probability_simplex\n\nMinimizes an objective function over the unit probability simplex until the Strong-Wolfe gap is below tolerance using Nesterov's accelerated gradient descent.\n\n\n\n\n\n","category":"function"},{"location":"reference/#FrankWolfe.simplex_gradient_descent_over_probability_simplex","page":"References","title":"FrankWolfe.simplex_gradient_descent_over_probability_simplex","text":"simplex_gradient_descent_over_probability_simplex\n\nMinimizes an objective function over the unit probability simplex until the Strong-Wolfe gap is below tolerance using gradient descent.\n\n\n\n\n\n","category":"function"},{"location":"reference/#FrankWolfe.projection_simplex_sort","page":"References","title":"FrankWolfe.projection_simplex_sort","text":"projection_simplex_sort(x; s=1.0)\n\nPerform a projection onto the probability simplex of radius s using a sorting algorithm.\n\n\n\n\n\n","category":"function"},{"location":"reference/#FrankWolfe.strong_frankwolfe_gap_probability_simplex","page":"References","title":"FrankWolfe.strong_frankwolfe_gap_probability_simplex","text":"strong_frankwolfe_gap_probability_simplex\n\nCompute the Strong-Wolfe gap over the unit probability simplex given a gradient.\n\n\n\n\n\n","category":"function"},{"location":"reference/#FrankWolfe.simplex_gradient_descent_over_convex_hull","page":"References","title":"FrankWolfe.simplex_gradient_descent_over_convex_hull","text":"simplex_gradient_descent_over_convex_hull(f, grad!, gradient, active_set, tolerance, t, time_start, non_simplex_iter)\n\nMinimizes an objective function over the convex hull of the active set until the Strong-Wolfe gap is below tolerance using simplex gradient descent.\n\n\n\n\n\n","category":"function"},{"location":"reference/#FrankWolfe.lp_separation_oracle","page":"References","title":"FrankWolfe.lp_separation_oracle","text":"Returns either a tuple (y, val) with y an atom from the active set satisfying the progress criterion and val the corresponding gap dot(y, direction) or the same tuple with y from the LMO.\n\ninplace_loop controls whether the iterate type allows in-place writes. kwargs are passed on to the LMO oracle.\n\n\n\n\n\n","category":"function"},{"location":"reference/#FrankWolfe.Emphasis","page":"References","title":"FrankWolfe.Emphasis","text":"Emphasis given to the algorithm for memory-saving or not. The memory-saving mode may not be faster than the default blas mode for small dimensions.\n\n\n\n\n\n","category":"type"},{"location":"reference/#FrankWolfe.ObjectiveFunction","page":"References","title":"FrankWolfe.ObjectiveFunction","text":"ObjectiveFunction\n\nRepresents an objective function optimized by algorithms. Subtypes of ObjectiveFunction must implement at least\n\ncompute_value(::ObjectiveFunction, x) for primal value evaluation\ncompute_gradient(::ObjectiveFunction, x) for gradient evaluation.\n\nand optionally compute_value_gradient(::ObjectiveFunction, x) returning the (primal, gradient) pair. compute_gradient may always use the same storage and return a reference to it.\n\n\n\n\n\n","category":"type"},{"location":"reference/#FrankWolfe.compute_value_gradient","page":"References","title":"FrankWolfe.compute_value_gradient","text":"compute_value_gradient(f::ObjectiveFunction, x; [kwargs...])\n\nComputes in one call the pair (value, gradient) evaluated at x. By default, calls compute_value and compute_gradient with keywords kwargs passed down to both.\n\n\n\n\n\n","category":"function"},{"location":"reference/#FrankWolfe.StochasticObjective","page":"References","title":"FrankWolfe.StochasticObjective","text":"StochasticObjective{F, G, XT, S}(f::F, grad!::G, xs::XT, storage::S)\n\nRepresents a composite function evaluated with stochastic gradient. f(θ, x) evaluates the loss for a single data point x and parameter θ. grad!(storage, θ, x) adds to storage the partial gradient with respect to data point x at parameter θ. xs must be an indexable iterable (Vector{Vector{Float64}} for instance). Functions using a StochasticObjective have optional keyword arguments rng, batch_size and full_evaluation controlling whether the function should be evaluated over all data points.\n\nNote: grad! must not reset the storage to 0 before adding to it.\n\n\n\n\n\n","category":"type"},{"location":"reference/#FrankWolfe.plot_results","page":"References","title":"FrankWolfe.plot_results","text":"plot_results\n\nGiven a series of list, generate subplots. listdatay -> contains a list of a list of lists (where each list refers to a subplot, and a list of lists refers to the y-values of the series inside a subplot). listdatax -> contains a list of a list of lists (where each list refers to a subplot, and a list of lists refers to the x-values of the series inside a subplot). So if we have one plot with two series, these might look like:     listdatay = [[[1, 2, 3, 4, 5, 6], [1, 2, 3, 4, 5, 6]]]     listdatax = [[[1, 2, 3, 4, 5, 6], [1, 2, 3, 4, 5, 6]]]\n\nAnd if we have two plots, each with two series, these might look like:     listdatay = [[[1, 2, 3, 4, 5, 6], [1, 2, 3, 4, 5, 6]], [[7, 8, 9, 10, 11, 12], [7, 8, 9, 10, 11, 12]]]     listdatax = [[[1, 2, 3, 4, 5, 6], [1, 2, 3, 4, 5, 6]], [[7, 8, 9, 10, 11, 12], [7, 8, 9, 10, 11, 12]]]\n\nlistlabel -> contains the labels for the series that will be plotted, which has to have a length equal to the number of series that are being plotted:     listlabel = [\"Series 1\", \"Series 2\"]\n\nlistaxisx -> contains the labels for the x-axis that will be plotted,  which has to have a length equal to the number of subplots:      listaxisx = [\"x-axis plot 1\", \"x-axis plot 1\"]\n\nlistaxisy -> Same as listaxisx but for the y-axis\n\nxscalelog -> A list of values indicating the type of axes to use in each subplot, must be equal to the number of subplots:     xscalelog = [:log, :identity]\n\nyscalelog -> Same as xscalelog but for the y-axis\n\n\n\n\n\n","category":"function"},{"location":"reference/#FrankWolfe.check_gradients","page":"References","title":"FrankWolfe.check_gradients","text":"Check if the gradient using finite differences matches the grad! provided.\n\n\n\n\n\n","category":"function"},{"location":"reference/#FrankWolfe.trajectory_callback","page":"References","title":"FrankWolfe.trajectory_callback","text":"trajectory_callback(storage)\n\nCallback pushing the state at each iteration to the passed storage. The state data is only the 5 first fields, usually: (t,primal,dual,dual_gap,time)\n\n\n\n\n\n","category":"function"},{"location":"reference/","page":"References","title":"References","text":"A note on iterates precision in algorithms depending on an active set:   The weights in the active set are currently defined as Float64 in the algorithm. This means that even with vertices using a lower precision, the iterate sum_i(lambda_i * v_i) will be upcast to Float64. One reason for keeping this as-is for now is the higher precision required by the computation of iterates from their barycentric decomposition.","category":"page"},{"location":"reference/#Custom-extreme-point-types","page":"References","title":"Custom extreme point types","text":"","category":"section"},{"location":"reference/","page":"References","title":"References","text":"For some feasible sets, the extreme points of the feasible set returned by the LMO possess a specific structure that can be represented in an efficient manner both for storage and for common operations like scaling and addition with an iterate. They are presented below:","category":"page"},{"location":"reference/","page":"References","title":"References","text":"FrankWolfe.ScaledHotVector\nFrankWolfe.RankOneMatrix","category":"page"},{"location":"reference/#FrankWolfe.ScaledHotVector","page":"References","title":"FrankWolfe.ScaledHotVector","text":"ScaledHotVector{T}\n\nRepresents a vector of at most one value different from 0.\n\n\n\n\n\n","category":"type"},{"location":"reference/#FrankWolfe.RankOneMatrix","page":"References","title":"FrankWolfe.RankOneMatrix","text":"RankOneMatrix{T, UT, VT}\n\nRepresents a rank-one matrix R = u * vt'. Composes like a charm.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Batch-and-momentum-iterators","page":"References","title":"Batch and momentum iterators","text":"","category":"section"},{"location":"reference/","page":"References","title":"References","text":"FrankWolfe.momentum_iterate\nFrankWolfe.ExpMomentumIterator\nFrankWolfe.ConstantMomentumIterator\nFrankWolfe.batchsize_iterate\nFrankWolfe.ConstantBatchIterator\nFrankWolfe.IncrementBatchIterator","category":"page"},{"location":"reference/#FrankWolfe.momentum_iterate","page":"References","title":"FrankWolfe.momentum_iterate","text":"momentum_iterate(iter::MomentumIterator) -> ρ\n\nMethod to implement for a type MomentumIterator. Returns the next momentum value ρ and updates the iterator internal state.\n\n\n\n\n\n","category":"function"},{"location":"reference/#FrankWolfe.ExpMomentumIterator","page":"References","title":"FrankWolfe.ExpMomentumIterator","text":"ExpMomentumIterator{T}\n\nIterator for the momentum used in the variant of Stochastic Frank-Wolfe. Momentum coefficients are the values of the iterator: ρ_t = 1 - num / (offset + t)^exp\n\nThe state corresponds to the iteration count.\n\nSource: Stochastic Conditional Gradient Methods: From Convex Minimization to Submodular Maximization Aryan Mokhtari, Hamed Hassani, Amin Karbasi, JMLR 2020.\n\n\n\n\n\n","category":"type"},{"location":"reference/#FrankWolfe.ConstantMomentumIterator","page":"References","title":"FrankWolfe.ConstantMomentumIterator","text":"ConstantMomentumIterator{T}\n\nIterator for momentum with a fixed damping value, always return the value and a dummy state.\n\n\n\n\n\n","category":"type"},{"location":"reference/#FrankWolfe.batchsize_iterate","page":"References","title":"FrankWolfe.batchsize_iterate","text":"batchsize_iterate(iter::BatchSizeIterator) -> b\n\nMethod to implement for a batch size iterator of type BatchSizeIterator. Calling batchsize_iterate returns the next batch size and typically update the internal state of iter.\n\n\n\n\n\n","category":"function"},{"location":"reference/#FrankWolfe.ConstantBatchIterator","page":"References","title":"FrankWolfe.ConstantBatchIterator","text":"ConstantBatchIterator(batch_size)\n\nBatch iterator always returning a constant batch size.\n\n\n\n\n\n","category":"type"},{"location":"reference/#FrankWolfe.IncrementBatchIterator","page":"References","title":"FrankWolfe.IncrementBatchIterator","text":"IncrementBatchIterator(starting_batch_size, max_batch_size, [increment = 1])\n\nBatch size starting at startingbatchsize and incrementing by increment at every iteration.\n\n\n\n\n\n","category":"type"},{"location":"indexlist/#Index","page":"Index","title":"Index","text":"","category":"section"},{"location":"indexlist/","page":"Index","title":"Index","text":"","category":"page"},{"location":"#Introduction","page":"Home","title":"Introduction","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"This package defines a generic interface and several implementations for Frank-Wolfe algorithms. A paper presenting the package and explaining the algorithms and numerous examples in detail can be found here: FrankWolfe.jl: A high-performance and flexible toolbox for Frank-Wolfe algorithms and Conditional Gradients. The package features four algorithms: a standard Frank-Wolfe implementation (frank_wolfe, FW), Away-step Frank-Wolfe (away_frank_wolfe, AFW), Blended Conditional Gradient (blended_conditional_gradient, BCG), and Stochastic Frank-Wolfe (FrankWolfe.stochastic_frank_wolfe, SFW). While the standard Frank-Wolfe algorithm can only move towards extreme points of the compact, convex set mathcalC, Away-step Frank-Wolfe can move away  from them. The following figure from FrankWolfe.jl: A high-performance and flexible toolbox for Frank-Wolfe algorithms and Conditional Gradients schematizes this behaviour: (Image: FW vs AFW) \nThe algorithms minimize a quadratic function (contour lines depicted) over a simple polytope. As the minimizer lies on a face, the standard Frank-Wolfe algorithm zig-zags towards the solution. \nThe following table compares the characteristics of the algorithms presented in the package:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Algorithm Progress/Iteration Time/Iteration Sparsity Numerical Stability Active Set Lazifiable\nFW Low Low Low High No Yes\nAFW Medium Medium-High Medium Medium-High Yes Yes\nBCG High Medium-High High Medium Yes By design\nSFW Low Low Low High No No","category":"page"}]
}

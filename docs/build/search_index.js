var documenterSearchIndex = {"docs":
[{"location":"examples2.html#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples.html#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples.html#Polynomial-Regression","page":"Examples","title":"Polynomial Regression","text":"","category":"section"},{"location":"examples.html","page":"Examples","title":"Examples","text":"The following example features the LMO for the l_1 norm ball. Given input/output pairs x_iy_i_i=1^N and sparse coefficients c_j, where","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"y_i=sum_j=1^m c_j f_j(x_i)","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"and f_j mathbbR^ntomathbbR, the task is to recover those c_j that are non-zero alongside their corresponding values. Under certain assumptions, this problem can be convexified into","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"min_cinmathcalCy-Ac^2","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"for a convex set mathcalC. A detailed explanation of the example can be found in chapter 4.1 of FrankWolfe.jl: A high-performance and flexible toolbox for Frank-Wolfe algorithms and Conditional Gradients. We will present a finite-dimensional example where the basis functions f_j are monomials of the features of the vector xinmathbbR^15 of maximum degree d, that is f_i(x)=Pi_j=1^n x_j^a_j with a_jinmathbbN and sum_j=1^n a_j  d. We generate a random vector c that will have 5% non-zero entries drawn from a normal distribution with mean 10 and unit variance. In order to evaluate the polynomial, we generate a total of 1000 data points x_i_i=1^N from the standard multivariate Gaussian in mathbbR^15, with which we will compute the output variables y_i_i=1^N. Before evaluating the polynomial, these points will be contaminated with noise drawn from a standard multivariate Gaussian. We leverage MultivariatePolynomials.jl (Legat et al., 2021) to create the input polynomial of degree up to 4 in mathbbR^15 and evaluate it on the training and test data. Solving a linear minimization problem over l_1 generates points with only one non-zero element. Moreover, there is a closed-form solution for these minimizers. We run the away_frank_wolfe and blended_conditional_gradient algorithms with the adaptive line search strategy from Pedregosa et al. (2020), and compare them to Projected Gradient Descent using a smoothness estimate. We will evaluate the output solution on test points drawn in a similar manner as the training points. The radius of the l_1 norm ball that we will use to regularize the problem will be equal to 095c_1.","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"\r\ninclude(\"C:\\Users\\jonat\\Documents\\Studium\\ZIB\\FW\\FrankWolfe.jl\\examples\\activate.jl\")\r\nusing LinearAlgebra\r\nimport Random\r\n\r\nusing MultivariatePolynomials\r\nusing DynamicPolynomials\r\n\r\nimport ReverseDiff\r\nusing FiniteDifferences\r\nimport JSON\r\n\r\nconst N = 15\r\n\r\nDynamicPolynomials.@polyvar X[1:15]\r\n\r\nconst max_degree = 4\r\ncoefficient_magnitude = 10\r\nnoise_magnitude = 1\r\n\r\nconst var_monomials = MultivariatePolynomials.monomials(X, 0:max_degree)\r\n\r\nRandom.seed!(42)\r\nconst all_coeffs = map(var_monomials) do m\r\n    d = MultivariatePolynomials.degree(m)\r\n    return coefficient_magnitude * rand() .* (rand() .> 0.95 * d / max_degree)\r\nend\r\n\r\nconst true_poly = dot(all_coeffs, var_monomials)\r\n\r\nfunction evaluate_poly(coefficients)\r\n    poly = dot(coefficients, var_monomials)\r\n    return function p(x)\r\n        return MultivariatePolynomials.subs(poly, Pair(X, x)).a[1]\r\n    end\r\nend\r\n\r\nconst training_data = map(1:500) do _\r\n    x = 0.1 * randn(N)\r\n    y = MultivariatePolynomials.subs(true_poly, Pair(X, x)) + noise_magnitude * randn()\r\n    return (x, y.a[1])\r\nend\r\n\r\nconst extended_training_data = map(training_data) do (x, y)\r\n    x_ext = getproperty.(MultivariatePolynomials.subs.(var_monomials, X => x), :α)\r\n    return (x_ext, y)\r\nend\r\n\r\nconst test_data = map(1:1000) do _\r\n    x = 0.4 * randn(N)\r\n    y = MultivariatePolynomials.subs(true_poly, Pair(X, x)) + noise_magnitude * randn()\r\n    return (x, y.a[1])\r\nend\r\n\r\nconst extended_test_data = map(test_data) do (x, y)\r\n    x_ext = getproperty.(MultivariatePolynomials.subs.(var_monomials, X => x), :α)\r\n    return (x_ext, y)\r\nend\r\n\r\nfunction f(coefficients)\r\n    return 0.5 / length(extended_training_data) * sum(extended_training_data) do (x, y)\r\n        return (dot(coefficients, x) - y)^2\r\n    end\r\nend\r\n\r\nfunction f_test(coefficients)\r\n    return 0.5 / length(extended_test_data) * sum(extended_test_data) do (x, y)\r\n        return (dot(coefficients, x) - y)^2\r\n    end\r\nend\r\n\r\nfunction coefficient_errors(coeffs)\r\n    return 0.5 * sum(eachindex(all_coeffs)) do idx\r\n        return (all_coeffs[idx] - coeffs[idx])^2\r\n    end\r\nend\r\n\r\nfunction grad!(storage, coefficients)\r\n    storage .= 0\r\n    for (x, y) in extended_training_data\r\n        p_i = dot(coefficients, x) - y\r\n        @. storage += x * p_i\r\n    end\r\n    storage ./= length(training_data)\r\n    return nothing\r\nend\r\n\r\nfunction build_callback(trajectory_arr)\r\n    return function callback(state)\r\n        return push!(\r\n            trajectory_arr,\r\n            (Tuple(state)[1:5]..., f_test(state.x), coefficient_errors(state.x)),\r\n        )\r\n    end\r\nend\r\n\r\n#Check the gradient using finite differences just in case\r\ngradient = similar(all_coeffs)\r\n\r\n#Disable for now.\r\nFrankWolfe.check_gradients(grad!, f, gradient)\r\n\r\nmax_iter = 100_000\r\nrandom_initialization_vector = rand(length(all_coeffs))\r\n\r\n#lmo = FrankWolfe.LpNormLMO{1}(100 * maximum(all_coeffs))\r\n\r\nlmo = FrankWolfe.LpNormLMO{1}(0.95 * norm(all_coeffs, 1))\r\n\r\n# L estimate\r\nnum_pairs = 10000\r\nL_estimate = -Inf\r\ngradient_aux = similar(gradient)\r\nfor i in 1:num_pairs\r\n    global L_estimate\r\n    x = compute_extreme_point(lmo, randn(size(all_coeffs)))\r\n    y = compute_extreme_point(lmo, randn(size(all_coeffs)))\r\n    grad!(gradient, x)\r\n    grad!(gradient_aux, y)\r\n    new_L = norm(gradient - gradient_aux) / norm(x - y)\r\n    if new_L > L_estimate\r\n        L_estimate = new_L\r\n    end\r\nend\r\n\r\n# L1 projection\r\n# inspired by https://github.com/MPF-Optimization-Laboratory/ProjSplx.jl\r\nfunction projnorm1(x, τ)\r\n    n = length(x)\r\n    if norm(x, 1) ≤ τ\r\n        return x\r\n    end\r\n    u = abs.(x)\r\n    # simplex projection\r\n    bget = false\r\n    s_indices = sortperm(u, rev=true)\r\n    tsum = zero(τ)\r\n\r\n    @inbounds for i in 1:n-1\r\n        tsum += u[s_indices[i]]\r\n        tmax = (tsum - τ) / i\r\n        if tmax ≥ u[s_indices[i+1]]\r\n            bget = true\r\n            break\r\n        end\r\n    end\r\n    if !bget\r\n        tmax = (tsum + u[s_indices[n]] - τ) / n\r\n    end\r\n\r\n    @inbounds for i in 1:n\r\n        u[i] = max(u[i] - tmax, 0)\r\n        u[i] *= sign(x[i])\r\n    end\r\n    return u\r\nend\r\n\r\n# gradient descent\r\n\r\nxgd = FrankWolfe.compute_extreme_point(lmo, random_initialization_vector)\r\ntraining_gd = Float64[]\r\ntest_gd = Float64[]\r\ncoeff_error = Float64[]\r\ntime_start = time_ns()\r\ngd_times = Float64[]\r\nfor iter in 1:max_iter\r\n    global xgd\r\n    grad!(gradient, xgd)\r\n    xgd = projnorm1(xgd - gradient / L_estimate, lmo.right_hand_side)\r\n    push!(training_gd, f(xgd))\r\n    push!(test_gd, f_test(xgd))\r\n    push!(coeff_error, coefficient_errors(xgd))\r\n    push!(gd_times, (time_ns() - time_start) * 1e-9)\r\nend\r\n\r\n@info \"Gradient descent training loss $(f(xgd))\"\r\n@info \"Gradient descent test loss $(f_test(xgd))\"\r\n@info \"Coefficient error $(coefficient_errors(xgd))\"\r\n\r\n\r\nx00 = FrankWolfe.compute_extreme_point(lmo, random_initialization_vector)\r\nx0 = deepcopy(x00)\r\n\r\n# lazy AFW\r\ntrajectory_lafw = []\r\ncallback = build_callback(trajectory_lafw)\r\n@time x_lafw, v, primal, dual_gap, _ = FrankWolfe.away_frank_wolfe(\r\n    f,\r\n    grad!,\r\n    lmo,\r\n    x0,\r\n    max_iteration=max_iter,\r\n    line_search=FrankWolfe.Adaptive(),\r\n    print_iter=max_iter ÷ 10,\r\n    emphasis=FrankWolfe.memory,\r\n    verbose=true,\r\n    lazy=true,\r\n    gradient=gradient,\r\n    callback=callback,\r\n    L=L_estimate,\r\n);\r\n\r\n@info \"Lazy AFW training loss $(f(x_lafw))\"\r\n@info \"Test loss $(f_test(x_lafw))\"\r\n@info \"Coefficient error $(coefficient_errors(x_lafw))\"\r\n\r\ntrajectory_bcg = []\r\ncallback = build_callback(trajectory_bcg)\r\n\r\nx0 = deepcopy(x00)\r\n@time x_bcg, v, primal, dual_gap, _ = FrankWolfe.blended_conditional_gradient(\r\n    f,\r\n    grad!,\r\n    lmo,\r\n    x0,\r\n    max_iteration=max_iter,\r\n    line_search=FrankWolfe.Adaptive(),\r\n    print_iter=max_iter ÷ 10,\r\n    emphasis=FrankWolfe.memory,\r\n    verbose=true,\r\n    weight_purge_threshold=1e-10,\r\n    callback=callback,\r\n    L=L_estimate,\r\n)\r\n\r\n@info \"BCG training loss $(f(x_bcg))\"\r\n@info \"Test loss $(f_test(x_bcg))\"\r\n@info \"Coefficient error $(coefficient_errors(x_bcg))\"\r\n\r\n\r\nx0 = deepcopy(x00)\r\n\r\n#  compute reference solution using lazy AFW\r\ntrajectory_lafw_ref = []\r\ncallback = build_callback(trajectory_lafw_ref)\r\n@time _, _, primal_ref, _, _ = FrankWolfe.away_frank_wolfe(\r\n    f,\r\n    grad!,\r\n    lmo,\r\n    x0,\r\n    max_iteration=2 * max_iter,\r\n    line_search=FrankWolfe.Adaptive(),\r\n    print_iter=max_iter ÷ 10,\r\n    emphasis=FrankWolfe.memory,\r\n    verbose=true,\r\n    lazy=true,\r\n    gradient=gradient,\r\n    callback=callback,\r\n    L=L_estimate,\r\n);\r\n\r\nopen(joinpath(@__DIR__, \"polynomial_result.json\"), \"w\") do f\r\n    data = JSON.json((\r\n        trajectory_arr_lafw=trajectory_lafw,\r\n        trajectory_arr_bcg=trajectory_bcg,\r\n        function_values_gd=training_gd,\r\n        function_values_test_gd=test_gd,\r\n        coefficient_error_gd=coeff_error,\r\n        gd_times=gd_times,\r\n        ref_primal_value=primal_ref,\r\n    ))\r\n    return write(f, data)\r\nend\r\n\r\n#Count missing\\extra terms\r\nprint(\"\\n Number of extra terms in GD: \", sum((all_coeffs .== 0) .* (xgd .!= 0)))\r\nprint(\"\\n Number of missing terms in GD: \", sum((all_coeffs .!= 0) .* (xgd .== 0)))\r\n\r\nprint(\"\\n Number of extra terms in BCG: \", sum((all_coeffs .== 0) .* (x_bcg .!= 0)))\r\nprint(\"\\n Number of missing terms in BCG: \", sum((all_coeffs .!= 0) .* (x_bcg .== 0)))\r\n\r\nprint(\"\\n Number of missing terms in Lazy AFW: \", sum((all_coeffs .== 0) .* (x_lafw .!= 0)))\r\nprint(\"\\n Number of extra terms in Lazy AFW: \", sum((all_coeffs .!= 0) .* (x_lafw .== 0)))","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"\r\nusing JSON\r\nusing LaTeXStrings\r\n\r\nresults = JSON.Parser.parsefile(joinpath(@__DIR__, \"polynomial_result.json\"))\r\n\r\niteration_list = [\r\n    [x[1] + 1 for x in results[\"trajectory_arr_lafw\"]],\r\n    [x[1] + 1 for x in results[\"trajectory_arr_bcg\"]],\r\n    collect(eachindex(results[\"function_values_gd\"])),\r\n]\r\ntime_list = [\r\n    [x[5] for x in results[\"trajectory_arr_lafw\"]],\r\n    [x[5] for x in results[\"trajectory_arr_bcg\"]],\r\n    results[\"gd_times\"],\r\n]\r\nprimal_list = [\r\n    [x[2] - results[\"ref_primal_value\"] for x in results[\"trajectory_arr_lafw\"]],\r\n    [x[2] - results[\"ref_primal_value\"] for x in results[\"trajectory_arr_bcg\"]],\r\n    [x - results[\"ref_primal_value\"] for x in results[\"function_values_gd\"]],\r\n]\r\ntest_list = [\r\n    [x[6] for x in results[\"trajectory_arr_lafw\"]],\r\n    [x[6] for x in results[\"trajectory_arr_bcg\"]],\r\n    results[\"function_values_test_gd\"],\r\n]\r\nlabel = [L\"\\textrm{L-AFW}\", L\"\\textrm{BCG}\", L\"\\textrm{GD}\"]\r\ncoefficient_error_values = [\r\n    [x[7] for x in results[\"trajectory_arr_lafw\"]],\r\n    [x[7] for x in results[\"trajectory_arr_bcg\"]],\r\n    results[\"coefficient_error_gd\"],\r\n]\r\n\r\n\r\nFrankWolfe.plot_results(\r\n    [primal_list, primal_list, test_list, test_list],\r\n    [iteration_list, time_list, iteration_list, time_list],\r\n    label,\r\n    [L\"\\textrm{Iteration}\", L\"\\textrm{Time}\", L\"\\textrm{Iteration}\", L\"\\textrm{Time}\"],\r\n    [L\"\\textrm{Primal Gap}\", L\"\\textrm{Primal Gap}\", L\"\\textrm{Test loss}\", L\"\\textrm{Test loss}\"],\r\n    xscalelog=[:log, :identity, :log, :identity],\r\n    legend_position=[:bottomleft, nothing, nothing, nothing],\r\n    filename=\"polynomial_result.pdf\",\r\n)","category":"page"},{"location":"examples.html#Matrix-Completion","page":"Examples","title":"Matrix Completion","text":"","category":"section"},{"location":"examples.html","page":"Examples","title":"Examples","text":"We present another example that is about matrix completion. The idea is, given a partially observed matrix YinmathbbRmtimes n, to find XinmathbbRmtimes n to minimize the sum of squared errors from the observed entries while 'completing' the matrix Y, i.e. filling the unobserved entries to match Y as good as possible. Again, a detailed explanation can be found in chapter 4.2 of FrankWolfe.jl: A high-performance and flexible toolbox for Frank-Wolfe algorithms and Conditional Gradients. We will try to solve","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"min_X_*le tau sum_(ij)inmathcalI (X_ijiY_ij)^2","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"where tau0 and mathcalI denotes the indices of the observed entries. We will use the FrankWolfe.NuclearNormLMO and compare our Frank-Wolfe implementation with a Projected Gradient Descent (PGD) algorithm which, after each gradient descent step, projects the iterates back onto the nuclear norm ball. We use a movielens dataset for comparison.","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"include(\"C:\\Users\\jonat\\Documents\\Studium\\ZIB\\FW\\FrankWolfe.jl\\examples\\activate.jl\")\r\n\r\n# download movielens data\r\nusing ZipFile, DataFrames, CSV\r\nimport JSON\r\n\r\nusing Random\r\n\r\nusing Profile\r\n\r\nusing SparseArrays, LinearAlgebra\r\n\r\ntemp_zipfile = download(\"http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\")\r\n\r\n# temp_zipfile = download(\"http://files.grouplens.org/datasets/movielens/ml-25m.zip\")\r\n#temp_zipfile = download(\"http://files.grouplens.org/datasets/movielens/ml-latest.zip\")\r\n\r\nzarchive = ZipFile.Reader(temp_zipfile)\r\n\r\nmovies_file = zarchive.files[findfirst(f -> occursin(\"movies\", f.name), zarchive.files)]\r\nmovies_frame = CSV.read(movies_file, DataFrame)\r\n\r\nratings_file = zarchive.files[findfirst(f -> occursin(\"ratings\", f.name), zarchive.files)]\r\nratings_frame = CSV.read(ratings_file, DataFrame)\r\n\r\n# ratings_frame has columns user_id, movie_id\r\n# we construct a new matrix with users as rows and all ratings as columns\r\n# we use missing for non-present movies\r\n\r\nusers = unique(ratings_frame[:, :userId])\r\nmovies = unique(ratings_frame[:, :movieId])\r\n\r\n@assert users == eachindex(users)\r\nmovies_revert = zeros(Int, maximum(movies))\r\nfor (idx, m) in enumerate(movies)\r\n    movies_revert[m] = idx\r\nend\r\nmovies_indices = [movies_revert[idx] for idx in ratings_frame[:, :movieId]]\r\n\r\nconst rating_matrix = sparse(\r\n    ratings_frame[:, :userId],\r\n    movies_indices,\r\n    ratings_frame[:, :rating],\r\n    length(users),\r\n    length(movies),\r\n)\r\n\r\nmissing_rate = 0.05\r\n\r\nRandom.seed!(42)\r\n\r\nconst missing_ratings = Tuple{Int,Int}[]\r\nconst present_ratings = Tuple{Int,Int}[]\r\nlet\r\n    (I, J, V) = SparseArrays.findnz(rating_matrix)\r\n    for idx in eachindex(I)\r\n        if V[idx] > 0\r\n            if rand() <= missing_rate\r\n                push!(missing_ratings, (I[idx], J[idx]))\r\n            else\r\n                push!(present_ratings, (I[idx], J[idx]))\r\n            end\r\n        end\r\n    end\r\nend\r\n\r\nfunction f(X)\r\n    # note: we iterate over the rating_matrix indices,\r\n    # since it is sparse unlike X\r\n    r = 0.0\r\n    for (i, j) in present_ratings\r\n        r += 0.5 * (X[i, j] - rating_matrix[i, j])^2\r\n    end\r\n    return r\r\nend\r\n\r\nfunction grad!(storage, X)\r\n    storage .= 0\r\n    for (i, j) in present_ratings\r\n        storage[i, j] = X[i, j] - rating_matrix[i, j]\r\n    end\r\n    return nothing\r\nend\r\n\r\nfunction test_loss(X)\r\n    r = 0.0\r\n    for (i, j) in missing_ratings\r\n        r += 0.5 * (X[i, j] - rating_matrix[i, j])^2\r\n    end\r\n    return r\r\nend\r\n\r\nfunction project_nuclear_norm_ball(X; radius=1.0)\r\n    U, sing_val, Vt = svd(X)\r\n    if (sum(sing_val) <= radius)\r\n        return X, -norm_estimation * U[:, 1] * Vt[:, 1]'\r\n    end\r\n    sing_val = FrankWolfe.projection_simplex_sort(sing_val, s=radius)\r\n    return U * Diagonal(sing_val) * Vt', -norm_estimation * U[:, 1] * Vt[:, 1]'\r\nend\r\n\r\n#norm_estimation = 400 * Arpack.svds(rating_matrix, nsv=1, ritzvec=false)[1].S[1]\r\nnorm_estimation = 10 * Arpack.svds(rating_matrix, nsv=1, ritzvec=false)[1].S[1]\r\n\r\nconst lmo = FrankWolfe.NuclearNormLMO(norm_estimation)\r\nconst x0 = FrankWolfe.compute_extreme_point(lmo, zero(rating_matrix))\r\nconst k = 100\r\n\r\n# benchmark the oracles\r\nFrankWolfe.benchmark_oracles(\r\n    f,\r\n    (str, x) -> grad!(str, x),\r\n    () -> randn(size(rating_matrix)),\r\n    lmo;\r\n    k=100,\r\n)\r\n\r\ngradient = spzeros(size(x0)...)\r\ngradient_aux = spzeros(size(x0)...)\r\n\r\n# pushes to the trajectory the first 5 elements of the trajectory and the test value at the current iterate\r\nfunction build_callback(trajectory_arr)\r\n    return function callback(state)\r\n        return push!(trajectory_arr, (Tuple(state)[1:5]..., test_loss(state.x)))\r\n    end\r\nend\r\n\r\n\r\n#Estimate the smoothness constant.\r\nnum_pairs = 1000\r\nL_estimate = -Inf\r\nfor i in 1:num_pairs\r\n    global L_estimate\r\n    # computing random rank-one matrices\r\n    u1 = rand(size(x0, 1))\r\n    u1 ./= sum(u1)\r\n    u1 .*= norm_estimation\r\n    v1 = rand(size(x0, 2))\r\n    v1 ./= sum(v1)\r\n    x = FrankWolfe.RankOneMatrix(u1, v1)\r\n    u2 = rand(size(x0, 1))\r\n    u2 ./= sum(u2)\r\n    u2 .*= norm_estimation\r\n    v2 = rand(size(x0, 2))\r\n    v2 ./= sum(v2)\r\n    y = FrankWolfe.RankOneMatrix(u2, v2)\r\n    grad!(gradient, x)\r\n    grad!(gradient_aux, y)\r\n    new_L = norm(gradient - gradient_aux) / norm(x - y)\r\n    if new_L > L_estimate\r\n        L_estimate = new_L\r\n    end\r\nend\r\n\r\n# PGD steps\r\n\r\nxgd = Matrix(x0)\r\nfunction_values = Float64[]\r\ntiming_values = Float64[]\r\nfunction_test_values = Float64[]\r\n\r\ntime_start = time_ns()\r\nfor _ in 1:k\r\n    f_val = f(xgd)\r\n    push!(function_values, f_val)\r\n    push!(function_test_values, test_loss(xgd))\r\n    push!(timing_values, (time_ns() - time_start) / 1e9)\r\n    @info f_val\r\n    grad!(gradient, xgd)\r\n    xgd_new, vertex = project_nuclear_norm_ball(xgd - gradient / L_estimate, radius=norm_estimation)\r\n    gamma, _ = FrankWolfe.backtrackingLS(f, gradient, xgd, xgd - xgd_new, 1.0)\r\n    @. xgd -= gamma * (xgd - xgd_new)\r\nend\r\n\r\ntrajectory_arr_fw = Vector{Tuple{Int64,Float64,Float64,Float64,Float64,Float64}}()\r\ncallback = build_callback(trajectory_arr_fw)\r\nxfin, _, _, _, traj_data = FrankWolfe.frank_wolfe(\r\n    f,\r\n    grad!,\r\n    lmo,\r\n    x0;\r\n    epsilon=1e-9,\r\n    max_iteration=10 * k,\r\n    print_iter=k / 10,\r\n    verbose=true,\r\n    linesearch_tol=1e-8,\r\n    line_search=FrankWolfe.Adaptive(),\r\n    emphasis=FrankWolfe.memory,\r\n    gradient=gradient,\r\n    callback=callback,\r\n)\r\n\r\ntrajectory_arr_lazy = Vector{Tuple{Int64,Float64,Float64,Float64,Float64,Float64}}()\r\ncallback = build_callback(trajectory_arr_lazy)\r\nxlazy, _, _, _, _ = FrankWolfe.lazified_conditional_gradient(\r\n    f,\r\n    grad!,\r\n    lmo,\r\n    x0;\r\n    epsilon=1e-9,\r\n    max_iteration=10 * k,\r\n    print_iter=k / 10,\r\n    verbose=true,\r\n    linesearch_tol=1e-8,\r\n    line_search=FrankWolfe.Adaptive(),\r\n    emphasis=FrankWolfe.memory,\r\n    gradient=gradient,\r\n    callback=callback,\r\n)\r\n\r\n\r\ntrajectory_arr_lazy_ref = Vector{Tuple{Int64,Float64,Float64,Float64,Float64,Float64}}()\r\ncallback = build_callback(trajectory_arr_lazy_ref)\r\nxlazy, _, _, _, _ = FrankWolfe.lazified_conditional_gradient(\r\n    f,\r\n    grad!,\r\n    lmo,\r\n    x0;\r\n    epsilon=1e-9,\r\n    max_iteration=50 * k,\r\n    print_iter=k / 10,\r\n    verbose=true,\r\n    linesearch_tol=1e-8,\r\n    line_search=FrankWolfe.Adaptive(),\r\n    emphasis=FrankWolfe.memory,\r\n    gradient=gradient,\r\n    callback=callback,\r\n)\r\n\r\n@info \"Gdescent test loss: $(test_loss(xgd))\"\r\n@info \"FW test loss: $(test_loss(xfin))\"\r\n@info \"LCG test loss: $(test_loss(xlazy))\"\r\n\r\nfw_test_values = getindex.(trajectory_arr_fw, 6)\r\nlazy_test_values = getindex.(trajectory_arr_lazy, 6)\r\n\r\n\r\nopen(joinpath(@__DIR__, \"movielens_result.json\"), \"w\") do f\r\n    data = JSON.json((\r\n        svals_gd=svdvals(xgd),\r\n        svals_fw=svdvals(xfin),\r\n        svals_lcg=svdvals(xlazy),\r\n        fw_test_values=fw_test_values,\r\n        lazy_test_values=lazy_test_values,\r\n        trajectory_arr_fw=trajectory_arr_fw,\r\n        trajectory_arr_lazy=trajectory_arr_lazy,\r\n        function_values_gd=function_values,\r\n        function_values_test_gd=function_test_values,\r\n        timing_values_gd=timing_values,\r\n        trajectory_arr_lazy_ref=trajectory_arr_lazy_ref,\r\n    ))\r\n    return write(f, data)\r\nend\r\n\r\n#Plot results w.r.t. iteration count\r\ngr()\r\npit = plot(\r\n    getindex.(trajectory_arr_fw, 1),\r\n    getindex.(trajectory_arr_fw, 2),\r\n    label=\"FW\",\r\n    xlabel=\"iterations\",\r\n    ylabel=\"Objective function\",\r\n    yaxis=:log,\r\n    yguidefontsize=8,\r\n    xguidefontsize=8,\r\n    legendfontsize=8,\r\n    legend=:bottomleft,\r\n)\r\nplot!(getindex.(trajectory_arr_lazy, 1), getindex.(trajectory_arr_lazy, 2), label=\"LCG\")\r\nplot!(eachindex(function_values), function_values, yaxis=:log, label=\"GD\")\r\nplot!(eachindex(function_test_values), function_test_values, label=\"GD_test\")\r\nplot!(getindex.(trajectory_arr_fw, 1), getindex.(trajectory_arr_fw, 6), label=\"FW_T\")\r\nplot!(getindex.(trajectory_arr_lazy, 1), getindex.(trajectory_arr_lazy, 6), label=\"LCG_T\")\r\nsavefig(pit, \"objective_func_vs_iteration.pdf\")\r\n\r\n#Plot results w.r.t. time\r\npit = plot(\r\n    getindex.(trajectory_arr_fw, 5),\r\n    getindex.(trajectory_arr_fw, 2),\r\n    label=\"FW\",\r\n    ylabel=\"Objective function\",\r\n    yaxis=:log,\r\n    xlabel=\"time (s)\",\r\n    yguidefontsize=8,\r\n    xguidefontsize=8,\r\n    legendfontsize=8,\r\n    legend=:bottomleft,\r\n)\r\n\r\nplot!(getindex.(trajectory_arr_lazy, 5), getindex.(trajectory_arr_lazy, 2), label=\"LCG\")\r\nplot!(getindex.(trajectory_arr_lazy, 5), getindex.(trajectory_arr_lazy, 6), label=\"LCG_T\")\r\nplot!(getindex.(trajectory_arr_fw, 5), getindex.(trajectory_arr_fw, 6), label=\"FW_T\")\r\n\r\nplot!(timing_values, function_values, label=\"GD\", yaxis=:log)\r\nplot!(timing_values, function_test_values, label=\"GD_test\")\r\n\r\nsavefig(pit, \"objective_func_vs_time.pdf\")","category":"page"},{"location":"examples.html","page":"Examples","title":"Examples","text":"# This example highlights the use of a linear minimization oracle\r\n# using an LP solver defined in MathOptInterface\r\n# we compare the performance of the two LMOs, in- and out of place\r\n#\r\n# to get accurate timings it is important to run twice so that the compile time of Julia for the first run\r\n# is not tainting the results\r\n\r\nusing JSON\r\nusing LaTeXStrings\r\nresults = JSON.Parser.parsefile(\"movielens_result.json\")\r\n\r\nref_optimum = results[\"trajectory_arr_lazy_ref\"][end][2]\r\n\r\niteration_list = [\r\n    [x[1] + 1 for x in results[\"trajectory_arr_fw\"]],\r\n    [x[1] + 1 for x in results[\"trajectory_arr_lazy\"]],\r\n    collect(1:1:length(results[\"function_values_gd\"])),\r\n]\r\ntime_list = [\r\n    [x[5] for x in results[\"trajectory_arr_fw\"]],\r\n    [x[5] for x in results[\"trajectory_arr_lazy\"]],\r\n    results[\"timing_values_gd\"],\r\n]\r\nprimal_gap_list = [\r\n    [x[2] - ref_optimum for x in results[\"trajectory_arr_fw\"]],\r\n    [x[2] - ref_optimum for x in results[\"trajectory_arr_lazy\"]],\r\n    [x - ref_optimum for x in results[\"function_values_gd\"]],\r\n]\r\ntest_list =\r\n    [results[\"fw_test_values\"], results[\"lazy_test_values\"], results[\"function_values_test_gd\"]]\r\n\r\nlabel = [L\"\\textrm{FW}\", L\"\\textrm{L-CG}\", L\"\\textrm{GD}\"]\r\n\r\nFrankWolfe.plot_results(\r\n    [primal_gap_list, primal_gap_list, test_list, test_list],\r\n    [iteration_list, time_list, iteration_list, time_list],\r\n    label,\r\n    [L\"\\textrm{Iteration}\", L\"\\textrm{Time}\", L\"\\textrm{Iteration}\", L\"\\textrm{Time}\"],\r\n    [\r\n        L\"\\textrm{Primal Gap}\",\r\n        L\"\\textrm{Primal Gap}\",\r\n        L\"\\textrm{Test Error}\",\r\n        L\"\\textrm{Test Error}\",\r\n    ],\r\n    xscalelog=[:log, :identity, :log, :identity],\r\n    legend_position=[:bottomleft, nothing, nothing, nothing],\r\n    filename=\"movielens_result.pdf\",\r\n)","category":"page"},{"location":"reference.html#Algorithms","page":"Reference","title":"Algorithms","text":"","category":"section"},{"location":"reference.html","page":"Reference","title":"Reference","text":"This section contains all algorithms of the FrankWolfe.jl package.","category":"page"},{"location":"reference.html#Functions","page":"Reference","title":"Functions","text":"","category":"section"},{"location":"reference.html","page":"Reference","title":"Reference","text":"blended_conditional_gradient\r\ncompute_extreme_point\r\nfrank_wolfe\r\nlazified_conditional_gradient\r\naway_frank_wolfe\r\n","category":"page"},{"location":"reference.html#FrankWolfe.compute_extreme_point","page":"Reference","title":"FrankWolfe.compute_extreme_point","text":"compute_extreme_point(lmo::LinearMinimizationOracle, direction; kwargs...)\n\nComputes the point argmin_{v ∈ C} v ⋅ direction with C the set represented by the LMO. All LMOs should accept keyword arguments that they can ignore.\n\n\n\n\n\n","category":"function"},{"location":"reference.html#LMOs","page":"Reference","title":"LMOs","text":"","category":"section"},{"location":"reference.html","page":"Reference","title":"Reference","text":"The Linear Minimization Oracle (LMO) is an integral part of the iterative step in the FW algorithm. Given din mathcalX, it returns:","category":"page"},{"location":"reference.html","page":"Reference","title":"Reference","text":"vin argmin_xin mathcalC langle dx rangle","category":"page"},{"location":"reference.html","page":"Reference","title":"Reference","text":"FrankWolfe.jl features the following common LMOs out of the box:","category":"page"},{"location":"reference.html","page":"Reference","title":"Reference","text":"probability simplex: FrankWolfe.ProbabilitySimplexOracle\nunit simplex: FrankWolfe.UnitSimplexOracle\nK-sparse polytope: FrankWolfe.KSparseLMO\nK-norm ball: FrankWolfe.KNormBallLMO\nL^p-norm ball: FrankWolfe.LpNormLMO\nBirkhoff polytope: FrankWolfe.BirkhoffPolytopeLMO","category":"page"},{"location":"reference.html","page":"Reference","title":"Reference","text":"All of them are subtypes of FrankWolfe.LinearMinimizationOracle and implement the compute_extreme_point method.","category":"page"},{"location":"reference.html#Functions-and-Structures","page":"Reference","title":"Functions and Structures","text":"","category":"section"},{"location":"reference.html","page":"Reference","title":"Reference","text":"FrankWolfe.LinearMinimizationOracle\r\nFrankWolfe.CachedLinearMinimizationOracle\r\nFrankWolfe.SingleLastCachedLMO\r\nFrankWolfe.MultiCacheLMO\r\nFrankWolfe.VectorCacheLMO\r\nFrankWolfe.ProductLMO\r\ncompute_extreme_point(lmo::FrankWolfe.ProductLMO, direction::Tuple; kwargs...)\r\ncompute_extreme_point(lmo::FrankWolfe.ProductLMO{N},direction::AbstractArray;storage=similar(direction),direction_indices,kwargs...,) where {N}\r\nFrankWolfe.UnitSimplexOracle\r\ncompute_extreme_point(lmo::FrankWolfe.UnitSimplexOracle{T}, direction) where {T}\r\nFrankWolfe.compute_dual_solution(::FrankWolfe.UnitSimplexOracle{T}, direction, primalSolution) where {T}\r\nFrankWolfe.ProbabilitySimplexOracle\r\ncompute_extreme_point(lmo::FrankWolfe.ProbabilitySimplexOracle{T}, direction; kwargs...) where {T}\r\nFrankWolfe.compute_dual_solution(::FrankWolfe.ProbabilitySimplexOracle{T},direction,primal_solution;kwargs...,) where {T}\r\nFrankWolfe.KSparseLMO\r\nFrankWolfe.BirkhoffPolytopeLMO\r\nFrankWolfe.LpNormLMO\r\nFrankWolfe.KNormBallLMO\r\nFrankWolfe.NuclearNormLMO\r\ncompute_extreme_point(lmo::FrankWolfe.NuclearNormLMO, direction::AbstractMatrix; tol=1e-8, kwargs...)\r\nFrankWolfe.MathOptLMO\r\nFrankWolfe.convert_mathopt","category":"page"},{"location":"reference.html#FrankWolfe.LinearMinimizationOracle","page":"Reference","title":"FrankWolfe.LinearMinimizationOracle","text":"Supertype for linear minimization oracles.\n\nAll LMOs must implement compute_extreme_point(lmo::LMO, direction) and return a vector v of the appropriate type.\n\n\n\n\n\n","category":"type"},{"location":"reference.html#FrankWolfe.CachedLinearMinimizationOracle","page":"Reference","title":"FrankWolfe.CachedLinearMinimizationOracle","text":"CachedLinearMinimizationOracle{LMO}\n\nOracle wrapping another one of type lmo. Subtypes of CachedLinearMinimizationOracle contain a cache of previous solutions.\n\nBy convention, the inner oracle is named inner. Cached optimizers are expected to implement Base.empty! and Base.length.\n\n\n\n\n\n","category":"type"},{"location":"reference.html#FrankWolfe.SingleLastCachedLMO","page":"Reference","title":"FrankWolfe.SingleLastCachedLMO","text":"SingleLastCachedLMO{LMO, VT}\n\nCaches only the last result from an LMO and stores it in last_vertex. Vertices of LMO have to be of type VT if provided.\n\n\n\n\n\n","category":"type"},{"location":"reference.html#FrankWolfe.MultiCacheLMO","page":"Reference","title":"FrankWolfe.MultiCacheLMO","text":"MultiCacheLMO{N, LMO, VT}\n\nCache for a LMO storing up to N vertices in the cache, removed in FIFO style. oldest_idx keeps track of the oldest index in the tuple, i.e. to replace next. VT, if provided, must be the type of vertices returned by LMO\n\n\n\n\n\n","category":"type"},{"location":"reference.html#FrankWolfe.VectorCacheLMO","page":"Reference","title":"FrankWolfe.VectorCacheLMO","text":"VectorCacheLMO{N, LMO, VT}\n\nCache for a LMO storing an unbounded number of vertices of type VT in the cache. VT, if provided, must be the type of vertices returned by LMO\n\n\n\n\n\n","category":"type"},{"location":"reference.html#FrankWolfe.ProductLMO","page":"Reference","title":"FrankWolfe.ProductLMO","text":"ProductLMO(lmos...)\n\nLinear minimization oracle over the Cartesian product of multiple LMOs.\n\n\n\n\n\n","category":"type"},{"location":"reference.html#FrankWolfe.compute_extreme_point-Tuple{FrankWolfe.ProductLMO, Tuple}","page":"Reference","title":"FrankWolfe.compute_extreme_point","text":"compute_extreme_point(lmo::ProductLMO, direction::Tuple; kwargs...)\n\nExtreme point computation on Cartesian product, with a direction (d1, d2, ...) given as a tuple of directions. All keyword arguments are passed to all LMOs.\n\n\n\n\n\n","category":"method"},{"location":"reference.html#FrankWolfe.compute_extreme_point-Union{Tuple{N}, Tuple{FrankWolfe.ProductLMO{N, TL} where TL<:Tuple{Vararg{FrankWolfe.LinearMinimizationOracle, N}}, AbstractArray}} where N","page":"Reference","title":"FrankWolfe.compute_extreme_point","text":"compute_extreme_point(lmo::ProductLMO, direction::AbstractArray; direction_indices, storage=similar(direction))\n\nExtreme point computation, with a direction array and direction_indices provided such that: direction[direction_indices[i]] is passed to the i-th LMO. The result is stored in the optional storage container.\n\nAll keyword arguments are passed to all LMOs.\n\n\n\n\n\n","category":"method"},{"location":"reference.html#FrankWolfe.UnitSimplexOracle","page":"Reference","title":"FrankWolfe.UnitSimplexOracle","text":"UnitSimplexOracle(right_side)\n\nRepresents the scaled unit simplex:\n\nC = {x ∈ R^n_+, ∑x ≤ right_side}\n\n\n\n\n\n","category":"type"},{"location":"reference.html#FrankWolfe.compute_extreme_point-Union{Tuple{T}, Tuple{FrankWolfe.UnitSimplexOracle{T}, Any}} where T","page":"Reference","title":"FrankWolfe.compute_extreme_point","text":"LMO for scaled unit simplex: ∑ x_i = τ Returns either vector of zeros or vector with one active value equal to RHS if there exists an improving direction.\n\n\n\n\n\n","category":"method"},{"location":"reference.html#FrankWolfe.compute_dual_solution-Union{Tuple{T}, Tuple{FrankWolfe.UnitSimplexOracle{T}, Any, Any}} where T","page":"Reference","title":"FrankWolfe.compute_dual_solution","text":"Dual costs for a given primal solution to form a primal dual pair for scaled unit simplex. Returns two vectors. The first one is the dual costs associated with the constraints  and the second is the reduced costs for the variables.\n\n\n\n\n\n","category":"method"},{"location":"reference.html#FrankWolfe.ProbabilitySimplexOracle","page":"Reference","title":"FrankWolfe.ProbabilitySimplexOracle","text":"ProbabilitySimplexOracle(right_side)\n\nRepresents the scaled probability simplex:\n\nC = {x ∈ R^n_+, ∑x = right_side}\n\n\n\n\n\n","category":"type"},{"location":"reference.html#FrankWolfe.compute_extreme_point-Union{Tuple{T}, Tuple{FrankWolfe.ProbabilitySimplexOracle{T}, Any}} where T","page":"Reference","title":"FrankWolfe.compute_extreme_point","text":"LMO for scaled probability simplex. Returns a vector with one active value equal to RHS in the most improving (or least degrading) direction.\n\n\n\n\n\n","category":"method"},{"location":"reference.html#FrankWolfe.compute_dual_solution-Union{Tuple{T}, Tuple{FrankWolfe.ProbabilitySimplexOracle{T}, Any, Any}} where T","page":"Reference","title":"FrankWolfe.compute_dual_solution","text":"Dual costs for a given primal solution to form a primal dual pair for scaled probability simplex. Returns two vectors. The first one is the dual costs associated with the constraints  and the second is the reduced costs for the variables.\n\n\n\n\n\n","category":"method"},{"location":"reference.html#FrankWolfe.KSparseLMO","page":"Reference","title":"FrankWolfe.KSparseLMO","text":"KSparseLMO{T}(K::Int, right_hand_side::T)\n\nLMO for the K-sparse polytope:\n\nC = B_1(τK) ∩ B_∞(τ)\n\nwith τ the right_hand_side parameter. The LMO results in a vector with the K largest absolute values of direction, taking values -τ sign(x_i).\n\n\n\n\n\n","category":"type"},{"location":"reference.html#FrankWolfe.BirkhoffPolytopeLMO","page":"Reference","title":"FrankWolfe.BirkhoffPolytopeLMO","text":"BirkhoffPolytopeLMO\n\nThe Birkhoff polytope encodes doubly stochastic matrices. Its extreme vertices are all permutation matrices of side-dimension dimension.\n\n\n\n\n\n","category":"type"},{"location":"reference.html#FrankWolfe.LpNormLMO","page":"Reference","title":"FrankWolfe.LpNormLMO","text":"LpNormLMO{T, p}(right_hand_side)\n\nLMO with feasible set being a bound on the L-p norm:\n\nC = {x ∈ R^n, norm(x, p) ≤ right_side}\n\n\n\n\n\n","category":"type"},{"location":"reference.html#FrankWolfe.KNormBallLMO","page":"Reference","title":"FrankWolfe.KNormBallLMO","text":"KNormBallLMO{T}(K::Int, right_hand_side::T)\n\nLMO for the K-norm ball, intersection of L1-ball (τK) and L∞-ball (τ/K)\n\nC_{K,τ} = conv { B_1(τ) ∪ B_∞(τ / K) }\n\nwith τ the right_hand_side parameter.\n\n\n\n\n\n","category":"type"},{"location":"reference.html#FrankWolfe.NuclearNormLMO","page":"Reference","title":"FrankWolfe.NuclearNormLMO","text":"NuclearNormLMO{T}(δ)\n\nLMO over matrices that have a nuclear norm less than δ. The LMO returns the rank-one matrix with singular value δ.\n\n\n\n\n\n","category":"type"},{"location":"reference.html#FrankWolfe.compute_extreme_point-Tuple{FrankWolfe.NuclearNormLMO, AbstractMatrix{T} where T}","page":"Reference","title":"FrankWolfe.compute_extreme_point","text":"Best rank-one approximation using the Golub-Kahan-Lanczos bidiagonalization from IterativeSolvers.\n\nWarning: this does not work (yet) with all number types, BigFloat and Float16 fail.\n\n\n\n\n\n","category":"method"},{"location":"reference.html#FrankWolfe.MathOptLMO","page":"Reference","title":"FrankWolfe.MathOptLMO","text":"MathOptLMO{OT <: MOI.Optimizer} <: LinearMinimizationOracle\n\nLinear minimization oracle with feasible space defined through a MathOptInterface.Optimizer. The oracle call sets the direction and reruns the optimizer.\n\nThe direction vector has to be set in the same order of variables as the MOI.ListOfVariableIndices() getter.\n\n\n\n\n\n","category":"type"},{"location":"reference.html#FrankWolfe.convert_mathopt","page":"Reference","title":"FrankWolfe.convert_mathopt","text":"convert_mathopt(lmo::LMO, optimizer::OT; kwargs...) -> MathOptLMO{OT}\n\nConverts the given LMO to its equivalent MathOptInterface representation using optimizer. Must be implemented by LMOs.\n\n\n\n\n\n","category":"function"},{"location":"reference.html#Components","page":"Reference","title":"Components","text":"","category":"section"},{"location":"reference.html","page":"Reference","title":"Reference","text":"This section gathers all additional relevant components of the FrankWolfe.jl package.","category":"page"},{"location":"reference.html#Functions-and-Structures-2","page":"Reference","title":"Functions and Structures","text":"","category":"section"},{"location":"reference.html","page":"Reference","title":"Reference","text":"FrankWolfe.ActiveSet\r\nFrankWolfe.active_set_update!\r\nFrankWolfe.compute_active_set_iterate\r\nFrankWolfe.active_set_argmin\r\nFrankWolfe.active_set_argminmax\r\nFrankWolfe.find_minmax_directions\r\nFrankWolfe.minimize_over_convex_hull!\r\nFrankWolfe.build_reduced_problem(atoms::AbstractVector{<:FrankWolfe.ScaledHotVector},hessian,weights,gradient,tolerance)\r\nFrankWolfe.build_reduced_problem(atoms::AbstractVector{<:SparseArrays.AbstractSparseArray},hessian,weights,gradient,tolerance)\r\nFrankWolfe.build_reduced_problem(atoms::AbstractVector{<:Array},hessian,weights,gradient,tolerance)\r\nFrankWolfe.Strong_Frank_Wolfe_gap\r\nFrankWolfe.accelerated_simplex_gradient_descent_over_probability_simplex\r\nFrankWolfe.simplex_gradient_descent_over_probability_simplex\r\nFrankWolfe.projection_simplex_sort\r\nFrankWolfe.Strong_Frank_Wolfe_gap_probability_simplex\r\nFrankWolfe.simplex_gradient_descent_over_convex_hull\r\nFrankWolfe.lp_separation_oracle\r\nFrankWolfe.LineSearchMethod\r\nFrankWolfe.Emphasis\r\nFrankWolfe.ObjectiveFunction\r\nFrankWolfe.compute_value_gradient\r\nFrankWolfe.StochasticObjective\r\nFrankWolfe.ScaledHotVector\r\nFrankWolfe.RankOneMatrix\r\nFrankWolfe.line_search_wrapper\r\nFrankWolfe.adaptive_step_size\r\nFrankWolfe.plot_results\r\nFrankWolfe._unsafe_equal\r\nFrankWolfe.check_gradients\r\nFrankWolfe.trajectory_callback","category":"page"},{"location":"reference.html#FrankWolfe.ActiveSet","page":"Reference","title":"FrankWolfe.ActiveSet","text":"ActiveSet{AT, R, IT}\n\nRepresents an active set of extreme vertices collected in a FW algorithm, along with their coefficients (λ_i, a_i). R is the type of the λ_i, AT is the type of the atoms a_i. The iterate x = ∑λ_i a_i is stored in x with type IT.\n\n\n\n\n\n","category":"type"},{"location":"reference.html#FrankWolfe.active_set_update!","page":"Reference","title":"FrankWolfe.active_set_update!","text":"active_set_update!(active_set::ActiveSet, lambda, atom)\n\nAdds the atom to the active set with weight lambda or adds lambda to existing atom.\n\n\n\n\n\n","category":"function"},{"location":"reference.html#FrankWolfe.compute_active_set_iterate","page":"Reference","title":"FrankWolfe.compute_active_set_iterate","text":"compute_active_set_iterate(active_set)\n\n\n\n\n\n","category":"function"},{"location":"reference.html#FrankWolfe.active_set_argmin","page":"Reference","title":"FrankWolfe.active_set_argmin","text":"active_set_argmin(active_set::ActiveSet, direction)\n\nComputes the linear minimizer in the direction on the active set. Returns (λ_i, a_i, i)\n\n\n\n\n\n","category":"function"},{"location":"reference.html#FrankWolfe.active_set_argminmax","page":"Reference","title":"FrankWolfe.active_set_argminmax","text":"active_set_argminmax(active_set::ActiveSet, direction)\n\nComputes the linear minimizer in the direction on the active set. Returns (λ_min, a_min, i_min, λ_max, a_max, i_max)\n\n\n\n\n\n","category":"function"},{"location":"reference.html#FrankWolfe.find_minmax_directions","page":"Reference","title":"FrankWolfe.find_minmax_directions","text":"find_minmax_directions(active_set::ActiveSet, direction, Φ)\n\nComputes the point of the active set minimizing in direction on the active set (local Frank Wolfe) and the maximizing one (away step). Returns the two corresponding indices in the active set, along with a flag indicating if the direction improvement is above a threshold. goodstep_tolerance ∈ (0, 1] is a tolerance coefficient multiplying Φ for the validation of the progress. \n\n\n\n\n\n","category":"function"},{"location":"reference.html#FrankWolfe.minimize_over_convex_hull!","page":"Reference","title":"FrankWolfe.minimize_over_convex_hull!","text":"minimize_over_convex_hull!\n\nGiven a function f with gradient grad! and an active set  active_set this function will minimize the function over  the convex hull of the active set until the strong-wolfe  gap over the active set is below tolerance.\n\nIt will either directly minimize over the convex hull using simplex gradient descent, or it will transform the problem  to barycentric coordinates and minimize over the unit  probability simplex using gradient descent or Nesterov's  accelerated gradient descent.\n\n\n\n\n\n","category":"function"},{"location":"reference.html#FrankWolfe.build_reduced_problem-Tuple{AbstractVector{var\"#s11\"} where var\"#s11\"<:FrankWolfe.ScaledHotVector, Any, Any, Any, Any}","page":"Reference","title":"FrankWolfe.build_reduced_problem","text":"build_reduced_problem(atoms::AbstractVector{<:FrankWolfe.ScaledHotVector}, hessian, weights, gradient, tolerance)\n\nGiven an active set formed by ScaledHotVector, a (constant) Hessian and a gradient constructs a quadratic problem  over the unit probability simplex that is equivalent to  minimizing the original function over the convex hull of the active set. If λ are the barycentric coordinates of dimension equal to the cardinality of the active set, the objective  function is:     f(λ) = reducedlinear^T λ + 0.5 * λ^T reducedhessian λ\n\nIn the case where we find that the current iterate has a strong-Wolfe  gap over the convex hull of the active set that is below the tolerance  we return nothing (as there is nothing to do).\n\n\n\n\n\n","category":"method"},{"location":"reference.html#FrankWolfe.build_reduced_problem-Tuple{AbstractVector{var\"#s11\"} where var\"#s11\"<:AbstractSparseArray, Any, Any, Any, Any}","page":"Reference","title":"FrankWolfe.build_reduced_problem","text":"buildreducedproblem\n\nSame as the function above, but for the case where the active  set is formed by Sparse Arrays.\n\n\n\n\n\n","category":"method"},{"location":"reference.html#FrankWolfe.build_reduced_problem-Tuple{AbstractVector{var\"#s11\"} where var\"#s11\"<:Array, Any, Any, Any, Any}","page":"Reference","title":"FrankWolfe.build_reduced_problem","text":"build_reduced_problem(atoms::AbstractVector{<:Array}, hessian, weights, gradient, tolerance)\n\nSame as the two function above, but for the case where the active  set is formed by dense Arrays.\n\n\n\n\n\n","category":"method"},{"location":"reference.html#FrankWolfe.Strong_Frank_Wolfe_gap","page":"Reference","title":"FrankWolfe.Strong_Frank_Wolfe_gap","text":"Checks the strong Frank-Wolfe gap for the reduced problem.\n\n\n\n\n\n","category":"function"},{"location":"reference.html#FrankWolfe.accelerated_simplex_gradient_descent_over_probability_simplex","page":"Reference","title":"FrankWolfe.accelerated_simplex_gradient_descent_over_probability_simplex","text":"accelerated_simplex_gradient_descent_over_probability_simplex\n\nMinimizes an objective function over the unit probability simplex  until the Strong-Wolfe gap is below tolerance using Nesterov's  accelerated gradient descent.\n\n\n\n\n\n","category":"function"},{"location":"reference.html#FrankWolfe.simplex_gradient_descent_over_probability_simplex","page":"Reference","title":"FrankWolfe.simplex_gradient_descent_over_probability_simplex","text":"simplexgradientdescentoverprobability_simplex\n\nMinimizes an objective function over the unit probability simplex  until the Strong-Wolfe gap is below tolerance using gradient descent.\n\n\n\n\n\n","category":"function"},{"location":"reference.html#FrankWolfe.projection_simplex_sort","page":"Reference","title":"FrankWolfe.projection_simplex_sort","text":"projection_simplex_sort(x; s=1.0)\n\nPerform a projection onto the probability simplex of radius s using a sorting algorithm.\n\n\n\n\n\n","category":"function"},{"location":"reference.html#FrankWolfe.Strong_Frank_Wolfe_gap_probability_simplex","page":"Reference","title":"FrankWolfe.Strong_Frank_Wolfe_gap_probability_simplex","text":"StrongFrankWolfegapprobability_simplex\n\nCompute the Strong-Wolfe gap over the unit probability simplex  given a gradient.\n\n\n\n\n\n","category":"function"},{"location":"reference.html#FrankWolfe.simplex_gradient_descent_over_convex_hull","page":"Reference","title":"FrankWolfe.simplex_gradient_descent_over_convex_hull","text":"simplexgradientdescentoverconvex_hull\n\nMinimizes an objective function over the convex hull of the active set until the Strong-Wolfe gap is below tolerance using simplex gradient descent.\n\n\n\n\n\n","category":"function"},{"location":"reference.html#FrankWolfe.lp_separation_oracle","page":"Reference","title":"FrankWolfe.lp_separation_oracle","text":"Returns either a tuple (y, val) with y an atom from the active set satisfying the progress criterion and val the corresponding gap dot(y, direction) or the same tuple with y from the LMO.\n\ninplace_loop controls whether the iterate type allows in-place writes. kwargs are passed on to the LMO oracle.\n\n\n\n\n\n","category":"function"},{"location":"reference.html#FrankWolfe.LineSearchMethod","page":"Reference","title":"FrankWolfe.LineSearchMethod","text":"Line search method to apply once the direction is computed.\n\n\n\n\n\n","category":"type"},{"location":"reference.html#FrankWolfe.Emphasis","page":"Reference","title":"FrankWolfe.Emphasis","text":"Emphasis given to the algorithm for memory-saving or not. The memory-saving mode may not be faster than the default blas mode for small dimensions.\n\n\n\n\n\n","category":"type"},{"location":"reference.html#FrankWolfe.ObjectiveFunction","page":"Reference","title":"FrankWolfe.ObjectiveFunction","text":"ObjectiveFunction{VT}\n\nRepresents an objective function optimized by algorithms. Subtypes of ObjectiveFunction must implement at least\n\ncompute_value(::ObjectiveFunction, x) for primal value evaluation\ncompute_gradient(::ObjectiveFunction, x) for gradient evaluation.\n\nand optionally compute_value_gradient(::ObjectiveFunction, x) returning the (primal, gradient) pair.\n\n\n\n\n\n","category":"type"},{"location":"reference.html#FrankWolfe.compute_value_gradient","page":"Reference","title":"FrankWolfe.compute_value_gradient","text":"compute_value_gradient(f::ObjectiveFunction, x; kwargs)\n\nComputes in one call the pair (function_value, function_grad) evaluated at x. By default, calls compute_value and compute_gradient with keyword kwargs passed to both.\n\n\n\n\n\n","category":"function"},{"location":"reference.html#FrankWolfe.StochasticObjective","page":"Reference","title":"FrankWolfe.StochasticObjective","text":"StochasticObjective{F, G, XT}(f::F, grad::G, xs::XT)\n\nRepresents an objective function evaluated with stochastic gradient. f(θ, x) evaluates the loss for data point x and parameter θ. grad(θ, x) evaluates the loss gradient with respect to data point x at parameter θ. xs must be an indexable iterable (Vector{Vector{Float64}} for instance). Functions using a StochasticObjective have optional keyword arguments rng, batch_size and full_evaluation controlling whether the function should be evaluated over all data points.\n\n\n\n\n\n","category":"type"},{"location":"reference.html#FrankWolfe.ScaledHotVector","page":"Reference","title":"FrankWolfe.ScaledHotVector","text":"ScaledHotVector{T}\n\nRepresents a vector of at most one value different from 0.\n\n\n\n\n\n","category":"type"},{"location":"reference.html#FrankWolfe.RankOneMatrix","page":"Reference","title":"FrankWolfe.RankOneMatrix","text":"RankOneMatrix{T, UT, VT}\n\nRepresents a rank-one matrix R = u * vt'. Composes like a charm.\n\n\n\n\n\n","category":"type"},{"location":"reference.html#FrankWolfe.line_search_wrapper","page":"Reference","title":"FrankWolfe.line_search_wrapper","text":"line search wrapper NOTE: The stepsize is defined as x - gamma * d\n\nReturns the step size gamma and the Lipschitz estimate L\n\n\n\n\n\n","category":"function"},{"location":"reference.html#FrankWolfe.adaptive_step_size","page":"Reference","title":"FrankWolfe.adaptive_step_size","text":"Slight modification of Adaptive Step Size strategy from https://arxiv.org/pdf/1806.05123.pdf\n\nNote: direction is opposite to the improving direction norm(gradient, direction) > 0 TODO: \n\nmake emphasis aware and optimize\n\n\n\n\n\n","category":"function"},{"location":"reference.html#FrankWolfe.plot_results","page":"Reference","title":"FrankWolfe.plot_results","text":"plot_results\n\nGiven a series of list, generate subplots. listdatay -> contains a list of a list of lists (where each list refers to a subplot, and a list of lists refers to the y-values of the series inside a subplot). listdatax -> contains a list of a list of lists (where each list refers to a subplot, and a list of lists refers to the x-values of the series inside a subplot). So if we have one plot with two series, these might look like:     listdatay = [[[1, 2, 3, 4, 5, 6], [1, 2, 3, 4, 5, 6]]]     listdatax = [[[1, 2, 3, 4, 5, 6], [1, 2, 3, 4, 5, 6]]]\n\nAnd if we have two plots, each with two series, these might look like:     listdatay = [[[1, 2, 3, 4, 5, 6], [1, 2, 3, 4, 5, 6]], [[7, 8, 9, 10, 11, 12], [7, 8, 9, 10, 11, 12]]]     listdatax = [[[1, 2, 3, 4, 5, 6], [1, 2, 3, 4, 5, 6]], [[7, 8, 9, 10, 11, 12], [7, 8, 9, 10, 11, 12]]]\n\nlistlabel -> contains the labels for the series that will be plotted, which has to have a length equal to the number of series that are being plotted:     listlabel = [\"Series 1\", \"Series 2\"]\n\nlistaxisx -> contains the labels for the x-axis that will be plotted,  which has to have a length equal to the number of subplots:      listaxisx = [\"x-axis plot 1\", \"x-axis plot 1\"]\n\nlistaxisy -> Same as listaxisx but for the y-axis\n\nxscalelog -> A list of values indicating the type of axes to use in each subplot, must be equal to the number of subplots:     xscalelog = [:log, :identity]\n\nyscalelog -> Same as xscalelog but for the y-axis\n\n\n\n\n\n","category":"function"},{"location":"reference.html#FrankWolfe._unsafe_equal","page":"Reference","title":"FrankWolfe._unsafe_equal","text":"isequal without the checks. Assumes a and b have the same axes.\n\n\n\n\n\n","category":"function"},{"location":"reference.html#FrankWolfe.check_gradients","page":"Reference","title":"FrankWolfe.check_gradients","text":"Check if the gradient using finite differences matches the grad! provided.\n\n\n\n\n\n","category":"function"},{"location":"reference.html#FrankWolfe.trajectory_callback","page":"Reference","title":"FrankWolfe.trajectory_callback","text":"trajectory_callback(storage)\n\nCallback pushing the state at each iteration to the passed storage. The state data is only the 5 first fields, usually: (t,primal,dual,dual_gap,time)\n\n\n\n\n\n","category":"function"},{"location":"indexlist.html#Index","page":"Index","title":"Index","text":"","category":"section"},{"location":"indexlist.html","page":"Index","title":"Index","text":"","category":"page"},{"location":"index.html#Introduction","page":"Home","title":"Introduction","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"This package defines a generic interface and several implementations for Frank-Wolfe algorithms.","category":"page"}]
}
